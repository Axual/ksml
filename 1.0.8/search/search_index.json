{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KSML Documentation","text":"<p>Welcome to the KSML documentation, Use the menu on the left to navigate through the various sections</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>If you want to get going quickly, go to the KSML Quickstart.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>KSML allows anyone to specify a powerful Kafka Streams application in just a few lines of YAML and Python snippets.</p>"},{"location":"#contents","title":"Contents","text":"<ol> <li>Introduction</li> <li>Stream Types</li> <li>Functions</li> <li>Pipelines</li> <li>Operations</li> <li>Data Types</li> <li>Runners</li> <li>Language specification</li> </ol> <p>Getting Started</p> <p>Release Notes</p>"},{"location":"functions/","title":"Functions","text":""},{"location":"functions/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Data types in Python<ul> <li>Data type mapping</li> <li>Automatic conversion</li> </ul> </li> <li>Function Types</li> <li>Function parameters<ul> <li>Logger</li> <li>Metrics</li> <li>State stores</li> </ul> </li> </ul>"},{"location":"functions/#introduction","title":"Introduction","text":"<p>Functions can be specified in the <code>functions</code> section of a KSML definition file. The layout typically looks like this:</p> <pre><code>functions:\n  my_first_predicate:\n    type: predicate\n    expression: key=='Some string'\n\n  compare_params:\n    type: generic\n    parameters:\n      - name: firstParam\n        type: string\n      - name: secondParam\n        type: int\n    globalCode: |\n      import something from package\n      globalVar = 3\n    code: |\n      print('Hello there!')\n    expression: firstParam == str(secondParam)\n</code></pre> <p>Functions are defined by the following tags:</p> Parameter Value Type Default Description <code>type</code> <code>string</code> <code>generic</code> The type of the function defined <code>parameters</code> List of parameter definitions empty list A list of parameters, each of which contains the mandatory fields <code>name</code> and <code>type</code>. See example above. <code>globalCode</code> <code>string</code> empty Snippet of Python code that is executed once upon creation of the Kafka Streams topology. This section can contain statements like <code>import</code> to import function libraries used in the <code>code</code> and <code>expression</code> sections. <code>code</code> <code>string</code> empty Python source code, which will be included in the called function. <code>expression</code> <code>string</code> empty Python expression that contains the returned function result. <p>See below for the list of supported function types.</p>"},{"location":"functions/#data-types-in-python","title":"Data types in Python","text":"<p>Internally, KSML uses an abstraction to deal with all kinds of data types. See types for more information on data types.</p>"},{"location":"functions/#data-type-mapping","title":"Data type mapping","text":"<p>Data types are automatically converted to/from Python in the following manner:</p> Data type Python type Example boolean bool True, False bytes bytearray double float 3.145 float float 1.23456 byte int between -128 and 127 short int between -65,536 and 65,535 int int between -2,147,483,648 and 2,147,483,647 long int between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 string str \"text\" enum str enum string literal, eg. \"BLUE\", \"EUROPE\" list array [ \"key1\", \"key2\" ] struct dict { \"key1\": \"value1\", \"key2\": \"value2\" } struct with schema dict { \"key1\": \"value1\", \"key2\": \"value2\", \"@type\": \"SensorData\", \"@schema\": \"...\" } tuple tuple (1, \"text\", 3.14, { \"key\": \"value\" }) union Real value is translated as specified in this table"},{"location":"functions/#automatic-conversion","title":"Automatic conversion","text":"<p>KSML is able to automatically convert between types. Examples are:</p> <ul> <li>To/from string conversion is handled automatically for almost all data types, including string-notations such as CSV,   JSON and XML.</li> <li>When a string is expected, but a struct is passed in, the struct is automatically converted to string.</li> <li>When a struct is expected, but a string is passed in, the string is parsed according to the notation specified.</li> <li>Field matching and field type conversion is done automatically. For instance, if a struct contains an integer field,   but the target schema expects a string, the integer is automatically converted to string.</li> </ul>"},{"location":"functions/#function-types","title":"Function Types","text":"<p>Functions in KSML always have a <code>type</code>. When no type is specified, the function type is inferred from the context, or it defaults back to <code>generic</code>. This section discusses the purpose of every function type, and what fixed arguments every call gets passed in.</p>"},{"location":"functions/#aggregator","title":"Aggregator","text":"<p>An <code>aggregator</code> incrementally integrates a new keu/value into an aggregatedValue. It is called for every new message that becomes part of the aggregated result.</p> <p>The following highlights which calls are made to which function type during a regular aggregation, in this case for counting the number of messages:</p> <pre><code># Aggregation starts\ninitializer() -&gt; 0\nmsg1: aggregator(msg1.key, msg1.value, 0) -&gt; 1\nmsg2: aggregator(msg2.key, msg2.value, 1) -&gt; 1\nmsg3: aggregator(msg3.key, msg3.value, 2) -&gt; 3\n</code></pre> <p>The result in this example is 3.</p> <p>Aggregators get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message to be included in the aggregated result thus far. <code>value</code> any The value of the message to be included in the aggregated result thus far. <code>aggregatedValue</code> any The aggregated value thus far. returns any The new aggregated result, which includes the latest message."},{"location":"functions/#foreach","title":"ForEach","text":"<p>A <code>forEach</code> function is called for every message in a stream. When part of a <code>forEach</code> operation at the end of a pipeline, the function is the last one called for every message. When this function is called during <code>peek</code> operations, it may look at the messages and cause side effects (e.g. printing the message to stdout), and the pipeline will continue with the unmodified message after doing so.</p> <p>ForEach functions get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns none Nothing is returned."},{"location":"functions/#foreignkeyextractor","title":"ForeignKeyExtractor","text":"<p>A <code>foreignKeyExtractor</code> is a function used during (left) joins of two tables. The function translates a value from \"this table\" and translates it into a key of the \"other table\" that is joined with.</p> <p>ForEach functions get the following fixed arguments:</p> Parameter Value Type Description <code>value</code> any The value of the message. returns any The key looked up in the table joined with."},{"location":"functions/#generator","title":"Generator","text":"<p>A <code>generator</code> is a function that generates new messages out of thin air. It is most often used to generate mock data for testing purposes.</p> <p>Generators get no arguments, and return messages to be sent to the output stream.</p> Parameter Value Type Description returns <code>(_any_, _any</code>) The key/value of the message to be sent to the output stream."},{"location":"functions/#generic","title":"Generic","text":"<p>A <code>generic</code> function can be used for generic purposes. It can be used for any operation, as long as its parameters match the expected types of the operation's function.</p> <p>Generic functions get any arguments, and may return anything.</p> Parameter Value Type Description self-defined any Self-defined parameters can be passed in. returns any Can return any value."},{"location":"functions/#initializer","title":"Initializer","text":"<p>An <code>initializer</code> is called upon the start of every (part of an) aggregation. It takes no arguments and should return an initial value for the aggregation.</p> Parameter Value Type Description returns any An initial value for the aggregation. In a counting aggregation, this would be <code>0</code>."},{"location":"functions/#keytransformer","title":"KeyTransformer","text":"<p>A <code>keyTransformer</code> is able to transform a key/value into a new key, which then gets combined with the original value as a new message on the output stream.</p> <p>KeyTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns any The key of the output message."},{"location":"functions/#keyvalueprinter","title":"KeyValuePrinter","text":"<p>A <code>keyValuePrinter</code> takes a message and converts it to <code>string</code> before outputting it to a file or printing it to stdout.</p> <p>KeyValuePrinters get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>string</code> The string to be written to file or stdout."},{"location":"functions/#keyvaluetokeyvaluelisttransformer","title":"KeyValueToKeyValueListTransformer","text":"<p>A <code>keyValueToKeyValueListTransformer</code> takes one message and converts it into a list of output messages, which then get sent to the output stream. An example for this type of function would be a message, which contains a list of items in its <code>value</code> (e.g. <code>(k, [item])</code>. Using a <code>transformKeyValueToKeyValueList</code> operation, this message can be converted into individual messages <code>(k,item1), (k,item2), ...</code> on the output stream.</p> <p>KeyValueToKeyValueListTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>[(_any_, _any_)]</code> A list of messages for the output stream."},{"location":"functions/#keyvaluetovaluelisttransformer","title":"KeyValueToValueListTransformer","text":"<p>A <code>keyValueToValueListTransformer</code> takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. An example for this type of function would be a message, which contains a list of items in its <code>value</code> (e.g. <code>(k, [item])</code>. Using a <code>transformKeyValueToValueList</code> operation, this message can be converted into a list of values <code>[item1, item2, ...]</code> which get combined with the key of the message into <code>(k,item1), (k,item2), ...</code>on the output stream.</p> <p>KeyValueToValueListTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>[_any_]</code> A list of values to be combined with the key on the output stream."},{"location":"functions/#keyvaluetransformer","title":"KeyValueTransformer","text":"<p>A <code>keyValueTransformer</code> takes one message and converts it into another message, which may have different key/value types.</p> <p>KeyValueTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>(_any_, _any_)</code> The transformed message."},{"location":"functions/#merger","title":"Merger","text":"<p>A <code>merger</code> takes a key and two values, and merges those values together into a new value. That value is combined with the original key and sent to the output stream.</p> <p>Mergers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value1</code> any The value of the first message. <code>value2</code> any The value of the second message. returns any The merged value of the output message."},{"location":"functions/#metadatatransformer","title":"MetadataTransformer","text":"<p>A <code>metadataTransformer</code> can transform a message's metadata (headers and timestamp).</p> <p>MetadataTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. <code>metadata</code> <code>dict</code> Contains the <code>headers</code> and <code>timestamp</code> of the message. returns <code>dict</code> The (optionally) modified metadata for the output message. This structure should have the same type as the <code>metadata</code> passed in."},{"location":"functions/#predicate","title":"Predicate","text":"<p>A <code>predicate</code> is a function that takes the key/value of a message and returns <code>True</code> or <code>False</code>. It is used for filtering and branching purposes (e.g. routing messages based on content).</p> <p>Predicates get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>boolean</code> <code>True</code> or <code>False</code>."},{"location":"functions/#reducer","title":"Reducer","text":"<p>A <code>reducer</code> is a function that combines two aggregated results into one.</p> <p>Reducers get the following fixed arguments:</p> Parameter Value Type Description <code>value1</code> any The value of the first aggregation result. <code>value2</code> any The value of the second aggregation result. returns any The value of the combined aggregation result."},{"location":"functions/#streampartitioner","title":"StreamPartitioner","text":"<p>A <code>streamPartitioner</code> is a function that can assign a partition number to every message. It is used to repartition Kafka topics, based on message contents.</p> <p>StreamPartitioners get the following fixed arguments:</p> Parameter Value Type Description <code>topic</code> <code>string</code> The topic of the message. <code>key</code> any The key of the message. <code>value</code> any The value of the message. <code>numPartitions</code> <code>integer</code> The number of partitions available on the output topic. returns <code>integer</code> The partition number to which this message gets sent."},{"location":"functions/#timestampextractor","title":"TimestampExtractor","text":"<p>A <code>timestampExtractor</code> is a function which can determine a timestamp from a given input message, which is used for all downstream processing.</p> <p>TimestampExtractors get the following fixed arguments:</p> Parameter Value Type Description <code>record</code> <code>struct</code> A dictionary containing the <code>timestamp</code>, <code>timestampType</code>, <code>key</code>, <code>value</code>, <code>topic</code>, <code>partition</code> and <code>offset</code> of the input message. <code>previousTimestamp</code> <code>long</code> The timestamp of the last message (before this one). returns <code>long</code> The timestamp to apply to this message."},{"location":"functions/#topicnameextractor","title":"TopicNameExtractor","text":"<p>A <code>topicNameExtractor</code> is a function which can derive a topic name from a message, for example by getting the customer name from a message and deriving the topic name from that. It is used by <code>toTopicNameExtractor</code> operations to send messages to individually determined topics.</p> <p>TopicNameExtractors get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns <code>string</code> The name of the topic to send this message to."},{"location":"functions/#valuejoiner","title":"ValueJoiner","text":"<p>A <code>valueJoiner</code> takes a key and two values, and combines the two values into one. That value is then combined with the original key and sent to the output stream.</p> <p>ValueJoiners get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value1</code> any The value of the first message. <code>value2</code> any The value of the second message. returns any The joined value of the output message."},{"location":"functions/#valuetransformer","title":"ValueTransformer","text":"<p>A <code>valueTransformer</code> takes a key/value and transforms it into a new value, which is combined with the original key and sent to the output stream.</p> <p>ValueTransformers get the following fixed arguments:</p> Parameter Value Type Description <code>key</code> any The key of the message. <code>value</code> any The value of the message. returns any The value of the output message."},{"location":"functions/#function-parameters","title":"Function parameters","text":"<p>Besides the parameters mentioned above, all Python functions in KSML get special parameters passed in:</p>"},{"location":"functions/#logger","title":"Logger","text":"<p>Every function can access the <code>log</code> variable, which is mapped to a plain Java Logger object. It can be used to send output to the KSML log by calling its methods. It supports the following operations:</p> Method Description <code>error(message: str, value_params...)</code> logs an error message <code>warn(message: str, value_params...)</code> logs a warning message <code>info(message: str, value_params...)</code> logs an informational message <code>debug(message: str, value_params...)</code> logs a debug message <code>trace(message: str, value_params...)</code> logs a trace message <p>The message contains double curly brackets <code>{}</code>, which will be substituted by the value parameters. Examples are:</p> <pre><code>log.error(\"Something went completely bad here!\")\nlog.info(\"Received message from topic: key={}, value={}\", key, value)\nlog.debug(\"I'm printing five variables here: {}, {}, {}, {}, {}. Lovely isn't it?\", 1, 2, 3, \"text\", {\"json\":\"is cool\"})\n</code></pre> <p>Output of the above statements looks like:</p> <pre><code>[LOG TIMESTAMP] ERROR function.name   Something went completely bad here!\n[LOG TIMESTAMP] INFO  function.name   Received message from topic: key=123, value={\"key\":\"value\"}\n[LOG TIMESTAMP] DEBUG function.name   I'm printing five variables here: 1, 2, 3, text, {\"json\":\"is cool\"}. Lovely isn't it?\n</code></pre>"},{"location":"functions/#metrics","title":"Metrics","text":"<p>KSML supports metric collection and exposure through JMX and built-in Prometheus agent. Metrics for Python functions are automatically generated and collected, but users can also specify their own metrics. For an example, see <code>17-example-inspect-with-metrics.yaml</code> in the <code>examples</code> directory.</p> <p>KSML supports the following metric types:</p> <ul> <li>Counter: an increasing integer, which counts for example the number of calls made to a Python function.</li> <li>Meter: used for periodically updating a measurement value. Preferred over Counter when don't care too much about exact   averages, but want to monitor trends instead.</li> <li>Timer: measures the time spent by processes or functions, that get called internally.</li> </ul> <p>Every Python function in KSML can use the <code>metrics</code> variable, which is made available by KSML. The object supports the following methods to create your own metrics:</p> <ul> <li>counter(name: str, tags: dict) -&gt; Counter</li> <li>counter(name: str) -&gt; Counter</li> <li>meter(name: str, tags: dict) -&gt; Meter</li> <li>meter(name: str) -&gt; Meter</li> <li>timer(name: str, tags: dict) -&gt; Timer</li> <li>timer(name: str) -&gt; Timer</li> </ul> <p>In turn these objects support the following:</p>"},{"location":"functions/#counter","title":"Counter","text":"<ul> <li>increment()</li> <li>increment(delta: int)</li> </ul>"},{"location":"functions/#meter","title":"Meter","text":"<ul> <li>mark()</li> <li>mark(nrOfEvents: int)</li> </ul>"},{"location":"functions/#timer","title":"Timer","text":"<ul> <li>updateSeconds(valueSeconds: int)</li> <li>updateMillis(valueMillis: int)</li> <li>updateNanos(valueNanos: int)</li> </ul>"},{"location":"functions/#state-stores","title":"State stores","text":"<p>Some functions are allowed to access local state stores. These functions specify the <code>stores</code> attribute in their definitions. The state stores they reference are accessible as variables with the same name as the state store.</p> <p>Examples:</p> <pre><code>streams:\n  sensor_source_avro:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n\nstores:\n  last_sensor_data_store:\n    type: keyValue\n    keyType: string\n    valueType: json\n    persistent: false\n    historyRetention: 1h\n    caching: false\n    logging: false\n\nfunctions:\n  process_message:\n    type: forEach\n    code: |\n      last_value = last_sensor_data_store.get(key)\n      if last_value is not None:\n        log.info(\"Found last value: {} = {}\", key, last_value)\n      last_sensor_data_store.put(key, value)\n      if value is not None:\n        log.info(\"Stored new value: {} = {}\", key, value)\n    stores:\n      - last_sensor_data_store\n\npipelines:\n  process_message:\n    from: sensor_source_avro\n    forEach: process_message\n</code></pre> <p>In this example the function <code>process_message</code> uses the state store <code>last_sensor_data_store</code> directly as a variable. It is allowed to do that when it declares such use in its definition under the <code>stores</code> attribute.</p> <p>State stores have common methods like <code>get</code> and <code>put</code>, which you can call directly from Python code.</p>"},{"location":"introduction/","title":"KSML: Kafka Streams for Low Code Environments","text":""},{"location":"introduction/#abstract","title":"Abstract","text":"<p>Kafka Streams has captured the hearts and minds of many developers that want to develop streaming applications on top of Kafka. But as powerful as the framework is, Kafka Streams has had a hard time getting around the requirement of writing Java code and setting up build pipelines. There were some attempts to rebuild Kafka Streams, but up until now popular languages like Python did not receive equally powerful (and maintained) stream processing frameworks. In this article we will present a new declarative approach to unlock Kafka Streams, called KSML. By the time you finish reading this document, you will be able to write streaming applications yourself, using only a few simple basic rules and Python snippets.</p> <ul> <li>Setting up a test environment</li> <li>KSML in practice<ul> <li>Example 1. Inspect data on a topic</li> <li>Example 2. Copying data to another topic</li> <li>Example 3. Filtering data</li> <li>Example 4. Branching messages</li> <li>Example 5. Dynamic routing</li> <li>Example 6. Multiple pipelines</li> </ul> </li> </ul>"},{"location":"introduction/#setting-up-a-test-environment","title":"Setting up a test environment","text":"<p>To demonstrate KSML's capabilities, you will need a working Kafka cluster, or an Axual Platform/Cloud environment. Check out the Runners page to configure KSML.  We set up a test topic, called <code>ksml_sensordata_avro</code> with key/value types of <code>String</code>/<code>SensorData</code>. The [SensorData] schema was created for demo purposes only and contains several fields to demonstrate KSML capabilities:</p> <pre><code>{\n  \"namespace\": \"io.axual.ksml.example\",\n  \"doc\": \"Emulated sensor data with a few additional attributes\",\n  \"name\": \"SensorData\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"doc\": \"The name of the sensor\",\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The timestamp of the sensor reading\",\n      \"name\": \"timestamp\",\n      \"type\": \"long\"\n    },\n    {\n      \"doc\": \"The value of the sensor, represented as string\",\n      \"name\": \"value\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The type of the sensor\",\n      \"name\": \"type\",\n      \"type\": {\n        \"name\": \"SensorType\",\n        \"type\": \"enum\",\n        \"doc\": \"The type of a sensor\",\n        \"symbols\": [\n          \"AREA\",\n          \"HUMIDITY\",\n          \"LENGTH\",\n          \"STATE\",\n          \"TEMPERATURE\"\n        ]\n      }\n    },\n    {\n      \"doc\": \"The unit of the sensor\",\n      \"name\": \"unit\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The color of the sensor\",\n      \"name\": \"color\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    },\n    {\n      \"doc\": \"The city of the sensor\",\n      \"name\": \"city\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    },\n    {\n      \"doc\": \"The owner of the sensor\",\n      \"name\": \"owner\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    }\n  ]\n}\n</code></pre> <p>For the rest of this document, we assume you have set up the <code>ksml_sensordata_avro</code> topic and populated it with some random data.</p> <p>So without any further delays, let's see how KSML allows us to process this data.</p>"},{"location":"introduction/#ksml-in-practice","title":"KSML in practice","text":""},{"location":"introduction/#example-1-inspect-data-on-a-topic","title":"Example 1. Inspect data on a topic","text":"<p>The first example is one where we inspect data on a specific topic. The definition is as follows:</p> <pre><code>streams:\n  sensor_source_avro:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n\nfunctions:\n  # Log the message using the built-in log variable that is passed in from Java\n  log_message:\n    type: forEach\n    parameters:\n      - name: format\n        type: string\n    code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value)\n\npipelines:\n  # Multiple pipelines can be created in a single KSML definition\n  consume_avro:\n    from: sensor_source_avro\n    forEach:\n      code: log_message(key, value, format=\"AVRO\")\n</code></pre> <p>Let's analyze this definition one element at a time. Before defining processing logic, we first define the streams used by the definition. In this case we define a stream named <code>sensor_source_avro</code> which reads from the topic <code>ksml_sensordata_avro</code>. The stream defines a <code>string</code> key and Avro <code>SensorData</code> values.</p> <p>Next is a list of functions that can be used by the processing logic. Here we define just one, <code>log_message</code>, which simply uses the provided logger to write the key, value and format of a message to the console.</p> <p>The third element <code>pipelines</code> defines the real processing logic. We define a pipeline called <code>consume_avro</code>, which takes messages from <code>ksml_sensordata_avro</code> and passes them to <code>print_message</code>.</p> <p>The definition file is parsed by KSML and translated into a Kafka Streams topology, which is described as follows:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; inspect_inspect_pipelines_consume_avro\n    Processor: inspect_inspect_pipelines_consume_avro (stores: [])\n      --&gt; none\n      &lt;-- ksml_sensordata_avro\n</code></pre> <p>And the output of the generated topology looks like this:</p> <pre><code>2024-03-06T18:31:57,196Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:57,631Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,082Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,528Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,970Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:59,412Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n</code></pre> <p>As you can see, the output of the application is exactly that what we defined it to be in the <code>log_message</code> function, namely a dump of all data found on the topic.</p>"},{"location":"introduction/#example-2-copying-data-to-another-topic","title":"Example 2. Copying data to another topic","text":"<p>Now that we can see what data is on a topic, we will start to manipulate its routing. In this example we are copying unmodified data to a secondary topic:</p> <pre><code>streams:\n  - topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n  - topic: ksml_sensordata_copy\n    keyType: string\n    valueType: avro:SensorData\n\nfunctions:\n  # Log the message using the built-in log variable that is passed in from Java\n  log_message:\n    type: forEach\n    parameters:\n      - name: format\n        type: string\n    code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value)\n\npipelines:\n  # Every pipeline logs its own message, passing in the format parameter to log_message above\n  consume_avro:\n    from: sensor_source_avro\n    via:\n      - type: peek\n        forEach:\n          code: log_message(key, value, format=\"AVRO\")\n    to: sensor_copy\n</code></pre> <p>You can see that we specified a second stream named <code>sensor_copy</code>  in this example, which is backed by the topic <code>ksml_sensordata_copy</code> target topic. The <code>log_message</code> function is unchanged, but the pipeline did undergo some changes. Two new elements are introduced here, namely <code>via</code> and <code>to</code>.</p> <p>The <code>via</code> tag allows users to define a series of operations executed on the data. In this case there is only one, namely a <code>peek</code> operation which does not modify any data, but simply outputs the data on stdout as a side effect.</p> <p>The <code>to</code> operation is a so-called \"sink operation\". Sink operations are always last in a pipeline. Processing of the pipeline does not continue after it was delivered to a sink operation. Note that in the first example above <code>forEach</code> is also a sink operation, whereas in this example we achieve the same result by passing the <code>log_message</code> function as a parameter to the <code>peek</code> operation.</p> <p>When this definition is translated by KSML, the following Kafka Streams topology is created:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; inspect_inspect_pipelines_consume_avro_via_1\n    Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: [])\n      --&gt; inspect_inspect_ToOperationParser_001\n      &lt;-- ksml_sensordata_avro\n    Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_copy)\n      &lt;-- inspect_inspect_pipelines_consume_avro_via_1\n</code></pre> <p>The output is similar to that of example 1, but the same data can also be found on the <code>ksml_sensordata_copy</code> topic now.</p>"},{"location":"introduction/#example-3-filtering-data","title":"Example 3. Filtering data","text":"<p>Now that we can read and write data, let's see if we can apply some logic to the processing as well. In this example we will be filtering data based on the contents of the value:</p> <pre><code># This example shows how to read from four simple streams and log all messages\n\nstreams:\n  sensor_source_avro:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n  sensor_filtered:\n    topic: ksml_sensordata_filtered\n    keyType: string\n    valueType: avro:SensorData\n\n\nfunctions:\n  # Log the message using the built-in log variable that is passed in from Java\n  log_message:\n    type: forEach\n    parameters:\n      - name: format\n        type: string\n    code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value)\n\n  sensor_is_blue:\n    type: predicate\n    code: |\n      if value[\"color\"] == \"blue\":\n        return True\n    expression: False\n\npipelines:\n  # Every pipeline logs its own message, passing in the format parameter to log_message above\n  consume_avro:\n    from: sensor_source_avro\n    via:\n      - type: filter\n        if: sensor_is_blue\n      - type: peek\n        forEach:\n          code: log_message(key, value, format=\"AVRO\")\n    to: sensor_filtered\n</code></pre> <p>Again, first we define the streams and the functions involved in the processing. You can see we added a new function called <code>filter_message</code> which returns <code>true</code> or <code>false</code> based on the <code>color</code> field in the value of the message. This function is used below in the pipeline.</p> <p>The pipeline is extended to include a <code>filter</code> operation, which takes a <code>predicate</code> function as parameter. That function is called for every input message. Only messages for which the function returns <code>true</code> are propagated. All other messages are discarded.</p> <p>Using this definition, KSML generates the following Kafka Streams topology:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; inspect_inspect_pipelines_consume_avro_via_1\n    Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: [])\n      --&gt; inspect_inspect_pipelines_consume_avro_via_2\n      &lt;-- ksml_sensordata_avro\n    Processor: inspect_inspect_pipelines_consume_avro_via_2 (stores: [])\n      --&gt; inspect_inspect_ToOperationParser_001\n      &lt;-- inspect_inspect_pipelines_consume_avro_via_1\n    Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_filtered)\n      &lt;-- inspect_inspect_pipelines_consume_avro_via_2\n</code></pre> <p>When it executes, we see the following output:</p> <pre><code>2024-03-06T18:45:10,401Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:45:10,735Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:45:11,215Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:45:11,484Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:45:11,893Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:45:12,008Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n</code></pre> <p>As you can see, the filter operation did its work. Only messages with field <code>color</code> set to <code>blue</code> are passed on to the <code>peek</code> operation, while other messages are discarded.</p>"},{"location":"introduction/#example-4-branching-messages","title":"Example 4. Branching messages","text":"<p>Another way to filter messages is to use a <code>branch</code> operation. This is also a sink operation, which closes the processing of a pipeline. It is similar to <code>forEach</code> and <code>to</code> in that respect, but has a different definition and behaviour.</p> <pre><code>streams:\n  sensor_source:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n  sensor_blue:\n    topic: ksml_sensordata_blue\n    keyType: string\n    valueType: avro:SensorData\n  sensor_red:\n    topic: ksml_sensordata_red\n    keyType: string\n    valueType: avro:SensorData\n\npipelines:\n  main:\n    from: sensor_source\n    via:\n      - type: peek\n        forEach:\n          code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value)\n    branch:\n      - if:\n          expression: value is not None and value[\"color\"] == \"blue\"\n        to: sensor_blue\n      - if:\n          expression: value is not None and value[\"color\"] == \"red\"\n        to: sensor_red\n      - forEach:\n          code: log.warn(\"UNKNOWN COLOR - {}\", value[\"color\"])\n</code></pre> <p>The <code>branch</code> operation takes a list of branches as its parameters, which each specifies a processing pipeline of its own. Branches contain the keyword <code>if</code>, which take a predicate function that determines if a message will flow into that particular branch, or if it will be passed to the next branch(es). Every message will only end up in one branch, namely the first one in order where the <code>if</code> predicate function returns <code>true</code>.</p> <p>In the example we see that the first branch will be populated only with messages with <code>color</code> field set to <code>blue</code>. Once there, these messages will be written to <code>ksml_sensordata_blue</code>. The second branch will only contain messages with <code>color</code>=<code>red</code> and these messages will be written to <code>ksml_sensordata_red</code>. Finally, the last branch outputs a message that the color is unknown and ends any further processing.</p> <p>When translated by KSML the following Kafka Streams topology is set up:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; branch_branch_pipelines_main_via_1\n    Processor: branch_branch_pipelines_main_via_1 (stores: [])\n      --&gt; branch_branch_branch_001\n      &lt;-- ksml_sensordata_avro\n    Processor: branch_branch_branch_001 (stores: [])\n      --&gt; branch_branch_branch_001-predicate-0, branch_branch_branch_001-predicate-1, branch_branch_branch_001-predicate-2\n      &lt;-- branch_branch_pipelines_main_via_1\n    Processor: branch_branch_branch_001-predicate-0 (stores: [])\n      --&gt; branch_branch_ToOperationParser_001\n      &lt;-- branch_branch_branch_001\n    Processor: branch_branch_branch_001-predicate-1 (stores: [])\n      --&gt; branch_branch_ToOperationParser_002\n      &lt;-- branch_branch_branch_001\n    Processor: branch_branch_branch_001-predicate-2 (stores: [])\n      --&gt; branch_branch_pipelines_main_branch_3\n      &lt;-- branch_branch_branch_001\n    Sink: branch_branch_ToOperationParser_001 (topic: ksml_sensordata_blue)\n      &lt;-- branch_branch_branch_001-predicate-0\n    Sink: branch_branch_ToOperationParser_002 (topic: ksml_sensordata_red)\n      &lt;-- branch_branch_branch_001-predicate-1\n    Processor: branch_branch_pipelines_main_branch_3 (stores: [])\n      --&gt; none\n      &lt;-- branch_branch_branch_001-predicate-2\n</code></pre> <p>It is clear that the branch operation is integrated in this topology. Its output looks like this:</p> <pre><code>2024-03-06T18:31:57,196Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:57,631Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,082Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,528Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,529Z WARN  k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black\n2024-03-06T18:31:58,970Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T18:31:58,972Z WARN  k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black\n2024-03-06T18:31:59,412Z INFO  k.f.branch_pipelines_main_via_1_forEach  SOURCE MESSAGE - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n</code></pre> <p>We see that every message processed by the pipeline is logged through the <code>k.f.branch_pipelines_main_via_1_forEach</code> logger. But the branch operation sorts the messages and sends messages with colors <code>blue</code> and <code>red</code> into their own branches. The only colors that show up as <code>UNKNOWN COLOR -</code> messages are non-blue and non-red and send through the <code>branch_pipelines_main_branch_3_forEach</code> logger.</p>"},{"location":"introduction/#example-5-dynamic-routing","title":"Example 5. Dynamic routing","text":"<p>Sometimes it is necessary to route a message to one stream or another based on the content of a message. This example shows how to route messages dynamically using a TopicNameExtractor.</p> <pre><code>streams:\n  sensor_source:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n\n\npipelines:\n  main:\n    from: sensor_source\n    via:\n      - type: peek\n        forEach:\n          code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value)\n    to:\n      topicNameExtractor:\n        code: |\n          if key == 'sensor1':\n            return 'ksml_sensordata_sensor1'\n          if key == 'sensor2':\n            return 'ksml_sensordata_sensor2'\n          return 'ksml_sensordata_sensor0'\n</code></pre> <p>The <code>topicNameExtractor</code> operation takes a function, which determines the routing of every message by returning a topic name string. In this case, when the key of a message is <code>sensor1</code> then the message will be sent to <code>ksml_sensordata_sensor1</code>. When it contains <code>sensor2</code> the message is sent to <code>ksml_sensordata_sensor2</code>. All other messages are sent to <code>ksml_sensordata_sensor0</code>.</p> <p>The equivalent Kafka Streams topology looks like this:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; route_route_pipelines_main_via_1\n    Processor: route_route_pipelines_main_via_1 (stores: [])\n      --&gt; route_route_ToOperationParser_001\n      &lt;-- ksml_sensordata_avro\n    Sink: route_route_ToOperationParser_001 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@5d28e108)\n      &lt;-- route_route_pipelines_main_via_1\n</code></pre> <p>The output does not show anything special compared to previous examples, since all messages are simply written by the logger.</p>"},{"location":"introduction/#example-6-multiple-pipelines","title":"Example 6. Multiple pipelines","text":"<p>In the previous examples there was always a single pipeline definition for processing data. KSML allows us to define multiple pipelines in a single file.</p> <p>In this example we combine the filtering example with the routing example. We will also define new pipelines with the sole purpose of logging the routed messages.</p> <pre><code># This example shows how to route messages to a dynamic topic. The target topic is the result of an executed function.\n\nstreams:\n  sensor_source_avro:\n    topic: ksml_sensordata_avro\n    keyType: string\n    valueType: avro:SensorData\n  sensor_filtered:\n    topic: ksml_sensordata_filtered\n    keyType: string\n    valueType: avro:SensorData\n  sensor_0:\n    topic: ksml_sensordata_sensor0\n    keyType: string\n    valueType: avro:SensorData\n  sensor_1:\n    topic: ksml_sensordata_sensor1\n    keyType: string\n    valueType: avro:SensorData\n  sensor_2:\n    topic: ksml_sensordata_sensor2\n    keyType: string\n    valueType: avro:SensorData\n\nfunctions:\n  # Only pass the message to the next step in the pipeline if the color is blue\n  sensor_is_blue:\n    type: predicate\n    code: |\n      if value[\"color\"] == \"blue\":\n        return True\n    expression: False\n\npipelines:\n  filtering:\n    from: sensor_source_avro\n    via:\n      - type: filter\n        if: sensor_is_blue\n    to: sensor_filtered\n\n  routing:\n    from: sensor_filtered\n    via:\n      - type: peek\n        forEach:\n          code: log.info(\"Routing Blue sensor - key={}, value={}\", key, value)\n    to:\n      topicNameExtractor:\n        code: |\n          if key == 'sensor1':\n            return 'ksml_sensordata_sensor1'\n          if key == 'sensor2':\n            return 'ksml_sensordata_sensor2'\n          return 'ksml_sensordata_sensor0'\n\n  sensor0_peek:\n    from: sensor_0\n    forEach:\n      code: log.info(\"SENSOR0 - key={}, value={}\", key, value)\n\n  sensor1_peek:\n    from: sensor_1\n    forEach:\n      code: log.info(\"SENSOR1 - key={}, value={}\", key, value)\n\n  sensor2_peek:\n    from: sensor_2\n    forEach:\n      code: log.info(\"SENSOR2 - key={}, value={}\", key, value)\n</code></pre> <p>In this definition we defined five pipelines:</p> <ol> <li><code>filtering</code> which filters out all sensor messages that don't have the color blue and sends it to    the <code>sensor_filtered</code> stream.</li> <li><code>routing</code> which routes the data on the <code>sensor_filtered</code> stream to one of three target topics</li> <li><code>sensor0_peek</code> which writes the content of the <code>sensor_0</code> stream to the console</li> <li><code>sensor1_peek</code> which writes the content of the <code>sensor_1</code> stream to the console</li> <li><code>sensor2_peek</code> which writes the content of the <code>sensor_2</code> stream to the console</li> </ol> <p>The equivalent Kafka Streams topology looks like this:</p> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro])\n      --&gt; multiple_multiple_pipelines_filtering_via_1\n    Processor: multiple_multiple_pipelines_filtering_via_1 (stores: [])\n      --&gt; multiple_multiple_ToOperationParser_001\n      &lt;-- ksml_sensordata_avro\n    Sink: multiple_multiple_ToOperationParser_001 (topic: ksml_sensordata_filtered)\n      &lt;-- multiple_multiple_pipelines_filtering_via_1\n\n  Sub-topology: 1\n    Source: ksml_sensordata_filtered (topics: [ksml_sensordata_filtered])\n      --&gt; multiple_multiple_pipelines_routing_via_1\n    Processor: multiple_multiple_pipelines_routing_via_1 (stores: [])\n      --&gt; multiple_multiple_ToOperationParser_002\n      &lt;-- ksml_sensordata_filtered\n    Sink: multiple_multiple_ToOperationParser_002 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@2700f556)\n      &lt;-- multiple_multiple_pipelines_routing_via_1\n\n  Sub-topology: 2\n    Source: ksml_sensordata_sensor0 (topics: [ksml_sensordata_sensor0])\n      --&gt; multiple_multiple_pipelines_sensor0_peek\n    Processor: multiple_multiple_pipelines_sensor0_peek (stores: [])\n      --&gt; none\n      &lt;-- ksml_sensordata_sensor0\n\n  Sub-topology: 3\n    Source: ksml_sensordata_sensor1 (topics: [ksml_sensordata_sensor1])\n      --&gt; multiple_multiple_pipelines_sensor1_peek\n    Processor: multiple_multiple_pipelines_sensor1_peek (stores: [])\n      --&gt; none\n      &lt;-- ksml_sensordata_sensor1\n\n  Sub-topology: 4\n    Source: ksml_sensordata_sensor2 (topics: [ksml_sensordata_sensor2])\n      --&gt; multiple_multiple_pipelines_sensor2_peek\n    Processor: multiple_multiple_pipelines_sensor2_peek (stores: [])\n      --&gt; none\n      &lt;-- ksml_sensordata_sensor2\n</code></pre> <p>And this is what the output would look something like this. The sensor peeks messages will not always be shown immediately after the Routing messages. This is because the pipelines are running in separate sub processes.</p> <pre><code>2024-03-06T20:11:39,520Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,523Z INFO  k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,533Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755889834, 'type': 'LENGTH', 'unit': 'm', 'value': '609', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,535Z INFO  k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755817913, 'type': 'STATE', 'unit': 'state', 'value': 'on', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,539Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,5419Z INFO  k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,546Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,549Z INFO  k.f.route2_pipelines_sensor1_peek_forEach SENSOR2 - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,552Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,555Z INFO  k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,558Z INFO  k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:11:39,562Z INFO  k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n</code></pre>"},{"location":"ksml-language-spec/","title":"TopologyDefinition","text":"<p>KSML definition</p>"},{"location":"ksml-language-spec/#properties","title":"Properties","text":"<ul> <li><code>functions</code> (object): (optional) Functions that can be referenced in producers and pipelines. Can contain additional properties.</li> <li>Additional properties<ul> <li>Any of</li> <li>object: Refer to #/$defs/AggregatorDefinition.</li> <li>object: Refer to #/$defs/ForEachActionDefinition.</li> <li>object: Refer to #/$defs/ForeignKeyExtractorDefinition.</li> <li>object: Refer to #/$defs/GeneratorDefinition.</li> <li>object: Refer to #/$defs/GenericFunctionDefinitionWithImplicitStoreType.</li> <li>object: Refer to #/$defs/InitializerDefinition.</li> <li>object: Refer to #/$defs/KeyTransformerDefinition.</li> <li>object: Refer to #/$defs/KeyValueMapperDefinition.</li> <li>object: Refer to #/$defs/KeyValuePrinterDefinition.</li> <li>object: Refer to #/$defs/KeyValueToKeyValueListTransformerDefinition.</li> <li>object: Refer to #/$defs/KeyValueToValueListTransformerDefinition.</li> <li>object: Refer to #/$defs/KeyValueTransformerDefinition.</li> <li>object: Refer to #/$defs/MergerDefinition.</li> <li>object: Refer to #/$defs/MetadataTransformerDefinition.</li> <li>object: Refer to #/$defs/PredicateDefinition.</li> <li>object: Refer to #/$defs/ReducerDefinition.</li> <li>object: Refer to #/$defs/StreamPartitionerDefinition.</li> <li>object: Refer to #/$defs/TimestampExtractorDefinition.</li> <li>object: Refer to #/$defs/TopicNameExtractorDefinition.</li> <li>object: Refer to #/$defs/ValueJoinerDefinition.</li> <li>object: Refer to #/$defs/ValueTransformerDefinition.</li> </ul> </li> <li><code>globalTables</code> (object): (optional) GlobalTables that can be referenced in producers and pipelines. Can contain additional properties.</li> <li>Additional properties (object): Refer to #/$defs/GlobalTableDefinition.</li> <li><code>pipelines</code> (object): (optional) Collection of named pipelines. Can contain additional properties.</li> <li>Additional properties (object): Refer to #/$defs/PipelineDefinition.</li> <li><code>producers</code> (object): (optional) Collection of named producers. Can contain additional properties.</li> <li>Additional properties (object): Refer to #/$defs/ProducerDefinition.</li> <li><code>stores</code> (object): (optional) State stores that can be referenced in producers and pipelines. Can contain additional properties.</li> <li>Additional properties<ul> <li>Any of</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinition.</li> <li>object: Refer to #/$defs/SessionStateStoreDefinition.</li> <li>object: Refer to #/$defs/WindowStateStoreDefinition.</li> </ul> </li> <li><code>streams</code> (object): (optional) Streams that can be referenced in producers and pipelines. Can contain additional properties.</li> <li>Additional properties (object): Refer to #/$defs/StreamDefinition.</li> <li><code>tables</code> (object): (optional) Tables that can be referenced in producers and pipelines. Can contain additional properties.</li> <li>Additional properties (object): Refer to #/$defs/TableDefinition.</li> </ul>"},{"location":"ksml-language-spec/#definitions","title":"Definitions","text":"<ul> <li><code>AggregateOperation</code> (object): An aggregate operation. Cannot contain additional properties.</li> <li><code>adder</code>: (optional) (GroupedTable) A function that adds a record to the aggregation result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/AggregatorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>aggregator</code>: (optional) (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/AggregatorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>initializer</code>: The initializer function, which generates an initial value for every set of aggregated records.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/InitializerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>merger</code>: (optional) (SessionWindowedStream, SessionWindowedCogroupedStream) A function that combines two aggregation results.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/MergerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: Materialized view of the result aggregation.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/SessionStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>subtractor</code>: (optional) (GroupedTable) A function that removes a record from the aggregation result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/AggregatorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"aggregate\"]</code>.</li> <li><code>AggregatorDefinition</code> (object): Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the aggregator.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the aggregator.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"aggregator\"]</code>.</li> <li><code>AggregatorDefinitionWithImplicitStoreType</code> (object): Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the aggregator.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the aggregator.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined.</li> <li><code>BranchDefinitionWithPipeline</code> (object): Defines a branch with sub-pipeline in a BranchOperation. Cannot contain additional properties.</li> <li><code>as</code> (string): (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines.</li> <li><code>branch</code> (array): (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate.<ul> <li>Items (object): Refer to #/$defs/StringOrInlinePredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>forEach</code>: (optional) A function that gets called for every message in the stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ForEachActionDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>if</code>: (optional) Defines the condition under which messages get sent down this branch.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>print</code> (object): (optional) The specification of where to print messages to. Refer to #/$defs/PrintOperation.</li> <li><code>to</code>: (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ToTopicDefinition.</li> </ul> </li> <li><code>toTopicNameExtractor</code>: (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ToTopicNameExtractorDefinition.</li> </ul> </li> <li><code>via</code> (array): (optional) A series of operations performed on the input stream.<ul> <li>Items</li> <li>Any of<ul> <li>object: Refer to #/$defs/AggregateOperation.</li> <li>object: Refer to #/$defs/CogroupOperation.</li> <li>object: Refer to #/$defs/ConvertKeyOperation.</li> <li>object: Refer to #/$defs/ConvertKeyValueOperation.</li> <li>object: Refer to #/$defs/ConvertValueOperation.</li> <li>object: Refer to #/$defs/CountOperation.</li> <li>object: Refer to #/$defs/FilterNotOperation.</li> <li>object: Refer to #/$defs/FilterOperation.</li> <li>object: Refer to #/$defs/GroupByKeyOperation.</li> <li>object: Refer to #/$defs/GroupByOperation.</li> <li>object: Refer to #/$defs/JoinWithGlobalTableOperation.</li> <li>object: Refer to #/$defs/JoinWithStreamOperation.</li> <li>object: Refer to #/$defs/JoinWithTableOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithGlobalTableOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithStreamOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithTableOperation.</li> <li>object: Refer to #/$defs/MergeOperation.</li> <li>object: Refer to #/$defs/OuterJoinWithStreamOperation.</li> <li>object: Refer to #/$defs/OuterJoinWithTableOperation.</li> <li>object: Refer to #/$defs/PeekOperation.</li> <li>object: Refer to #/$defs/ReduceOperationWithAdderAndSubtractor.</li> <li>object: Refer to #/$defs/ReduceOperationWithReducer.</li> <li>object: Refer to #/$defs/RepartitionOperation.</li> <li>object: Refer to #/$defs/SuppressOperationUntilTimeLimit.</li> <li>object: Refer to #/$defs/SuppressOperationUntilWindowCloses.</li> <li>object: Refer to #/$defs/ToStreamOperation.</li> <li>object: Refer to #/$defs/ToTableOperation.</li> <li>object: Refer to #/$defs/TransformKeyOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueToKeyValueListOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueToValueListOperation.</li> <li>object: Refer to #/$defs/TransformMetadataOperation.</li> <li>object: Refer to #/$defs/TransformValueOperation.</li> <li>object: Refer to #/$defs/WindowBySessionOperation.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithHoppingWindow.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithSlidingWindow.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithTumblingWindow.</li> </ul> </li> </ul> </li> <li><code>CogroupOperation</code> (object): A cogroup operation. Cannot contain additional properties.</li> <li><code>aggregator</code>: (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/AggregatorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"cogroup\"]</code>.</li> <li><code>ConvertKeyOperation</code> (object): An operation to convert the stream key type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties.</li> <li><code>into</code> (string, required): The type to convert the stream key into.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"convertKey\"]</code>.</li> <li><code>ConvertKeyValueOperation</code> (object): An operation to convert the stream key and value types to other types. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties.</li> <li><code>into</code> (string, required): The tuple type to convert the stream key/value into.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"convertKeyValue\"]</code>.</li> <li><code>ConvertValueOperation</code> (object): An operation to convert the stream value type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties.</li> <li><code>into</code> (string, required): The type to convert the stream value into.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"convertValue\"]</code>.</li> <li><code>CountOperation</code> (object): Count the number of times a key is seen in a given window. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the count operation's result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/SessionStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"count\"]</code>.</li> <li><code>FilterNotOperation</code> (object): Filter records based on the inverse result of a predicate function. Cannot contain additional properties.</li> <li><code>if</code>: A function that returns \"false\" when records are accepted, \"true\" otherwise.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the filtered table (only applies to tables, ignored for streams).<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"filterNot\"]</code>.</li> <li><code>FilterOperation</code> (object): Filter records based on a predicate function. Cannot contain additional properties.</li> <li><code>if</code>: A function that returns \"true\" when records are accepted, \"false\" otherwise.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the filtered table (only applies to tables, ignored for streams).<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"filter\"]</code>.</li> <li><code>ForEachActionDefinition</code> (object): Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the foreach action.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the foreach action.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"forEach\"]</code>.</li> <li><code>ForEachActionDefinitionWithImplicitStoreType</code> (object): Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the foreach action.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the foreach action.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>ForeignKeyExtractorDefinition</code> (object): Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the foreign key extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the foreign key extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"foreignKeyExtractor\"]</code>.</li> <li><code>ForeignKeyExtractorDefinitionWithImplicitStoreType</code> (object): Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the foreign key extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the foreign key extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined.</li> <li><code>GeneratorDefinition</code> (object): Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the message generator.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the message generator. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the message generator.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"generator\"]</code>.</li> <li><code>GeneratorDefinitionWithImplicitStoreType</code> (object): Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the message generator.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the message generator. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the message generator.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined.</li> <li><code>GenericFunctionDefinitionWithImplicitStoreType</code> (object): Defines a generic function function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the generic function.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the generic function. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the generic function. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the generic function. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the generic function.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the generic function. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"generic\"]</code>.</li> <li><code>GlobalTableDefinition</code> (object): Contains a definition of a globalTable, which can be referenced by producers and pipelines. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the globalTable.</li> <li><code>offsetResetPolicy</code> (string): (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted).</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) KeyValue state store definition.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>timestampExtractor</code>: (optional) A function that extracts the event time from a consumed record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TimestampExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic for this globalTable.</li> <li><code>valueType</code> (string): (optional) The value type of the globalTable.</li> <li><code>GlobalTableDefinitionAsJoinTarget</code> (object): Reference to a globalTable in a join operation. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the globalTable.</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) KeyValue state store definition.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic for this globalTable.</li> <li><code>valueType</code> (string): (optional) The value type of the globalTable.</li> <li><code>GroupByKeyOperation</code> (object): Operation to group all messages with the same key together. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the grouped stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"groupByKey\"]</code>.</li> <li><code>GroupByOperation</code> (object): Operation to group all messages with together based on a keying function. Cannot contain additional properties.</li> <li><code>mapper</code>: Function to map records to a key they can be grouped on.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueMapperDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the grouped stream or table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"groupBy\"]</code>.</li> <li><code>InitializerDefinition</code> (object): Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the initializer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the initializer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the initializer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"initializer\"]</code>.</li> <li><code>InitializerDefinitionWithImplicitStoreType</code> (object): Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the initializer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the initializer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the initializer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined.</li> <li><code>JoinWithGlobalTableOperation</code> (object): Operation to join with a table. Cannot contain additional properties.</li> <li><code>globalTable</code>: A reference to the globalTable, or an inline definition of the globalTable to join with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/GlobalTableDefinitionAsJoinTarget.</li> </ul> </li> <li><code>mapper</code>: A function that maps the key value from the stream to the primary key type of the globalTable.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueMapperDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"join\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>JoinWithStreamOperation</code> (object): Operation to join with a stream. Cannot contain additional properties.</li> <li><code>grace</code>: (optional) The window grace period (the time to admit out-of-order events after the end of the window).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>otherStore</code>: Materialized view of the joined stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>stream</code>: A reference to the stream, or an inline definition of the stream to join with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamDefinitionAsJoinTarget.</li> </ul> </li> <li><code>thisStore</code>: Materialized view of the source stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>timeDifference</code>: The maximum time difference for a join over two streams on the same key.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"join\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>JoinWithTableOperation</code> (object): Operation to join with a table. Cannot contain additional properties.</li> <li><code>foreignKeyExtractor</code>: (optional) A function that can translate the join table value to a primary key.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ForeignKeyExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>grace</code>: (optional) The window grace period (the time to admit out-of-order events after the end of the window).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>otherPartitioner</code>: (optional) A function that partitions the records on the join table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>partitioner</code>: (optional) A function that partitions the records on the primary table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) Materialized view of the joined table (only used for Table-Table joins).<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>table</code>: A reference to the table, or an inline definition of the table to join with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TableDefinitionAsJoinTarget.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"join\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>KeyTransformerDefinition</code> (object): Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the key transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the key transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyTransformer\"]</code>.</li> <li><code>KeyTransformerDefinitionWithImplicitStoreType</code> (object): Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the key transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the key transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>KeyValueMapperDefinition</code> (object): Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue mapper.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue mapper.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyValueMapper\"]</code>.</li> <li><code>KeyValueMapperDefinitionWithImplicitStoreType</code> (object): Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue mapper.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue mapper.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined.</li> <li><code>KeyValuePrinterDefinition</code> (object): Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue printer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue printer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyValuePrinter\"]</code>.</li> <li><code>KeyValuePrinterDefinitionWithImplicitStoreType</code> (object): Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue printer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue printer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined.</li> <li><code>KeyValueStateStoreDefinition</code> (object): Definition of a keyValue state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>historyRetention</code>: (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>keyType</code> (string): (optional) The key type of the keyValue store.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise.</li> <li><code>segmentInterval</code>: (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"keyValue\"]</code>.</li> <li><code>valueType</code> (string): (optional) The value type of the keyValue store.</li> <li><code>versioned</code> (boolean): (optional) \"true\" if elements in the store are versioned, \"false\" otherwise.</li> <li><code>KeyValueStateStoreDefinitionWithImplicitKeyAndValueType</code> (object): Definition of a keyValue state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>historyRetention</code>: (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise.</li> <li><code>segmentInterval</code>: (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"keyValue\"]</code>.</li> <li><code>versioned</code> (boolean): (optional) \"true\" if elements in the store are versioned, \"false\" otherwise.</li> <li><code>KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType</code> (object): Definition of a keyValue state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>historyRetention</code>: (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise.</li> <li><code>segmentInterval</code>: (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"keyValue\"]</code>.</li> <li><code>versioned</code> (boolean): (optional) \"true\" if elements in the store are versioned, \"false\" otherwise.</li> <li><code>KeyValueToKeyValueListTransformerDefinition</code> (object): Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyValueToKeyValueListTransformer\"]</code>.</li> <li><code>KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType</code> (object): Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>KeyValueToValueListTransformerDefinition</code> (object): Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue-to-valuelist transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyValueToValueListTransformer\"]</code>.</li> <li><code>KeyValueToValueListTransformerDefinitionWithImplicitStoreType</code> (object): Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue-to-valuelist transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>KeyValueTransformerDefinition</code> (object): Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"keyValueTransformer\"]</code>.</li> <li><code>KeyValueTransformerDefinitionWithImplicitStoreType</code> (object): Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the keyvalue transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the keyvalue transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>LeftJoinWithGlobalTableOperation</code> (object): Operation to leftJoin with a globalTable. Cannot contain additional properties.</li> <li><code>globalTable</code>: A reference to the globalTable, or an inline definition of the globalTable to join with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/GlobalTableDefinitionAsJoinTarget.</li> </ul> </li> <li><code>mapper</code>: A function that maps the key value from the stream with the primary key of the globalTable.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueMapperDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"leftJoin\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>LeftJoinWithStreamOperation</code> (object): Operation to leftJoin with a stream. Cannot contain additional properties.</li> <li><code>grace</code>: (optional) The window grace period (the time to admit out-of-order events after the end of the window).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>otherStore</code>: Materialized view of the leftJoined stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>stream</code>: A reference to the stream, or an inline definition of the stream to leftJoin with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamDefinitionAsJoinTarget.</li> </ul> </li> <li><code>thisStore</code>: Materialized view of the source stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>timeDifference</code>: The maximum time difference for a leftJoin over two streams on the same key.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"leftJoin\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>LeftJoinWithTableOperation</code> (object): Operation to leftJoin with a table. Cannot contain additional properties.</li> <li><code>foreignKeyExtractor</code>: (optional) A function that can translate the join table value to a primary key.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ForeignKeyExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>grace</code>: (optional) The window grace period (the time to admit out-of-order events after the end of the window).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>otherPartitioner</code>: (optional) A function that partitions the records on the join table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>partitioner</code>: (optional) A function that partitions the records on the primary table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) Materialized view of the leftJoined table (only used for Table-Table joins).<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>table</code>: A reference to the table, or an inline definition of the table to join with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TableDefinitionAsJoinTarget.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"leftJoin\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>MergeOperation</code> (object): A merge operation to join two Streams. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>stream</code>: The stream to merge with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamDefinition.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"merge\"]</code>.</li> <li><code>MergerDefinition</code> (object): Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the merger.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the merger. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the merger.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the merger. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"merger\"]</code>.</li> <li><code>MergerDefinitionWithImplicitStoreType</code> (object): Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the merger.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the merger. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the merger.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the merger. Only required for function types, which are not pre-defined.</li> <li><code>MetadataTransformerDefinition</code> (object): Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the metadata transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the metadata transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"metadataTransformer\"]</code>.</li> <li><code>MetadataTransformerDefinitionWithImplicitStoreType</code> (object): Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the metadata transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the metadata transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>OuterJoinWithStreamOperation</code> (object): Operation to outerJoin with a stream. Cannot contain additional properties.</li> <li><code>grace</code>: (optional) The window grace period (the time to admit out-of-order events after the end of the window).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>otherStore</code>: Materialized view of the outerJoined stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>stream</code>: A reference to the stream, or an inline definition of the stream to outerJoin with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamDefinitionAsJoinTarget.</li> </ul> </li> <li><code>thisStore</code>: Materialized view of the source stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>timeDifference</code>: The maximum time difference for an outerJoin over two streams on the same key.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"outerJoin\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>OuterJoinWithTableOperation</code> (object): Operation to outerJoin with a table. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the outerJoined table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>table</code>: A reference to the table, or an inline definition of the table to outerJoin with.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TableDefinitionAsJoinTarget.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"outerJoin\"]</code>.</li> <li><code>valueJoiner</code>: A function that joins two values.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueJoinerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>ParameterDefinition</code> (object): Defines a parameter for a user function. Cannot contain additional properties.</li> <li><code>defaultValue</code> (string): (optional) The default value for the parameter.</li> <li><code>name</code> (string, required): The name of the parameter.</li> <li><code>type</code> (string, required): The type of the parameter.</li> <li><code>PeekOperation</code> (object): Operation to peek into a stream, without modifying the stream contents. Cannot contain additional properties.</li> <li><code>forEach</code>: A function that gets called for every message in the stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ForEachActionDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"peek\"]</code>.</li> <li><code>PipelineDefinition</code> (object): Defines a pipeline through a source, a series of operations to perform on it and a sink operation to close the stream with. Cannot contain additional properties.</li> <li><code>as</code> (string): (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines.</li> <li><code>branch</code> (array): (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate.<ul> <li>Items (object): Refer to #/$defs/BranchDefinitionWithPipeline.</li> </ul> </li> <li><code>forEach</code>: (optional) A function that gets called for every message in the stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ForEachActionDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>from</code>: Pipeline source.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TopicDefinitionSource.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>print</code> (object): (optional) The specification of where to print messages to. Refer to #/$defs/PrintOperation.</li> <li><code>to</code>: (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ToTopicDefinition.</li> </ul> </li> <li><code>toTopicNameExtractor</code>: (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ToTopicNameExtractorDefinition.</li> </ul> </li> <li><code>via</code> (array): (optional) A series of operations performed on the input stream.<ul> <li>Items</li> <li>Any of<ul> <li>object: Refer to #/$defs/AggregateOperation.</li> <li>object: Refer to #/$defs/CogroupOperation.</li> <li>object: Refer to #/$defs/ConvertKeyOperation.</li> <li>object: Refer to #/$defs/ConvertKeyValueOperation.</li> <li>object: Refer to #/$defs/ConvertValueOperation.</li> <li>object: Refer to #/$defs/CountOperation.</li> <li>object: Refer to #/$defs/FilterNotOperation.</li> <li>object: Refer to #/$defs/FilterOperation.</li> <li>object: Refer to #/$defs/GroupByKeyOperation.</li> <li>object: Refer to #/$defs/GroupByOperation.</li> <li>object: Refer to #/$defs/JoinWithGlobalTableOperation.</li> <li>object: Refer to #/$defs/JoinWithStreamOperation.</li> <li>object: Refer to #/$defs/JoinWithTableOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithGlobalTableOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithStreamOperation.</li> <li>object: Refer to #/$defs/LeftJoinWithTableOperation.</li> <li>object: Refer to #/$defs/MergeOperation.</li> <li>object: Refer to #/$defs/OuterJoinWithStreamOperation.</li> <li>object: Refer to #/$defs/OuterJoinWithTableOperation.</li> <li>object: Refer to #/$defs/PeekOperation.</li> <li>object: Refer to #/$defs/ReduceOperationWithAdderAndSubtractor.</li> <li>object: Refer to #/$defs/ReduceOperationWithReducer.</li> <li>object: Refer to #/$defs/RepartitionOperation.</li> <li>object: Refer to #/$defs/SuppressOperationUntilTimeLimit.</li> <li>object: Refer to #/$defs/SuppressOperationUntilWindowCloses.</li> <li>object: Refer to #/$defs/ToStreamOperation.</li> <li>object: Refer to #/$defs/ToTableOperation.</li> <li>object: Refer to #/$defs/TransformKeyOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueToKeyValueListOperation.</li> <li>object: Refer to #/$defs/TransformKeyValueToValueListOperation.</li> <li>object: Refer to #/$defs/TransformMetadataOperation.</li> <li>object: Refer to #/$defs/TransformValueOperation.</li> <li>object: Refer to #/$defs/WindowBySessionOperation.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithHoppingWindow.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithSlidingWindow.</li> <li>object: Refer to #/$defs/WindowByTimeOperationWithTumblingWindow.</li> </ul> </li> </ul> </li> <li><code>PredicateDefinition</code> (object): Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the predicate.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the predicate. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the predicate.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"predicate\"]</code>.</li> <li><code>PredicateDefinitionWithImplicitStoreType</code> (object): Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the predicate.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the predicate. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the predicate.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>PrintOperation</code> (object): Operation to print the contents of a pipeline on the screen or to write them to a file. Cannot contain additional properties.</li> <li><code>filename</code> (string): (optional) The filename to output records to. If nothing is specified, then messages will be printed on stdout.</li> <li><code>label</code> (string): (optional) A label to attach to the output records.</li> <li><code>mapper</code>: (optional) A function to convert record into a string for output.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValuePrinterDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>ProducerDefinition</code> (object): Definition of a Producer that regularly generates messages for a topic. Cannot contain additional properties.</li> <li><code>batchSize</code> (integer): (optional) The size of batches.</li> <li><code>condition</code>: (optional) A function that validates the generator's result message. Returns \"true\" when the message may be produced on the topic, \"false\" otherwise.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>count</code> (integer): (optional) The number of messages to produce.</li> <li><code>generator</code>: The function that generates records.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/GeneratorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>interval</code>: (optional) The interval with which the generator is called.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>to</code>: The topic to produce to.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TopicDefinition.</li> </ul> </li> <li><code>until</code>: (optional) A predicate that returns true to indicate producing should stop.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>ReduceOperationWithAdderAndSubtractor</code> (object): Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties.</li> <li><code>adder</code>: A function that adds a record to the aggregate result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ReducerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the aggregation.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>subtractor</code>: A function that removes a record from the aggregate result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ReducerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"reduce\"]</code>.</li> <li><code>ReduceOperationWithReducer</code> (object): Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>reducer</code>: A function that computes a new aggregate result.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ReducerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) Materialized view of the aggregation.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/SessionStateStoreDefinitionWithImplicitKeyAndValueType.</li> <li>object: Refer to #/$defs/WindowStateStoreDefinitionWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"reduce\"]</code>.</li> <li><code>ReducerDefinition</code> (object): Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the reducer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the reducer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the reducer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"reducer\"]</code>.</li> <li><code>ReducerDefinitionWithImplicitStoreType</code> (object): Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the reducer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the reducer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the reducer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined.</li> <li><code>RepartitionOperation</code> (object): Operation to (re)partition a stream. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>numberOfPartitions</code> (integer): (optional) The target number of partitions.</li> <li><code>partitioner</code>: (optional) A function that partitions stream records.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"repartition\"]</code>.</li> <li><code>SessionStateStoreDefinition</code> (object): Definition of a session state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>keyType</code> (string, required): The key type of the session store.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the session store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise.</li> <li><code>retention</code>: (optional) The duration for which elements in the session store are retained.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"session\"]</code>.</li> <li><code>valueType</code> (string, required): The value type of the session store.</li> <li><code>SessionStateStoreDefinitionWithImplicitKeyAndValueType</code> (object): Definition of a session state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the session store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise.</li> <li><code>retention</code>: (optional) The duration for which elements in the session store are retained.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"session\"]</code>.</li> <li><code>StreamDefinition</code> (object): Contains a definition of a Stream, which can be referenced by producers and pipelines. Cannot contain additional properties.</li> <li><code>keyType</code> (string, required): The key type of the stream.</li> <li><code>offsetResetPolicy</code> (string): (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted).</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>timestampExtractor</code>: (optional) A function extracts the event time from a consumed record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TimestampExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic for this stream.</li> <li><code>valueType</code> (string, required): The value type of the stream.</li> <li><code>StreamDefinitionAsJoinTarget</code> (object): Reference to a Stream in a join or merge operation. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the stream.</li> <li><code>topic</code> (string, required): The name of the Kafka topic for this stream.</li> <li><code>valueType</code> (string): (optional) The value type of the stream.</li> <li><code>StreamPartitionerDefinition</code> (object): Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the stream partitioner.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the stream partitioner.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"streamPartitioner\"]</code>.</li> <li><code>StreamPartitionerDefinitionWithImplicitStoreType</code> (object): Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the stream partitioner.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the stream partitioner.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined.</li> <li><code>StringOrInlinePredicateDefinitionWithImplicitStoreType</code> (object): Defines the condition under which messages get sent down this branch. Cannot contain additional properties.</li> <li><code>if</code>: (optional) Defines the condition under which messages get sent down this branch.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/PredicateDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>SuppressOperationUntilTimeLimit</code> (object): Operation to suppress messages in the source stream until a time limit is reached. Cannot contain additional properties.</li> <li><code>bufferFullStrategy</code>: (optional) What to do when the buffer is full. Must be one of: <code>[\"emitEarlyWhenFull\", \"shutdownWhenFull\"]</code>.</li> <li><code>duration</code>: The duration for which messages are suppressed.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>maxBytes</code> (string): (optional) The maximum number of bytes in the buffer.</li> <li><code>maxRecords</code> (string): (optional) The maximum number of records in the buffer.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"suppress\"]</code>.</li> <li><code>until</code>: The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: <code>[\"timeLimit\"]</code>.</li> <li><code>SuppressOperationUntilWindowCloses</code> (object): Operation to suppress messages in the source stream until a window limit is reached. Cannot contain additional properties.</li> <li><code>bufferFullStrategy</code>: (optional) What to do when the buffer is full. Must be one of: <code>[\"emitEarlyWhenFull\", \"shutdownWhenFull\"]</code>.</li> <li><code>maxBytes</code> (string): (optional) The maximum number of bytes in the buffer.</li> <li><code>maxRecords</code> (string): (optional) The maximum number of records in the buffer.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"suppress\"]</code>.</li> <li><code>until</code>: The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: <code>[\"windowCloses\"]</code>.</li> <li><code>TableDefinition</code> (object): Contains a definition of a table, which can be referenced by producers and pipelines. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the table.</li> <li><code>offsetResetPolicy</code> (string): (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted).</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) KeyValue state store definition.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>timestampExtractor</code>: (optional) A function that extracts the event time from a consumed record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TimestampExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic for this table.</li> <li><code>valueType</code> (string): (optional) The value type of the table.</li> <li><code>TableDefinitionAsJoinTarget</code> (object): Reference to a table in a join operation. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the table.</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>store</code>: (optional) KeyValue state store definition.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic for this table.</li> <li><code>valueType</code> (string): (optional) The value type of the table.</li> <li><code>TimestampExtractorDefinition</code> (object): Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the timestamp extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the timestamp extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"timestampExtractor\"]</code>.</li> <li><code>TimestampExtractorDefinitionWithImplicitStoreType</code> (object): Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the timestamp extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the timestamp extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined.</li> <li><code>ToStreamOperation</code> (object): Convert a Table into a Stream, optionally through a custom key transformer. Cannot contain additional properties.</li> <li><code>mapper</code>: (optional) A function that computes the output key for every record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"toStream\"]</code>.</li> <li><code>ToTableOperation</code> (object): Convert a Stream into a Table. Cannot contain additional properties.</li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the result table.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"toTable\"]</code>.</li> <li><code>ToTopicDefinition</code> (object): Writes out pipeline messages to a topic. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the topic.</li> <li><code>partitioner</code>: (optional) A function that partitions the records in the output topic.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic.</li> <li><code>valueType</code> (string): (optional) The value type of the topic.</li> <li><code>ToTopicNameExtractorDefinition</code> (object): Writes out pipeline messages to a topic as given by a topic name extractor. Cannot contain additional properties.</li> <li><code>partitioner</code>: (optional) A function that partitions the records in the output topic.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topicNameExtractor</code>: Reference to a pre-defined topic name extractor, or an inline definition of a topic name extractor.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TopicNameExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>TopicDefinition</code> (object): Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties.</li> <li><code>keyType</code> (string): (optional) The key type of the topic.</li> <li><code>topic</code> (string, required): The name of the Kafka topic.</li> <li><code>valueType</code> (string): (optional) The value type of the topic.</li> <li><code>TopicDefinitionSource</code> (object): Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties.</li> <li><code>keyType</code> (string, required): The key type of the topic.</li> <li><code>offsetResetPolicy</code> (string): (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted).</li> <li><code>partitioner</code>: (optional) A function that determines to which topic partition a given message needs to be written.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/StreamPartitionerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>timestampExtractor</code>: (optional) A function extracts the event time from a consumed record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/TimestampExtractorDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>topic</code> (string, required): The name of the Kafka topic.</li> <li><code>valueType</code> (string, required): The value type of the topic.</li> <li><code>TopicNameExtractorDefinition</code> (object): Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the topic name extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the topic name extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"topicNameExtractor\"]</code>.</li> <li><code>TopicNameExtractorDefinitionWithImplicitStoreType</code> (object): Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the topic name extractor.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the topic name extractor.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined.</li> <li><code>TransformKeyOperation</code> (object): Convert the key of every record in the stream to another key. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that computes a new key for each record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"transformKey\", \"mapKey\", \"selectKey\"]</code>.</li> <li><code>TransformKeyValueOperation</code> (object): Convert the key/value of every record in the stream to another key/value. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that computes a new key/value for each record.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"map\", \"transformKeyValue\"]</code>.</li> <li><code>TransformKeyValueToKeyValueListOperation</code> (object): Convert a stream by transforming every record into a list of derived records. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that converts every record of a stream to a list of output records.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"transformKeyValueToKeyValueList\", \"flatMap\"]</code>.</li> <li><code>TransformKeyValueToValueListOperation</code> (object): Convert every record in the stream to a list of output records with the same key. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that converts every key/value into a list of result values, each of which will be combined with the original key to form a new message in the output stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueToValueListTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"transformKeyValueToValueList\", \"flatMapValues\"]</code>.</li> <li><code>TransformMetadataOperation</code> (object): Convert the metadata of every record in the stream. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that converts the metadata (Kafka headers, timestamp) of every record in the stream.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/MetadataTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"transformMetadata\"]</code>.</li> <li><code>TransformValueOperation</code> (object): Convert the value of every record in the stream to another value. Cannot contain additional properties.</li> <li><code>mapper</code>: A function that converts the value of every record into another value.<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/ValueTransformerDefinitionWithImplicitStoreType.</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>store</code>: (optional) Materialized view of the transformed table (only applies to tables, ignored for streams).<ul> <li>Any of</li> <li>string</li> <li>object: Refer to #/$defs/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType.</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"mapValue\", \"transformValue\", \"mapValues\"]</code>.</li> <li><code>ValueJoinerDefinition</code> (object): Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the value joiner.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the value joiner.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined.</li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"valueJoiner\"]</code>.</li> <li><code>ValueJoinerDefinitionWithImplicitStoreType</code> (object): Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the value joiner.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the value joiner.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined.</li> <li><code>ValueTransformerDefinition</code> (object): Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the value transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the value transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>type</code>: The type of the function. Must be one of: <code>[\"valueTransformer\"]</code>.</li> <li><code>ValueTransformerDefinitionWithImplicitStoreType</code> (object): Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties.</li> <li><code>code</code>: (optional) The (multiline) code of the value transformer.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>expression</code>: (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>globalCode</code>: (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables.<ul> <li>Any of</li> <li>boolean</li> <li>integer</li> <li>number</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context.</li> <li><code>parameters</code> (array): (optional) A list of parameters to be passed into the value transformer.<ul> <li>Items (object): Refer to #/$defs/ParameterDefinition.</li> </ul> </li> <li><code>resultType</code> (string): (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined.</li> <li><code>stores</code> (array): (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store.<ul> <li>Items (string)</li> </ul> </li> <li><code>WindowBySessionOperation</code> (object): Operation to window messages by session, configured by an inactivity gap. Cannot contain additional properties.</li> <li><code>grace</code>: (optional) (Tumbling, Hopping) The grace period, during which out-of-order records can still be processed.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>inactivityGap</code>: The inactivity gap, below which two messages are considered to be of the same session.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"windowBySession\"]</code>.</li> <li><code>WindowByTimeOperationWithHoppingWindow</code> (object): Operation to window records based on time criteria. Cannot contain additional properties.</li> <li><code>advanceBy</code>: The amount of time to increase time windows by.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>duration</code>: The duration of time windows.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>grace</code>: (optional) The grace period, during which out-of-order records can still be processed.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"windowByTime\"]</code>.</li> <li><code>windowType</code>: The windowType of the time window. Must be one of: <code>[\"hopping\"]</code>.</li> <li><code>WindowByTimeOperationWithSlidingWindow</code> (object): Operation to window records based on time criteria. Cannot contain additional properties.</li> <li><code>grace</code>: (optional) The grace period, during which out-of-order records can still be processed.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>timeDifference</code>: The maximum amount of time difference between two records.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"windowByTime\"]</code>.</li> <li><code>windowType</code>: The windowType of the time window. Must be one of: <code>[\"sliding\"]</code>.</li> <li><code>WindowByTimeOperationWithTumblingWindow</code> (object): Operation to window records based on time criteria. Cannot contain additional properties.</li> <li><code>duration</code>: The duration of time windows.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>grace</code>: (optional) The grace period, during which out-of-order records can still be processed.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>name</code> (string): (optional) The name of the operation processor.</li> <li><code>type</code>: The type of the operation. Must be one of: <code>[\"windowByTime\"]</code>.</li> <li><code>windowType</code>: The windowType of the time window. Must be one of: <code>[\"tumbling\"]</code>.</li> <li><code>WindowStateStoreDefinition</code> (object): Definition of a window state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>keyType</code> (string, required): The key type of the window store.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the window store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise.</li> <li><code>retainDuplicates</code> (boolean): (optional) Whether or not to retain duplicates.</li> <li><code>retention</code>: (optional) The duration for which elements in the window store are retained.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"window\"]</code>.</li> <li><code>valueType</code> (string, required): The value type of the window store.</li> <li><code>windowSize</code>: (optional) Size of the windows (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>WindowStateStoreDefinitionWithImplicitKeyAndValueType</code> (object): Definition of a window state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the window store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise.</li> <li><code>retainDuplicates</code> (boolean): (optional) Whether or not to retain duplicates.</li> <li><code>retention</code>: (optional) The duration for which elements in the window store are retained.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"window\"]</code>.</li> <li><code>windowSize</code>: (optional) Size of the windows (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>WindowStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType</code> (object): Definition of a window state store. Cannot contain additional properties.</li> <li><code>caching</code> (boolean): (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly.</li> <li><code>logging</code> (boolean): (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise.</li> <li><code>name</code> (string): (optional) The name of the window store. If this field is not defined, then the name is derived from the context.</li> <li><code>persistent</code> (boolean): (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise.</li> <li><code>retainDuplicates</code> (boolean): (optional) Whether or not to retain duplicates.</li> <li><code>retention</code>: (optional) The duration for which elements in the window store are retained.<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> <li><code>timestamped</code> (boolean): (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise.</li> <li><code>type</code>: The type of the state store. Must be one of: <code>[\"window\"]</code>.</li> <li><code>windowSize</code>: (optional) Size of the windows (cannot be negative).<ul> <li>Any of</li> <li>integer</li> <li>string</li> </ul> </li> </ul>"},{"location":"notations/","title":"Notations","text":""},{"location":"notations/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Avro</li> <li>CSV</li> <li>JSON</li> <li>SOAP</li> <li>XML</li> </ol>"},{"location":"notations/#introduction","title":"Introduction","text":"<p>KSML is able to express its internal data types in a number of external representations. Internally these are called notations. The different notations are described below.</p>"},{"location":"notations/#avro","title":"AVRO","text":"<p>Avro types are supported through the \"avro\" prefix in types. The notation is <code>avro:schema</code>, where schema is the schema fqdn, or just the schema name itself.</p> <p>On Kafka topics, Avro types are serialized in binary format. Internally they are represented as structs.</p> <p>Examples:</p> <pre><code>avro:SensorData\navro:io.axual.ksml.example.SensorData\n</code></pre> <p>Note: when referencing an AVRO schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .avsc file extension.</p>"},{"location":"notations/#csv","title":"CSV","text":"<p>Comma-separated values are supported through the \"csv\" prefix in types. The notation is <code>csv:schema</code>, where schema is the schema fqdn, or just the schema name itself.</p> <p>On Kafka topics, CSV types are serialized as <code>string</code>. Internally they are represented as structs.</p> <p>Examples:</p> <pre><code>csv:SensorData\ncsv:io.axual.ksml.example.SensorData\n</code></pre> <p>Note: when referencing an CSV schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .csv file extension.</p>"},{"location":"notations/#json","title":"JSON","text":"<p>JSON types are supported through the \"json\" prefix in types. The notation is <code>json:schema</code>, where <code>schema</code> is the schema fqdn, or just the schema name itself.</p> <p>On Kafka topics, JSON types are serialized as <code>string</code>. Internally they are represented as structs or lists.</p> <p>Examples:</p> <pre><code>json:SensorData\njson:io.axual.ksml.example.SensorData\n</code></pre> <p>If you want to use JSON without a schema, you can leave out the colon and schema name:</p> <pre><code>json\n</code></pre> <p>Note: when referencing an JSON schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .json file extension.</p>"},{"location":"notations/#soap","title":"SOAP","text":"<p>SOAP is supported through built-in serializers and deserializers. The representation on Kafka will always be <code>string</code>. Internally SOAP objects are structs with their own schema. Field names are derived from the SOAP standards.</p>"},{"location":"notations/#xml","title":"XML","text":"<p>XML is supported through built-in serializers and deserializers. The representation on Kafka will always be <code>string</code>. Internally XML objects are structs.</p> <p>Examples:</p> <pre><code>xml:SensorData\nxml:io.axual.ksml.example.SensorData\n</code></pre> <p>Note: when referencing an XML schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .xsd file extension.</p>"},{"location":"operations/","title":"Operations","text":""},{"location":"operations/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Operations<ul> <li>aggregate</li> <li>cogroup</li> <li>convertKey</li> <li>convertKeyValue</li> <li>convertValue</li> <li>count</li> <li>filter</li> <li>filterNot</li> <li>flatMap</li> <li>flatMapValues</li> <li>groupBy</li> <li>groupByKey</li> <li>join</li> <li>leftJoin</li> <li>map</li> <li>mapKey</li> <li>mapValue</li> <li>mapValues</li> <li>merge</li> <li>outerJoin</li> <li>peek</li> <li>reduce</li> <li>repartition</li> <li>selectKey</li> <li>suppress</li> <li>toStream</li> <li>transformKey</li> <li>transformKeyValue</li> <li>transformKeyValueToKeyValueList</li> <li>transformKeyValueToValueList</li> <li>transformValue</li> <li>windowBySession</li> <li>windowByTime</li> </ul> </li> <li>Sink Operations<ul> <li>as</li> <li>branch</li> <li>forEach</li> <li>print</li> <li>to</li> <li>toTopicNameExtractor</li> </ul> </li> </ol>"},{"location":"operations/#introduction","title":"Introduction","text":"<p>Pipelines in KSML have a beginning, a middle and (optionally) an end. Operations form the middle part of pipelines. They are modeled as separate YAML entities, where each operation takes input from the previous operation and applies its own logic. The returned stream then serves as input for the next operation.</p>"},{"location":"operations/#transform-operations","title":"Transform Operations","text":"<p>Transformations are operations that take an input stream and convert it to an output stream. This section lists all supported transformations. Each one states the type of stream it returns.</p> Parameter Value Type Description <code>name</code> string The name of the operation. <p>Note that not all combinations of output/input streams are supported by Kafka Streams. The user that writes the KSML definition needs to make sure that streams that result from one operation can actually serve as input to the next. KSML does type checking and will exit with an error when operations that can not be chained together are listed after another in the KSML definition.</p>"},{"location":"operations/#aggregate","title":"aggregate","text":"<p>This operation aggregates multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types.</p> Stream Type Returns Parameter Value Type Required Description KGroupedStream<code>&lt;K,V&gt;</code> KTable<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <code>aggregator</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. KGroupedTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <code>adder</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. <code>subtractor</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should remove the key/value from the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. SessionWindowedKStream<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>session</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <code>aggregator</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. <code>merger</code> Inline or reference Yes A Merger function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the merged result, also of type <code>V</code>. [TimeWindowedKStreamObject]<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <code>aggregator</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. [CogroupedKStream]<code>&lt;K,V&gt;</code> KTable<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. SessionWindowedCogroupedKStream<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>session</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <code>merger</code> Inline or reference Yes A Merger function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the merged result, also of type <code>V</code>. TimeWindowedCogroupedKStream<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>initializer</code> Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type <code>VR</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: aggregate\n    initializer:\n      expression: 0\n    aggregator:\n      expression: aggregatedValue + value\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#cogroup","title":"cogroup","text":"<p>This operation cogroups multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types.</p> Stream Type Returns Parameter Value Type Required Description KGroupedStream<code>&lt;K,V&gt;</code> [CogroupedKStream]<code>&lt;K,VR&gt;</code> <code>aggregator</code> Inline or reference Yes An Aggregator function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>VR</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>VR</code>. [CogroupedKStream]<code>&lt;K,V&gt;</code> n/a n/a n/a n/a This method is currently not supported in KSML. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: cogroup\n    aggregator:\n      expression: aggregatedValue + value\n  - type: toStream\nto: output_stream\n</code></pre> <p>Note: this operation was added to KSML for completion purposes, but is not considered ready or fully functional. Feel free to experiment, but don't rely on this in production. Syntax changes may occur in future KSML releases.</p>"},{"location":"operations/#convertkey","title":"convertKey","text":"<p>This built-in operation takes a message and converts the key into a given type.</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,V&gt;</code> <code>into</code> string The type to convert the key into. Conversion to <code>KR</code> is done by KSML. <p>Example:</p> <pre><code>from:\n  topic: input_stream\n  keyType: string\n  valueType: string\nvia:\n  - type: convertKey\n    into: json\nto: output_stream\n</code></pre>"},{"location":"operations/#convertkeyvalue","title":"convertKeyValue","text":"<p>This built-in operation takes a message and converts the key and value into a given type.</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,VR&gt;</code> <code>into</code> string The type to convert the key and value into. Conversion of key into <code>KR</code> and value into <code>VR</code> is done by KSML. <p>Example:</p> <pre><code>from:\n  topic: input_stream\n  keyType: string\n  valueType: string\nvia:\n  - type: convertKeyValue\n    into: (json,xml)\nto: output_stream\n</code></pre>"},{"location":"operations/#convertvalue","title":"convertValue","text":"<p>This built-in operation takes a message and converts the value into a given type.</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>into</code> string The type to convert the value into. Conversion of value into <code>VR</code> is done by KSML. <p>Example:</p> <pre><code>from:\n  topic: input_stream\n  keyType: string\n  valueType: string\nvia:\n  - type: convertValue\n    into: xml\nto: output_stream\n</code></pre>"},{"location":"operations/#count","title":"count","text":"<p>This operation counts the number of messages and returns a table multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types.</p> Stream Type Returns Parameter Value Type Required Description KGroupedStream<code>&lt;K,V&gt;</code> KTable<code>&lt;K,Long&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. KGroupedTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,Long&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. SessionWindowedKStream<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,Long&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>session</code>. [TimeWindowedKStreamObject]<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,Long&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: count\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#filter","title":"filter","text":"<p>Filter all incoming messages according to some predicate. The predicate function is called for every message. Only when the predicate returns <code>true</code>, then the message will be sent to the output stream.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,V&gt;</code> <code>if</code> Yes Inline or reference A Predicate function, which returns <code>True</code> if the message can pass the filter, <code>False</code> otherwise. KTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>if</code> Yes Inline or reference A Predicate function, which returns <code>True</code> if the message can pass the filter, <code>False</code> otherwise. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: filter\n    if:\n      expression: key.startswith('a')\nto: output_stream\n</code></pre>"},{"location":"operations/#filternot","title":"filterNot","text":"<p>This operation works exactly like filter, but negates all predicates before applying them. That means messages for which the predicate returns <code>False</code> are accepted, while those that the predicate returns <code>True</code> for are filtered out. See filter for details on how to implement.</p>"},{"location":"operations/#flatmap","title":"flatMap","text":"<p>This operation takes a message and transforms it into zero, one or more new messages, which may have different key and value types than the source.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,VR&gt;</code> <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return type should be a list of type <code>[(KR,VR)]</code> containing a list of transformed <code>key</code> and <code>value</code> pairs. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: flatMap\n    mapper:\n      expression: [ (key,value), (key,value) ]   # duplicate all incoming messages\nto: output_stream\n</code></pre>"},{"location":"operations/#flatmapvalues","title":"flatMapValues","text":"<p>This operation takes a message and transforms it into zero, one or more new values, which may have different value types than the source. Every entry in the result list is combined with the source key and produced on the output stream.</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>mapper</code> Inline or reference A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return type should be a list of type <code>[VR]</code> containing a list of transformed <code>value</code>s. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: flatMapValues\n    mapper:\n      expression: [ value+1, value+2, value+3 ]   # creates 3 new messages [key,VR] for every input message\nto: output_stream\n</code></pre>"},{"location":"operations/#groupby","title":"groupBy","text":"<p>Group the records of a stream by value resulting from a KeyValueMapper.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KGroupedStream<code>&lt;KR,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code> and returns a value of type <code>KR</code> to group the stream by. KTable<code>&lt;K,V&gt;</code> KGroupedTable<code>&lt;KR,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code> and returns a value of type <code>KR</code> to group the stream by. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper:\n      expression: value[\"some_field\"]\n      resultType: string\n  - type: aggregate\n    initializer:\n      expression: 0\n    aggregator:\n      expression: value1+value2\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#groupbykey","title":"groupByKey","text":"<p>Group the records of a stream by the stream's key.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KGroupedStream<code>&lt;K,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupByKey\n  - type: aggregate\n    initializer:\n      expression: 0\n    aggregator:\n      expression: value1+value2\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#join","title":"join","text":"<p>Join records of this stream with another stream's records using inner join. The join is computed on the records' key with join predicate <code>thisStream.key == otherStream.key</code>. If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>stream</code> <code>string</code> Yes The name of the stream to join with. The stream should be of key type <code>K</code> and value type <code>VR</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the joined value of type <code>VR</code>. <code>timeDifference</code> <code>duration</code> Yes The maximum allowed between two joined records. <code>grace</code> <code>duration</code> No A grace period during with out-of-order to-be-joined records may still arrive. KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>table</code> <code>string</code> Yes The name of the table to join with. The table should be of key type <code>K</code> and value type <code>VO</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>value1</code> of type <code>V</code> from the source table and a <code>value2</code> of type <code>VO</code> from the join table. The return value is the joined value of type <code>VR</code>. <code>grace</code> <code>duration</code> No A grace period during with out-of-order to-be-joined records may still arrive. KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>globalTable</code> <code>string</code> Yes The name of the global table to join with. The global table should be of key type <code>GK</code> and value type <code>GV</code>. <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return value is the key of type <code>GK</code> of the records from the GlobalTable to join with. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the joined value of type <code>VR</code>. KTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No The Store configuration, should be of type <code>keyValue</code>. <code>table</code> <code>string</code> Yes The name of the table to join with. The table should be of key type <code>K</code> and value type <code>VO</code>. <code>foreignKeyExtractor</code> Inline or reference No A [ForeignKeyExtractor] function, which takes a <code>value</code> of type <code>V</code>, which needs to be converted into the key type <code>KO</code> of the table to join with. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>value1</code> of type <code>V</code> from the source table and a <code>value2</code> of type <code>VO</code> from the join table. The return value is the joined value of type <code>VR</code>. <code>partitioner</code> Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. <code>otherPartitioner</code> Inline or reference No A [Partitioner] function, which partitions the records on the join table. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: join\n    stream: second_stream\n    valueJoiner: my_key_value_mapper\n    timeDifference: 1m\nto: output_stream\n</code></pre>"},{"location":"operations/#leftjoin","title":"leftJoin","text":"<p>Join records of this stream with another stream's records using left join. The join is computed on the records' key with join predicate <code>thisStream.key == otherStream.key</code>. If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>stream</code> <code>string</code> Yes The name of the stream to join with. The stream should be of key type <code>K</code> and value type <code>VR</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the joined value of type <code>VR</code>. <code>timeDifference</code> <code>duration</code> Yes The maximum allowed between two joined records. <code>grace</code> <code>duration</code> No A grace period during with out-of-order to-be-joined records may still arrive. KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>table</code> <code>string</code> Yes The name of the table to join with. The table should be of key type <code>K</code> and value type <code>VO</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>value1</code> of type <code>V</code> from the source table and a <code>value2</code> of type <code>VO</code> from the join table. The return value is the joined value of type <code>VR</code>. <code>grace</code> <code>duration</code> No A grace period during with out-of-order to-be-joined records may still arrive. KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>globalTable</code> <code>string</code> Yes The name of the global table to join with. The global table should be of key type <code>GK</code> and value type <code>GV</code>. <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return value is the key of type <code>GK</code> of the records from the GlobalTable to join with. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the joined value of type <code>VR</code>. KTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No The Store configuration, should be of type <code>keyValue</code>. <code>table</code> <code>string</code> Yes The name of the table to join with. The table should be of key type <code>K</code> and value type <code>VO</code>. <code>foreignKeyExtractor</code> Inline or reference No A [ForeignKeyExtractor] function, which takes a <code>value</code> of type <code>V</code>, which needs to be converted into the key type <code>KO</code> of the table to join with. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>value1</code> of type <code>V</code> from the source table and a <code>value2</code> of type <code>VO</code> from the join table. The return value is the joined value of type <code>VR</code>. <code>partitioner</code> Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. <code>otherPartitioner</code> Inline or reference No A [Partitioner] function, which partitions the records on the join table. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: leftJoin\n    stream: second_stream\n    valueJoiner: my_key_value_mapper\n    timeDifference: 1m\nto: output_stream\n</code></pre>"},{"location":"operations/#map","title":"map","text":"<p>This operation takes a message and transforms the key and value into a new key and value, which can each have a different type than the source message key and value.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,VR&gt;</code> <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return type should be a tuple of type <code>(KR,VR)</code> containing the transformed <code>key</code> and <code>value</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: map\n    mapper:\n      expression: (str(key), str(value))   # convert key and value from source type to string\nto: output_stream\n</code></pre>"},{"location":"operations/#mapkey","title":"mapKey","text":"<p>This is an alias for selectKey.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: mapKey\n    mapper:\n      expression: str(key)   # convert key from source type to string\nto: output_stream\n</code></pre>"},{"location":"operations/#mapvalue","title":"mapValue","text":"<p>This is an alias for mapValues.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: mapValue\n    mapper:\n      expression: str(value)   # convert value from source type to String\nto: output_stream\n</code></pre>"},{"location":"operations/#mapvalues","title":"mapValues","text":"<p>This operation takes a message and transforms its value to a new value, which may have different value type than the source.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return type should be a value of type <code>VR</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: mapValues\n    mapper:\n      expression: str(value)   # convert value from source type to String\nto: output_stream\n</code></pre>"},{"location":"operations/#merge","title":"merge","text":"<p>Merge this stream and the given stream into one larger stream. There is no ordering guarantee between records from this stream and records from the provided stream in the merged stream. Relative order is preserved within each input stream though (ie, records within one input stream are processed in order).</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,V&gt;</code> <code>stream</code> <code>string</code> The name of the stream to merge with. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: merge\n    stream: second_stream\nto: output_stream\n</code></pre>"},{"location":"operations/#outerjoin","title":"outerJoin","text":"<p>Join records of this stream with another stream's records using outer join. The join is computed on the records' key with join predicate <code>thisStream.key == otherStream.key</code>. If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>stream</code> <code>string</code> Yes The name of the stream to join with. The stream should be of key type <code>K</code> and value type <code>VR</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>key</code> of type <code>K</code>, and two values <code>value1</code> and <code>value2</code> of type <code>V</code>. The return value is the joined value of type <code>VR</code>. <code>timeDifference</code> <code>duration</code> Yes The maximum allowed between two joined records. <code>grace</code> <code>duration</code> No A grace period during with out-of-order to-be-joined records may still arrive. KTable<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>table</code> <code>string</code> Yes The name of the table to join with. The table should be of key type <code>K</code> and value type <code>VO</code>. <code>valueJoiner</code> Inline or reference Yes A [ValueJoiner] function, which takes a <code>value1</code> of type <code>V</code> from the source table and a <code>value2</code> of type <code>VO</code> from the join table. The return value is the joined value of type <code>VR</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: outerJoin\n    stream: second_stream\n    valueJoiner: my_key_value_mapper\n    timeDifference: 1m\nto: output_stream\n</code></pre>"},{"location":"operations/#peek","title":"peek","text":"<p>Perform an action on each record of a stream. This is a stateless record-by-record operation. Peek is a non-terminal operation that triggers a side effect (such as logging or statistics collection) and returns an unchanged stream.</p> Stream Type Returns Parameter Value Type Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,V&gt;</code> <code>forEach</code> Inline or reference The [ForEach] function that will be called for every message, receiving arguments <code>key</code> of type <code>K</code> and <code>value</code> of type <code>V</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: peek\n    forEach: print_key_and_value\nto: output_stream\n</code></pre>"},{"location":"operations/#reduce","title":"reduce","text":"<p>Combine the values of records in this stream by the grouped key. Records with null key or value are ignored. Combining implies that the type of the aggregate result is the same as the type of the input value, similar to aggregate(Initializer, Aggregator).</p> Stream Type Returns Parameter Value Type Required Description KGroupedStream<code>&lt;K,V&gt;</code> KTable<code>&lt;K,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>reducer</code> Inline or reference Yes A Reducer function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>V</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>V</code>. KGroupedTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>keyValue</code>. <code>adder</code> Inline or reference Yes A Reducer function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>V</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>V</code>. <code>subtractor</code> Inline or reference Yes A Reducer function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>V</code>. It should remove the key/value from the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>V</code>. SessionWindowedKStream<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>session</code>. <code>reducer</code> Inline or reference Yes A Reducer function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>V</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>V</code>. [TimeWindowedKStreamObject]<code>&lt;K,V&gt;</code> KTable<code>&lt;Windowed&lt;K&gt;,V&gt;</code> <code>store</code> Store configuration No An optional Store configuration, should be of type <code>window</code>. <code>reducer</code> Inline or reference Yes A Reducer function, which takes a <code>key</code> of type <code>K</code>, a <code>value</code> of type <code>V</code> and <code>aggregatedValue</code> of type <code>V</code>. It should add the key/value to the previously calculated <code>aggregateValue</code> and return a new aggregate value of type <code>V</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: aggregate\n    initializer:\n      expression: 0\n    aggregator:\n      expression: aggregatedValue + value\n  - type: toStream\nto: output_stream\n</code></pre> <p>Example:</p> <pre><code>[ yaml ]\n  ----\nfrom: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: reduce\n    reducer:\n      expression: value1+value2\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#repartition","title":"repartition","text":"<p>Materialize this stream to an auto-generated repartition topic with a given number of partitions, using a custom partitioner. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be automatically purged. The topic will be named as \"${applicationId}--repartition\". Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,V&gt;</code> <code>numberOfPartitions</code> integer No The number of partitions of the repartitioned topic. <code>partitioner</code> Inline or reference No A custom [Partitioner] function to partition records. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: repartition\n    name: my_partitioner\n    numberOfPartitions: 3\n    partitioner: my_own_partitioner\n  - type: peek\n    forEach: print_key_and_value\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#selectkey","title":"selectKey","text":"<p>This operation takes a message and transforms the key into a new key, which may have a different type.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,V&gt;</code> <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return value is the key of resulting stream, which is of type <code>KR</code>. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: selectKey\n    mapper:\n      expression: str(key)   # convert key from source type to string\nto: output_stream\n</code></pre>"},{"location":"operations/#suppress","title":"suppress","text":"<p>Suppress some updates from this changelog stream, determined by the supplied Suppressed configuration. When windowCloses is selected and no further restrictions are provided, then this is interpreted as Suppressed.untilWindowCloses(unbounded()).</p> Stream Type Returns Parameter Value Type Required Description KTable<code>&lt;K,V&gt;</code> KTable<code>&lt;K,V&gt;</code> <code>until</code> <code>string</code> Yes This value can either be <code>timeLimit</code> or <code>windowCloses</code>. Note that timeLimit suppression works on any stream, while windowCloses suppression works only on Windowed streams. For the latter, see [windowByTime] or [windowBySession]. <code>duration</code> <code>string</code> No The Duration to suppress updates (only when <code>until</code>==<code>timeLimit</code>) <code>maxBytes</code> <code>int</code> No The maximum number of bytes to suppress updates <code>maxRecords</code> <code>int</code> No The maximum number of records to suppress updates <code>bufferFullStrategy</code> <code>string</code> No Can be one of <code>emitEarlyWhenFull</code>, <code>shutdownWhenFull</code> <p>Example:</p> <pre><code>from: input_table\nvia:\n  - type: suppress\n    until: timeLimit\n    duration: 30s\n    maxBytes: 128000\n    maxRecords: 10000\n    bufferFullStrategy: emitEarlyWhenFull\n  - type: peek\n    forEach: print_key_and_value\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#tostream","title":"toStream","text":"<p>Convert a KTable into a KStream object.</p> Stream Type Returns Parameter Value Type Required Description KTable<code>&lt;K,V&gt;</code> KStream<code>&lt;KR,V&gt;</code> <code>mapper</code> Inline or reference No A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return value is the key of resulting stream, which is of type <code>KR</code>. If no mapper is provided, then keys remain unchanged. <p>Example:</p> <pre><code>from: input_table\nvia:\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#transformkey","title":"transformKey","text":"<p>This is an alias for selectKey.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformKey\n    mapper:\n      expression: str(key)   # convert key from source type to string\nto: output_stream\n</code></pre>"},{"location":"operations/#transformkeyvalue","title":"transformKeyValue","text":"<p>This is an alias for map.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformKeyValue\n    mapper:\n      expression: (str(key), str(value))   # convert key and value from source type to string\nto: output_stream\n</code></pre>"},{"location":"operations/#transformkeyvaluetokeyvaluelist","title":"transformKeyValueToKeyValueList","text":"<p>This is an alias for flatMap.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformKeyValueToKeyValueList\n    mapper:\n      expression: [ (key,value), (key,value) ]   # duplicate all incoming messages\nto: output_stream\n</code></pre>"},{"location":"operations/#transformkeyvaluetovaluelist","title":"transformKeyValueToValueList","text":"<p>This is an alias for flapMapValues.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformKeyValueToValueList\n    mapper:\n      expression: [ value+1, value+2, value+3 ]   # creates 3 new messages [key,VR] for every input message\nto: output_stream\n</code></pre>"},{"location":"operations/#transformmetadata","title":"transformMetadata","text":"<p>This operation takes a message and transforms its value to a new value, which may have different value type than the source.</p> Stream Type Returns Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> KStream<code>&lt;K,VR&gt;</code> <code>mapper</code> Inline or reference Yes A [MetadataTransformer] function that converts the metadata (Kafka headers, timestamp) of every record in the stream. It gets a metadata object as input and should return the same type, but potentially with modified fields. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformValue\n    mapper:\n      expression: str(value)   # convert value from source type to String\nto: output_stream\n</code></pre>"},{"location":"operations/#transformvalue","title":"transformValue","text":"<p>This is an alias for mapValues.</p> <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: transformValue\n    mapper:\n      expression: str(value)   # convert value from source type to String\nto: output_stream\n</code></pre>"},{"location":"operations/#windowbysession","title":"windowBySession","text":"<p>Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page].</p> Stream Type Returns Parameter Value Type Required Description KGroupedStream<code>&lt;K,V&gt;</code> SessionWindowedKStream<code>&lt;K,V&gt;</code> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. [CogroupedKStream]<code>&lt;K,V&gt;</code> SessionWindowedCogroupedKStream<code>&lt;K,V&gt;</code> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: windowedBySession\n    inactivityGap: 1h\n    grace: 5m\n  - type: reduce\n    reducer: my_reducer_function\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#windowbytime","title":"windowByTime","text":"<p>Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page].</p> Stream Type Returns Parameter Value Type Description KGroupedStream<code>&lt;K,V&gt;</code> TimeWindowedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>sliding</code>. timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. KGroupedStream<code>&lt;K,V&gt;</code> TimeWindowedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>hopping</code>. advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration, which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. KGroupedStream<code>&lt;K,V&gt;</code> TimeWindowedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>tumbling</code>. duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream]<code>&lt;K,V&gt;</code> TimeWindowedCogroupedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>sliding</code>. timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. [CogroupedKStream]<code>&lt;K,V&gt;</code> TimeWindowedCogroupedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>hopping</code>. advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration, which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream]<code>&lt;K,V&gt;</code> TimeWindowedCogroupedKStream<code>&lt;K,V&gt;</code> <code>windowType</code> <code>string</code> Fixed value <code>tumbling</code>. duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. <p>Example:</p> <pre><code>from: input_stream\nvia:\n  - type: groupBy\n    mapper: my_mapper_function\n  - type: windowedBy\n    windowType: time\n    duration: 1h\n    advanceBy: 15m\n    grace: 5m\n  - type: reduce\n    reducer: my_reducer_function\n  - type: toStream\nto: output_stream\n</code></pre>"},{"location":"operations/#sink-operations","title":"Sink Operations","text":""},{"location":"operations/#as","title":"as","text":"<p>Pipelines closed of with <code>as</code> can be referred by other pipelines as their starting reference. This allows for a common part of processing logic to be placed in its own pipeline in KSML, serving as an intermediate result.</p> Applies to Value Type Required Description Any pipeline<code>&lt;K,V&gt;</code> string Yes The name under which the pipeline result can be referenced by other pipelines. <p>Example:</p> <pre><code>pipelines:\n  first:\n    from: some_source_topic\n    via:\n      - type: ...\n    as: first_pipeline\n\n  second:\n    from: first_pipeline\n    via:\n      - type: ...\n    to: ...\n</code></pre> <p>Here, the first pipeline ends by sending its output to a stream internally called <code>first_pipeline</code>. This stream is used as input for the <code>second</code> pipeline.</p>"},{"location":"operations/#branch","title":"branch","text":"<p>Branches out messages from the input stream into several branches based on predicates. Each branch is defined as a list item below the branch operation. Branch predicates are defined using the <code>if</code> keyword. Messages are only processed by one of the branches, namely the first one for which the predicate returns <code>true</code>.</p> Applies to Value Type Required Description KStream<code>&lt;K,V&gt;</code> List of branch definitions Yes See for description of branch definitions below. <p>Branches in KSML are nested pipelines, which are parsed without the requirement of a source attribute. Each branch accepts the following parameters:</p> Branch element Value Type Required Description <code>if</code> Inline Predicate or reference No The Predicate function that determines if the message is sent down this branch, or is passed on to the next branch in line. Inline All pipeline parameters, see [Pipeline] Yes The inlined pipeline describes the topology of the specific branch. <p>Example:</p> <pre><code>from: some_source_topic\nbranch:\n  - if:\n      expression: value['color'] == 'blue'\n    to: ksml_sensordata_blue\n  - if:\n      expression: value['color'] == 'red'\n    to: ksml_sensordata_red\n  - forEach:\n      code: |\n        print('Unknown color sensor: '+value[\"color\"])\n</code></pre> <p>In this example, the first two branches are entered if the respective predicate matches (the color attribute of value matches a certain color). If the predicate returns <code>false</code>, then the next predicate/branch is tried. Only the last branch in the list can be a sink operation.</p>"},{"location":"operations/#foreach","title":"forEach","text":"<p>This sends each message to a custom defined function. This function is expected to handle each message as its final step. The function does not (need to) return anything.</p> Applies to Value Type Description KStream<code>&lt;K,V&gt;</code> Inline or reference The [ForEach] function that is called for every record on the source stream. Its arguments are <code>key</code> of type <code>K</code> and <code>value</code> of type <code>V</code>. <p>Examples:</p> <pre><code>forEach: my_foreach_function\n</code></pre> <pre><code>forEach:\n  code: print(value)\n</code></pre>"},{"location":"operations/#print","title":"print","text":"<p>This sends each message to a custom defined print function. This function is expected to handle each message as the final in the pipeline. The function does not (need to) return anything.</p> <p>As target, you can specify a filename. If none is specified, then all messages are printed to stdout.</p> Applies to Parameter Value Type Required Description KStream<code>&lt;K,V&gt;</code> filename string No The filename to output records to. If nothing is specified, then messages will be printed on stdout. label string No A label to attach to every output record. <code>mapper</code> Inline or reference Yes A KeyValueMapper function, which takes a <code>key</code> of type <code>K</code> and a <code>value</code> of type <code>V</code>. The return value should be of type <code>string</code> and is sent to the specified file or stdout. <p>Examples:</p> <pre><code>from: source\nvia:\n  - type: ...\nprint:\n  filename: file.txt\n  mapper:\n    expression: \"record value: \" + str(value)\n</code></pre>"},{"location":"operations/#to","title":"to","text":"<p>Messages are sent directly to a named <code>Stream</code>.</p> Applies to Value Type Required Description KStream<code>&lt;K,V&gt;</code> Inline [Topic] or reference to a stream, table or global table Yes The name of a defined stream. <p>Examples:</p> <pre><code>to: my_target_topic\n</code></pre> <pre><code>from: source\nvia:\n  - type: ...\nto:\n  topic: my_target_topic\n  keyType: someType\n  valueType: someOtherType\n  partitioner:\n    expression: hash_of(key)\n</code></pre>"},{"location":"operations/#totopicnameextractor","title":"toTopicNameExtractor","text":"<p>Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. This operation acts as a Sink and is always the last operation in a pipeline.</p> Applies to Value Type Required Description KStream<code>&lt;K,V&gt;</code> Inline or reference Yes The [TopicNameExtractor] function that is called for every message and returns the topic name to which the message shall be written. <p>Examples:</p> <pre><code>toTopicNameExtractor: my_extractor_function\n</code></pre> <pre><code>toTopicNameExtractor:\n  code: |\n    if key == 'sensor1':\n      return 'ksml_sensordata_sensor1'\n    elif key == 'sensor2':\n      return 'ksml_sensordata_sensor2'\n    else:\n      return 'ksml_sensordata_sensor0'\n</code></pre>"},{"location":"pipelines/","title":"Pipeline","text":""},{"location":"pipelines/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Definition</li> <li>Elements<ul> <li>Source</li> <li>Operations</li> <li>Sink</li> </ul> </li> </ol>"},{"location":"pipelines/#introduction","title":"Introduction","text":"<p>Pipelines form the heart of KSML streams logic. They take one or more input streams and apply processing logic to them. Output is passed on from operation to operation.</p>"},{"location":"pipelines/#definition","title":"Definition","text":"<p>Pipelines are contained in the <code>pipelines</code> section. Each pipeline has a name, which is the name of the YAML tag.</p> <p>As an example, the following defines a pipeline called <code>copy_pipeline</code>, which consumes messages from <code>some_input_stream</code> and outputs the same messages to <code>some_output_stream</code>.</p> <pre><code>pipelines:\n  copy_pipeline:\n    from: some_input_stream\n    to: some_output_stream\n</code></pre>"},{"location":"pipelines/#elements","title":"Elements","text":"<p>All pipelines contain three elements:</p>"},{"location":"pipelines/#source","title":"Source","text":"<p>The source of a pipeline is marked with the <code>from</code> keyword. The value of the YAML node is the name of a defined <code>Stream</code>, <code>Table</code> or <code>GlobalTable</code>. See Streams for more information.</p>"},{"location":"pipelines/#operations","title":"Operations","text":"<p>Once a source was selected, a list of operations can be applied to the input. The list is started through the keyword <code>via</code>, below which a list of operations is defined.</p> <p>Example:</p> <pre><code>pipelines:\n  copy_pipeline:\n    from: some_input_stream\n    via:\n      - type: peek\n        forEach: my_peek_function\n      - type: transformKeyValue\n        mapper: my_transform_function\n    to: some_output_stream\n</code></pre> <p>Each operation must have a <code>type</code> field, which indicates the type of operations applied to the input. See Operations for a full list of operations that can be applied.</p>"},{"location":"pipelines/#sinks","title":"Sinks","text":"<p>After all transformation operations are applied, a pipeline can define a sink to which all messages are sent. There are four sink types in KSML:</p> Sink type Description <code>as</code> Allows the pipeline result to be saved under an internal name, which can later be referenced. Pipelines defined after this point may refer to this name in their <code>from</code> statement. <code>branch</code> This statement allows the pipeline to be split up in several branches. Each branch filters messages with an <code>if</code> statement. Messages will be processed only by the first branch of which the <code>if</code> statement is true. <code>forEach</code> Sends every message to a function, without expecting any return type. Because there is no return type, the pipeline always stops after this statement. <code>print</code> Prints out every message according to a given output specification. <code>to</code> Sends all output messages to a specific target. This target can be a pre-defined <code>stream</code>, <code>table</code> or <code>globalTable</code>, or an inline-defined topic. <code>toTopicNameExtractor</code> Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. <p>For more information, see the respective documentation on pipeline definitions in the definitions section of the KSML language spec.</p>"},{"location":"pipelines/#duration","title":"Duration","text":"<p>Some pipeline operations require specifying durations. Durations can be expressed as strings with the following syntax:</p> <pre><code>###x\n</code></pre> <p>where <code>#</code> is a positive number between 0 and 999999 and <code>x</code> is an optional letter from the following table:</p> Letter Description none Duration in milliseconds s Duration in seconds m Duration in minutes h Duration in hours d Duration in days w Duration in weeks <p>Examples:</p> <pre><code>100 ==&gt; hundred milliseconds\n30s ==&gt; thirty seconds\n8h ==&gt; eight hours\n2w ==&gt; two weeks\n</code></pre> <p>Note that durations are not a data type that can used as key or value on a Kafka topic.</p>"},{"location":"quick-start/","title":"Quick start","text":""},{"location":"quick-start/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Starting a demo setup</li> <li>Starting a KSML runner</li> <li>Next steps</li> </ol>"},{"location":"quick-start/#introduction","title":"Introduction","text":"<p>KSML comes with example definitions, which contain a producer that outputs SensorData messages to Kafka, and several pipelines, which each independently consume and process the produced messages.</p>"},{"location":"quick-start/#starting-a-demo-setup","title":"Starting a demo setup","text":"<p>After checking out the repository, go to the KSML directory and execute the following:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will start Zookeeper, Kafka and a Schema Registry in the background. It will also start the demo producer, which outputs two random messages per second on a <code>ksml_sensordata_avro</code> topic.</p> <p>You can check the valid starting of these containers using the following command:</p> <pre><code>docker compose logs -f\n</code></pre> <p>Press CTRL-C when you verified data is produced. This typically looks like this:</p> <pre><code>example-producer-1  | 2024-03-06T20:24:49,480Z INFO  i.a.k.r.backend.KafkaProducerRunner  Calling generate_sensordata_message\nexample-producer-1  | 2024-03-06T20:24:49,480Z INFO  i.a.k.r.backend.ExecutableProducer   Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"}\nexample-producer-1  | 2024-03-06T20:24:49,481Z INFO  i.a.k.r.backend.ExecutableProducer   Produced message to ksml_sensordata_avro, partition 0, offset 1975\nexample-producer-1  | 2024-03-06T20:24:49,481Z INFO  i.a.k.r.backend.KafkaProducerRunner  Calling generate_sensordata_message\nexample-producer-1  | 2024-03-06T20:24:49,481Z INFO  i.a.k.r.backend.ExecutableProducer   Message: key=sensor3, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor3\", \"owner\":\"Dave\", \"timestamp\":\"1709756689481\", \"type\":\"STATE\", \"unit\":\"state\", \"value\":\"off\"}\nexample-producer-1  | 2024-03-06T20:24:49,483Z INFO  i.a.k.r.backend.Execut^Cestamp\":1709756689814, \"type\":\"AREA\", \"unit\":\"ft2\", \"value\":\"76\"}\nexample-producer-1  | 2024-03-06T20:24:49,815Z INFO  i.a.k.r.backend.ExecutableProducer   Produced message to ksml_sensordata_xml, partition 0, offset 1129\nexample-producer-1  | 2024-03-06T20:24:49,921Z INFO  i.a.k.r.backend.KafkaProducerRunner  Calling generate_sensordata_message\nexample-producer-1  | 2024-03-06T20:24:49,922Z INFO  i.a.k.r.backend.ExecutableProducer   Message: key=sensor6, value=SensorData: {\"city\":\"Amsterdam\", \"color\":\"yellow\", \"name\":\"sensor6\", \"owner\":\"Evan\", \"timestamp\":1709756689922, \"type\":\"AREA\", \"unit\":\"m2\", \"value\":\"245\"}\nexample-producer-1  | 2024-03-06T20:24:49,923Z INFO  i.a.k.r.backend.ExecutableProducer   Produced message to ksml_sensordata_avro, partition 0, offset 1976\nexample-producer-1  | 2024-03-06T20:24:50,035Z INFO  i.a.k.r.backend.KafkaProducerRunner  Calling generate_sensordata_message\nexample-producer-1  | 2024-03-06T20:24:50,035Z INFO  i.a.k.r.backend.ExecutableProducer   Message: key=sensor7, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor7\", \"owner\":\"Dave\", \"timestamp\":\"1709756690035\", \"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"0\"}\n</code></pre>"},{"location":"quick-start/#starting-a-ksml-runner","title":"Starting a KSML runner","text":"<p>To start a container which executes the example KSML definitions, type</p> <pre><code>./examples/run.sh\n</code></pre> <p>This will start the KSML docker container. You should see the following typical output:</p> <pre><code>2024-03-06T20:24:51,921Z INFO  io.axual.ksml.runner.KSMLRunner      Starting KSML Runner 1.76.0.0\n...\n...\n...\n...\n2024-03-06T20:24:57,196Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:24:57,631Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:24:58,082Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:24:58,528Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:24:58,970Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n2024-03-06T20:24:59,412Z INFO  ksml.functions.log_message  Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { &lt;&lt;Cleaned KSML Representation of Avro Schema&gt;&gt;}}\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next steps","text":"<p>Check out the examples in the <code>examples</code> directory of the project. By modifying the file <code>examples/ksml-runner.yaml</code> you can select the example(s) to run.</p> <p>For a more elaborate introduction, you can start here or refer to the documentation.</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#releases","title":"Releases","text":"<ul> <li>Release Notes<ul> <li>Releases<ul> <li>1.0.4 (2024-10-18)</li> <li>1.0.2 (2024-09-20)</li> <li>1.0.1 (2024-07-17)</li> <li>1.0.0 (2024-06-28)</li> <li>0.8.0 (2024-03-08)</li> <li>0.9.1 (2024-06-21)</li> <li>0.9.0 (2024-06-05)</li> <li>0.2.2 (2024-01-30)</li> <li>0.2.1 (2023-12-20)</li> <li>0.2.0 (2023-12-07)</li> <li>0.1.0 (2023-03-15)</li> <li>0.0.4 (2022-12-02)</li> <li>0.0.3 (2021-07-30)</li> <li>0.0.2 (2021-06-28)</li> <li>0.0.1 (2021-04-30)</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/#103-2024-10-18","title":"1.0.3 (2024-10-18)","text":"<ul> <li>KSML</li> <li>Fix high CPU usage</li> <li>Upgrade to Avro 1.11.4 to fix CVE-2024-47561</li> </ul>"},{"location":"release-notes/#102-2024-09-20","title":"1.0.2 (2024-09-20)","text":"<ul> <li>KSML</li> <li>Upgrade to Kafka Streams 3.8.0</li> <li>Avro Schema Registry settings no longer required if Avro not used </li> <li>Add missing object in KSML Json Schema</li> <li>Fix serialisation and list handling issues</li> <li>Helm charts</li> <li>Use liveness and readiness and startup probes to fix state issues</li> <li>Fix conflicting default configuration Prometheus export and ServiceMonitor</li> </ul>"},{"location":"release-notes/#101-2024-07-17","title":"1.0.1 (2024-07-17)","text":"<ul> <li>Topology Optimization can be applied</li> <li>Runtime dependencies, like LZ4 compression support, are back in the KSML image</li> <li>Fix parse error messages during join</li> <li>Fix windowed aggregation flow errors</li> <li>Update windowed object support in multiple operations and functions</li> </ul>"},{"location":"release-notes/#100-2024-06-28","title":"1.0.0 (2024-06-28)","text":"<ul> <li>Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language   specification. This allows for better syntax checking in IDEs.</li> <li>Lots of small fixes and completion modifications.</li> </ul>"},{"location":"release-notes/#091-2024-06-21","title":"0.9.1 (2024-06-21)","text":"<ul> <li>Fix failing test in GitHub Actions during release</li> <li>Unified build workflows</li> </ul>"},{"location":"release-notes/#090-2024-06-05","title":"0.9.0 (2024-06-05)","text":"<ul> <li>Collectable metrics</li> <li>New topology test suite</li> <li>Python context hardening</li> <li>Improved handling of Kafka tombstones</li> <li>Added flexibility to producers (single shot, n-shot, or user condition-based)</li> <li>JSON Logging support</li> <li>Bumped GraalVM to 23.1.2</li> <li>Bumped several dependency versions</li> <li>Several fixes and security updates</li> </ul>"},{"location":"release-notes/#080-2024-03-08","title":"0.8.0 (2024-03-08)","text":"<ul> <li>Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification:<ul> <li>docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness.</li> <li>examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion.</li> </ul> </li> <li>Improved schema handling:<ul> <li>Better compatibility checking between schema fields.</li> </ul> </li> <li>Improved support for state stores:<ul> <li>Update to state store typing and handling.</li> <li>Manual state stores can be defined and referenced in pipelines.</li> <li>Manual state stores are also available in Python functions.</li> <li>State stores can be used 'side-effect-free' (e.g. no AVRO schema registration)</li> </ul> </li> <li>Python function improvements:<ul> <li>Automatic variable assignment for state stores.</li> <li>Every Python function can use a Java Logger, integrating Python output with KSML log output.</li> <li>Type inference in situations where parameters or result types can be derived from the context.</li> </ul> </li> <li>Lots of small language updates:<ul> <li>Improve readability for store types, filter operations and windowing operations</li> <li>Introduction of the \"as\" operation, which allows for pipeline referencing and chaining.</li> </ul> </li> <li>Better data type handling:<ul> <li>Separation of data types and KSML core, allowing for easier addition of new data types in the future.</li> <li>Automatic conversion of data types, removing common pipeline failure scenarios.</li> <li>New implementation for CSV handling.</li> </ul> </li> <li>Merged the different runners into a single runner.<ul> <li>KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies).</li> <li>Removal of Kafka and Axual backend distinctions.</li> </ul> </li> <li>Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace).</li> <li>Examples updated to reflect the latest definition format.</li> <li>Documentation updated.</li> </ul>"},{"location":"release-notes/#022-2024-01-30","title":"0.2.2 (2024-01-30)","text":"<p>Changes:</p> <ul> <li>Fix KSML java process not stopping on exception</li> <li>Fix stream-stream join validation and align other checks</li> <li>Bump logback to 1.4.12</li> <li>Fix to enable Streams optimisations to be applied to topology</li> <li>Fix resolving admin client issues causing warning messages</li> </ul>"},{"location":"release-notes/#021-2023-12-20","title":"0.2.1 (2023-12-20)","text":"<p>Changes:</p> <ul> <li>Fixed an issue with AVRO and field validations</li> </ul>"},{"location":"release-notes/#020-2023-12-07","title":"0.2.0 (2023-12-07)","text":"<p>Changes:</p> <ul> <li>Optimized Docker build</li> <li>Merged KSML Runners into one module, optimize Docker builds and workflow</li> <li>KSML documentation updates</li> <li>Docker image, GraalPy venv, install and GU commands fail</li> <li>Update GitHub Actions</li> <li>Small robustness improvements</li> <li>Issue #72 - Fix build failures when trying to use venv and install python packages</li> <li>Manual state store support, Kafka client cleanups and configuration changes</li> <li>Update and clean up dependencies</li> <li>Update documentation to use new runner configurations</li> <li>Update to GraalVM for JDK 21 Community</li> </ul>"},{"location":"release-notes/#010-2023-03-15","title":"0.1.0 (2023-03-15)","text":"<p>Changes:</p> <ul> <li>Added XML/SOAP support</li> <li>Added data generator</li> <li>Added Automatic Type Conversion</li> <li>Added Schema Support for XML, Avro, JSON, Schema</li> <li>Added Basic Error Handling</li> </ul>"},{"location":"release-notes/#004-2022-12-02","title":"0.0.4 (2022-12-02)","text":"<p>Changes:</p> <ul> <li>Update to kafka 3.2.3</li> <li>Update to Java 17</li> <li>Support multiple architectures in KSML, linux/amd64 and linux/arm64</li> <li>Refactored internal typing system, plus some fixes to store operations</li> <li>Introduce queryable state stores</li> <li>Add better handling of NULL keys and values from Kafka</li> <li>Implement schema support</li> <li>Added Docker multistage build</li> <li>Bug fix for windowed objects</li> <li>Store improvements</li> <li>Support Liberica NIK</li> <li>Switch from Travis CI to GitHub workflow</li> <li>Build snapshot Docker image on pull request merged</li> </ul>"},{"location":"release-notes/#003-2021-07-30","title":"0.0.3 (2021-07-30)","text":"<p>Changes:</p> <ul> <li>Support for Python 3 through GraalVM</li> <li>improved data structuring</li> <li>bug fixes</li> </ul>"},{"location":"release-notes/#002-2021-06-28","title":"0.0.2 (2021-06-28)","text":"<p>Changes:</p> <ul> <li>Added JSON support, Named topology and name store supported</li> </ul>"},{"location":"release-notes/#001-2021-04-30","title":"0.0.1 (2021-04-30)","text":"<p>Changes:</p> <ul> <li>First alpha release </li> </ul>"},{"location":"runners/","title":"KSML Runner","text":""},{"location":"runners/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Configuration<ul> <li>Namespace support</li> </ul> </li> <li>Starting a container</li> </ol>"},{"location":"runners/#introduction","title":"Introduction","text":"<p>KSML is built around the <code>ksml</code> module, which provides the core functionality for parsing KSML definition files and converting them into Kafka Streams topologies. To keep KSML lightweight and flexible, it does not execute these topologies directly.</p> <p>Instead, execution is handled by the <code>ksml-runner</code> module \u2014 a standalone Java application that loads KSML configurations and runs the corresponding Kafka Streams applications.</p> <p>The <code>ksml-runner</code> supports standard Kafka configurations and includes advanced features for working with Kafka clusters that use namespacing (e.g., tenant-aware deployments).</p> <p>Example runner configurations are provided below.</p>"},{"location":"runners/#configuration","title":"Configuration","text":"<p>The configuration file passed to the KSML runner is in YAML format and should contain at least the following:</p> <pre><code>ksml:\n  applicationServer:                           # The application server is currently only offering REST querying of state stores\n    enabled: true                              # true if you want to enable REST querying of state stores\n    host: 0.0.0.0                              # by default listen on all interfaces\n    port: 8080                                 # port to listen on\n  configDirectory: /ksml/config                # Location of the KSML definitions. Default is the current working directory\n  schemaDirectory: /ksml/schemas               # Location of the schema definitions. Default is the config directory\n  storageDirectory: /ksml/data                 # Where the stateful data is written. Defaults is the default JVM temp directory\n  errorHandling:                               # how to handle errors\n    consume:\n      log: true                                # log errors\n      logPayload: true                         # log message payloads upon error\n      loggerName: ConsumeError                 # logger name\n      handler: continueOnFail                  # continue or stop on error\n    process:\n      log: true                                # log errors\n      logPayload: true                         # log message payloads upon error\n      loggerName: ProcessError                 # logger name\n      handler: stopOnFail                      # continue or stop on error\n    produce:\n      log: true                                # log errors\n      logPayload: true                         # log message payloads upon error\n      loggerName: ProduceError                 # logger name\n      handler: continueOnFail                  # continue or stop on error\n    enableProducers: true                      # False to disable producers in the KSML definition\n    enablePipelines: true                      # False to disable pipelines in the KSML definition\n    definitions:                               # KSML definition files from the working directory\n      namedDefinition1: definition1.yaml\n      namedDefinition2: definition2.yaml\n      namedDefinition3: &lt;more here...&gt;\n\nkafka:                                         # Kafka streams configuration options \n  application.id: io.ksml.example.processor\n  bootstrap.servers: broker-1:9092,broker-2:9092\n  security.protocol: SSL\n  ssl.protocol: TLSv1.3\n  ssl.enabled.protocols: TLSv1.3,TLSv1.2\n  ssl.endpoint.identification.algorithm: \"\"\n  ssl.truststore.location: /ksml/config/truststore.jks\n  ssl.truststore.password: password-for-truststore\n\n  # Schema Registry client configuration, needed when schema registry is used\n  schema.registry.url: http://schema-registry:8083\n  schema.registry.ssl.truststore.location: /ksml/config/truststore.jks\n  schema.registry.ssl.truststore.password: password-for-truststore\n</code></pre>"},{"location":"runners/#using-with-axual-platform-or-other-namespaced-kafka-clusters","title":"Using with Axual platform or other namespaced Kafka clusters","text":"<p>A special mode for connecting to clusters that use namespaced Kafka resources is available. This mode can be activated by specifying the namespace pattern to use. This pattern will be resolved to a complete name by KSML using the provided configuration options.</p> <p>The following config will resolve the backing topic of a stream or table</p> <pre><code>kafka:\n   # The patterns for topics, groups and transactional ids.\n   # Each field between the curly braces must be specified in the configuration, except the topic,\n   # group.id and transactional.id fields, which is used to identify the place where the resource name\n   # is used\n   topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\"\n   group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\"\n   transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\"\n\n   # Additional configuration options used for resolving the pattern to values\n   tenant: \"ksmldemo\"\n   instance: \"dta\"\n   environment: \"dev\"\n</code></pre>"},{"location":"runners/#starting-a-container","title":"Starting a container","text":"<p>To start a container the KSML definitions and Runner configuration files need to be available in a directory mounted inside the docker container.</p> <p>The default Runner configuration filename is ksml-runner.yaml. If no arguments are given, the runner will look for this file in the home directory</p> <pre><code>## -w sets the current working directory in the container\n\ndocker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot\n\n## or\n\ndocker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot /ksml/ksml-runner.yaml\n</code></pre> <p>or, if the runner configuration is in a different file, like my-runner.yaml.</p> <pre><code>docker run --rm -ti -v /path/to/local/ksml/directory:/ksml axual/ksml-axual:latest /ksml/my-runner.yaml\n</code></pre>"},{"location":"stores/","title":"State Stores","text":""},{"location":"stores/#introduction","title":"Introduction","text":"<p>Several stream operations use state stores to retain (intermediate) results of their calculations. State stores are typically configured with some additional parameters to limit their data storage (retention time) and/or determine when data is emitted from them to the next stream operation.</p> <pre><code>stores:\n  owner_count_store:\n    name: owner_count\n    retention: 3m\n    caching: false\n</code></pre>"},{"location":"stores/#configuration","title":"Configuration","text":"<p>State store configurations are defined by the following tags:</p> Parameter Value Type Default Required Description <code>name</code> <code>string</code> none Optional The name of the state store. This field is not mandatory, but operations that use the state store configuration will require a name for their store. If the store configuration does not specify an explicit name, then the operation will default back to the operation's name, specified with its <code>name</code> attribute. If that name is unspecified, then an exception will be thrown. In general, it is considered good practice to always specify the store name explicitly with its definition. <code>type</code> <code>string</code> none Required The type of the state store. Possible types are <code>keyValue</code>, <code>session</code> and <code>window</code>. <code>persistent</code> <code>boolean</code> <code>false</code> Optional <code>true</code> if the state store should be retained on disk. See [link] for more information on how Kafka Streams maintains state store state in a state directory. When this parameter is false or undefined, the state store is (re)built up in memory during upon KSML start. <code>timestamped</code> <code>boolean</code> <code>false</code> Optional (Only relevant for keyValue and window stores) <code>true</code> if all messages in the state store need to be timestamped. This effectively changes the state store from type  to . The timestamp contains the last timestamp that updated the aggregated value in the window. <code>versioned</code> <code>boolean</code> <code>false</code> Optional (Only relevant for keyValue stores) <code>true</code> if elements in the store are versioned, <code>false</code> otherwise <code>keyType</code> <code>string</code> none Required The key type of the state store. See Types for more information. <code>valueType</code> <code>string</code> none Required The value type of the state store. See Types for more information. <code>caching</code> <code>boolean</code> <code>false</code> Optional This parameter controls the internal state store caching. When true, the state store caches entries and does not emit every state change but only. When false all changes to the state store will be emitted immediately. <code>logging</code> <code>boolean</code> <code>false</code> Optional This parameter determines whether state changes are written out to a changelog topic, or not. When true all state store changes are produced to a changelog topic. The changelog topic is named <code>appId-storeName-changelog</code>. When false no changelog topic is written to. <p>Example:</p> <pre><code>stores:\n  owner_count_store:\n    name: owner_count\n    retention: 3m\n    caching: false\n\npipelines:\n  main:\n    from: sensor_source\n    via:\n      - type: groupBy\n        name: ksml_sensordata_grouped\n        mapper:\n          expression: value[\"owner\"]\n          resultType: string\n      - type: windowedBy\n        windowType: time\n        duration: 20s\n        grace: 40s\n      - type: aggregate\n        store: owner_count_store     # refer to predefined store configuration above\n        initializer:\n          expression: 0\n          resultType: long\n        aggregator:\n          expression: aggregatedValue+1\n          resultType: long\n</code></pre> <p>Instead of referring to predefined state store configurations, you may also use an inline definition for the store:</p> <pre><code>      - type: aggregate\n        store:\n          name: owner_count\n          retention: 3m\n          caching: false\n        initializer:\n          expression: 0\n          resultType: long\n        aggregator:\n          expression: aggregatedValue+1\n          resultType: long\n</code></pre>"},{"location":"streams/","title":"Streams","text":""},{"location":"streams/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Stream</li> <li>Table</li> <li>GlobalTable</li> </ol>"},{"location":"streams/#introduction","title":"Introduction","text":"<p>Every KSML definition file contains a list of declared streams. There are three types of streams supported:</p> Type Kafka Streams equivalent Description Stream <code>KStream</code> KStream is an abstraction of a record stream of KeyValue pairs, i.e., each record is an independent entity/event in the real world. For example a user X might buy two items I1 and I2, and thus there might be two records ,  in the stream.A KStream is either defined from one or multiple Kafka topics that are consumed message by message or the result of a KStream transformation. A KTable can also be converted into a KStream.A KStream can be transformed record by record, joined with another KStream, KTable, GlobalKTable, or can be aggregated into a KTable. Table <code>KTable</code> KTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key.A KTable is either defined from a single Kafka topic that is consumed message by message or the result of a KTable transformation. An aggregation of a KStream also yields a KTable.A KTable can be transformed record by record, joined with another KTable or KStream, or can be re-partitioned and aggregated into a new KTable. GlobalTable <code>GlobalKTable</code> GlobalKTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key.GlobalKTable can only be used as right-hand side input for stream-table joins.In contrast to a KTable that is partitioned over all KafkaStreams instances, a GlobalKTable is fully replicated per KafkaStreams instance. Every partition of the underlying topic is consumed by each GlobalKTable, such that the full set of data is available in every KafkaStreams instance. This provides the ability to perform joins with KStream without having to repartition the input stream. All joins with the GlobalKTable require that a KeyValueMapper is provided that can map from the KeyValue of the left hand side KStream to the key of the right hand side GlobalKTable. <p>The definitions of these stream types are done as described below.</p>"},{"location":"streams/#stream","title":"Stream","text":"<p>Example:</p> <pre><code>streams:\n  my_stream_reference:\n    topic: some_kafka_topic\n    keyType: string\n    valueType: string\n    offsetResetPolicy: earliest\n    timestampExtractor: my_timestamp_extractor\n</code></pre>"},{"location":"streams/#table","title":"Table","text":"<p>Example:</p> <pre><code>tables:\n  my_table_reference:\n    topic: some_kafka_topic\n    keyType: string\n    valueType: string\n    store: &lt;keyValue state store reference or inline definition&gt;\n</code></pre>"},{"location":"streams/#globaltable","title":"GlobalTable","text":"<p>Example:</p> <pre><code>globalTables:\n  my_global_table_reference:\n    topic: some_kafka_topic\n    keyType: string\n    valueType: string\n</code></pre>"},{"location":"types/","title":"Types","text":""},{"location":"types/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Notations</li> <li>Primitives</li> <li>Any</li> <li>Duration</li> <li>Enum</li> <li>List</li> <li>Struct</li> <li>Tuple</li> <li>Windowed</li> </ol>"},{"location":"types/#introduction","title":"Introduction","text":"<p>KSML supports a wide range of simple and complex types. In addition, each data type can be read from Kafka, and written to Kafka, using Notations. More about Notations in the following paragraph. The paragraphs after will dive more into the data types that KSML supports.</p>"},{"location":"types/#notations","title":"Notations","text":"<p>KSML has a number of simple and complex data types. Simple data types contain a single value. For example, a <code>long</code> or a <code>string</code> type can hold only one value. Complex data types contain multiple values. For example, a <code>list</code> or a <code>struct</code> can contain zero or more other simple or complex values. For example, a <code>struct</code> is comparable to a key/value map, where all keys are of type <code>string</code>. Its contents could be represented as:</p> <pre><code>{\n  \"name\": \"Albert\",\n  \"lastName\": \"Einstein\",\n  \"dateOfBirth\": \"14-08-1879\",\n  \"profession\": \"Patent clerk\",\n  \"children\": 3,\n  \"isAlive\": false\n}\n</code></pre> <p>There are several ways in which structured objects are read from or written to Kafka topics. For instance, using AVRO, JSON or XML. Each use the same <code>struct</code> but translate it differently to a binary or string representation that can be stored in Kafka.</p> <p>To facilitate these different ways of writing, KSML supports Notations. Each notation can be seen as 'the format that data is written in' to Kafka. Below is a list of all supported notations:</p> Notation Link Implementations AVRO https://avro.apache.org apicurio_avro, confluent_avro CSV https://en.wikipedia.org/wiki/Comma-separated_values JSON https://json.org SOAP https://en.wikipedia.org/wiki/SOAP XML https://en.wikipedia.org/wiki/XML <p>Notations are a natural extension to data types. They can be used as: <pre><code>notation:datatype\n</code></pre></p> <p>For example: <pre><code>avro:SensorData     # AVRO with schema SensorData (loaded from SensorData.avsc)\njson                # Schemaless JSON\nxml:PersonSchema    # XML with schema PersonSchema (loaded from PersonSchema.xsd)\n</code></pre></p>"},{"location":"types/#primitives","title":"Primitives","text":"<p>The following native types are supported.</p> Type Description ?, or any Any type null, or none Null type, available for variables without a value (eg. Kafka tombstone messages, or optional AVRO fields) boolean Boolean values, ie. true or false double Double precision floating point float Single precision floating point byte 8-bit integer short 16-bit integer int 32-bit integer long 64-bit long bytes Byte array string String of characters struct Key-value map, where with <code>string</code> keys and values of any type"},{"location":"types/#any","title":"Any","text":"<p>The special type <code>?</code> or <code>any</code> can be used in places where input is uncertain. Code that deals with input of this type should always perform proper type checking before assuming any specific underlying type.</p>"},{"location":"types/#duration","title":"Duration","text":"<p>Some fields in the KSML spec are of type <code>duration</code>. These fields have a fixed format <code>123x</code>, where <code>123</code> is an integer and <code>x</code> is any of the following:</p> <ul> <li>: milliseconds <li><code>s</code>: seconds</li> <li><code>m</code>: minutes</li> <li><code>h</code>: hours</li> <li><code>d</code>: days</li> <li><code>w</code>: weeks</li>"},{"location":"types/#enum","title":"Enum","text":"<p>Enumerations can be defined as individual types, through:</p> <pre><code>enum(literal1, literal2, ...)\n</code></pre>"},{"location":"types/#list","title":"List","text":"<p>Lists contain elements of the same type. They are defined using:</p> <pre><code>[elementType]\n</code></pre> <p>Examples:</p> <pre><code>[string]\n[long]\n[avro:SensorData]\n[(long,string)]\n</code></pre>"},{"location":"types/#struct","title":"Struct","text":"<p>Structs are key-value maps, as usually found in AVRO or JSON messages. They are defined using:</p> <pre><code>struct\n</code></pre>"},{"location":"types/#tuple","title":"Tuple","text":"<p>Tuples combine multiple subtypes into one. For example <code>(1, \"text\")</code> is a tuple containing an integer and a string element. Tuple types always have a fixed number of elements.</p> <p>Examples:</p> <pre><code>(long, string)\n(avro:SensorData, string, long, string)\n([string], long)\n</code></pre>"},{"location":"types/#union","title":"Union","text":"<p>Unions are 'either-or' types. They have their own internal structure and can be described by respective data schema. Unions are defined using:</p> <pre><code>union(type1, type2, ...)\n</code></pre> <p>Examples:</p> <pre><code>union(null, string)\nunion(avro:SensorData, long, string)\n</code></pre>"},{"location":"types/#windowed","title":"Windowed","text":"<p>Some Kafka Streams operations modify the key type from K to Windowed\\. Kafka Streams uses the Windowed\\ type to group Kafka messages with similar keys together. The result is always a time-bound window, with a defined start and end time. <p>KSML can convert the internal Windowed\\ type into a struct type with five fields: <ul> <li><code>start</code>: The window start timestamp (type <code>long</code>)</li> <li><code>end</code>: The window end timestamp (type <code>long</code>)</li> <li><code>startTime</code>: The window start time (type <code>string</code>)</li> <li><code>endTime</code>: The window end time (type <code>string</code>)</li> <li><code>key</code>: The key used to group items together in this window (type is the same as the original key type)</li> </ul> <p>However, in pipelines or topic definitions users may need to refer to this type explicitly. This is done in the following manner:</p> <pre><code>notation:windowed(keytype)\n</code></pre> <p>For example:</p> <pre><code>avro:windowed(avro:SensorData)\nxml:windowed(long)\n</code></pre>"}]}