# Error Monitoring and Observability Example
# This example demonstrates comprehensive error monitoring with metrics,
# alerting, and observability for error handling patterns

name: error-monitoring-example
version: 1.0.0

# Input streams
streams:
  application_events:
    topic: application-events
    keyType: string
    valueType: json
    offsetResetPolicy: latest
  
  error_events:
    topic: error-events
    keyType: string
    valueType: json
    offsetResetPolicy: latest

# Output topics
producers:
  error_metrics:
    topic: error-metrics
    keyType: string
    valueType: json
  
  error_alerts:
    topic: error-alerts
    keyType: string
    valueType: json
  
  error_dashboard:
    topic: error-dashboard-data
    keyType: string
    valueType: json
  
  sla_violations:
    topic: sla-violations
    keyType: string
    valueType: json

# State stores for monitoring
stores:
  error_rate_window:
    type: keyValue
    keyType: string  # metric_key (e.g., "error_rate_5min")
    valueType: json  # windowed counts
    persistent: true
  
  circuit_breaker_metrics:
    type: keyValue
    keyType: string  # service_name
    valueType: json  # circuit state history
    persistent: true
  
  retry_metrics:
    type: keyValue
    keyType: string  # operation_name
    valueType: json  # retry statistics
    persistent: true
  
  dlq_metrics:
    type: keyValue
    keyType: string  # topic_name
    valueType: json  # DLQ statistics
    persistent: true
  
  recovery_metrics:
    type: keyValue
    keyType: string  # workflow_name
    valueType: json  # recovery success rates
    persistent: true
  
  alert_history:
    type: keyValue
    keyType: string  # alert_key
    valueType: json  # alert occurrences
    persistent: true

# Monitoring functions
functions:
  # Calculate error rates across different time windows
  calculate_error_rates:
    type: valueTransformer
    globalCode: |
      import time
      from collections import deque
      
      # Time windows in milliseconds
      TIME_WINDOWS = {
        "1min": 60000,
        "5min": 300000,
        "15min": 900000,
        "1hour": 3600000
      }
      
      # Error rate thresholds for alerting
      ERROR_RATE_THRESHOLDS = {
        "1min": 0.2,    # 20% error rate
        "5min": 0.15,   # 15% error rate
        "15min": 0.1,   # 10% error rate
        "1hour": 0.05   # 5% error rate
      }
    code: |
      event = value
      is_error = event.get("type") == "error" or event.get("status") == "failed"
      current_time = int(time.time() * 1000)
      
      metrics = []
      
      for window_name, window_ms in TIME_WINDOWS.items():
        metric_key = f"error_rate_{window_name}"
        
        # Get or initialize window data
        window_data = state_stores.get("error_rate_window", metric_key) or {
          "events": [],
          "error_count": 0,
          "total_count": 0
        }
        
        # Convert to deque for efficient operations
        events = deque(window_data["events"])
        
        # Add current event
        events.append({
          "timestamp": current_time,
          "is_error": is_error
        })
        
        # Remove events outside window
        cutoff_time = current_time - window_ms
        while events and events[0]["timestamp"] < cutoff_time:
          old_event = events.popleft()
          window_data["total_count"] -= 1
          if old_event["is_error"]:
            window_data["error_count"] -= 1
        
        # Update counts
        window_data["total_count"] += 1
        if is_error:
          window_data["error_count"] += 1
        
        # Calculate rate
        error_rate = window_data["error_count"] / window_data["total_count"] if window_data["total_count"] > 0 else 0
        
        # Check threshold
        threshold = ERROR_RATE_THRESHOLDS.get(window_name, 0.1)
        is_above_threshold = error_rate > threshold
        
        # Store updated window data
        window_data["events"] = list(events)[-1000:]  # Keep last 1000 events
        state_stores.put("error_rate_window", metric_key, window_data)
        
        # Create metric
        metric = {
          "metric_type": "error_rate",
          "window": window_name,
          "error_rate": error_rate,
          "error_count": window_data["error_count"],
          "total_count": window_data["total_count"],
          "threshold": threshold,
          "alert": is_above_threshold,
          "timestamp": current_time
        }
        
        metrics.append(metric)
      
      return {
        "event": event,
        "metrics": metrics,
        "has_alerts": any(m["alert"] for m in metrics)
      }
    resultType: json
    stores:
      - error_rate_window
  
  # Monitor circuit breaker states
  monitor_circuit_breakers:
    type: valueTransformer
    globalCode: |
      import time
      
      CIRCUIT_STATES = ["CLOSED", "OPEN", "HALF_OPEN"]
    code: |
      event = value
      
      # Check if this is a circuit breaker event
      if event.get("type") != "circuit_breaker_state_change":
        return {"skip": True}
      
      service_name = event.get("service_name")
      new_state = event.get("new_state")
      old_state = event.get("old_state")
      current_time = int(time.time() * 1000)
      
      # Get circuit metrics
      metrics = state_stores.get("circuit_breaker_metrics", service_name) or {
        "state_changes": [],
        "open_count": 0,
        "total_opens": 0,
        "last_opened": None,
        "total_open_duration": 0
      }
      
      # Record state change
      metrics["state_changes"].append({
        "timestamp": current_time,
        "old_state": old_state,
        "new_state": new_state
      })
      
      # Keep only last 100 state changes
      if len(metrics["state_changes"]) > 100:
        metrics["state_changes"] = metrics["state_changes"][-100:]
      
      # Update metrics based on state transition
      if new_state == "OPEN":
        metrics["open_count"] += 1
        metrics["total_opens"] += 1
        metrics["last_opened"] = current_time
      elif old_state == "OPEN" and new_state in ["CLOSED", "HALF_OPEN"]:
        if metrics["last_opened"]:
          open_duration = current_time - metrics["last_opened"]
          metrics["total_open_duration"] += open_duration
        metrics["open_count"] = max(0, metrics["open_count"] - 1)
      
      # Calculate statistics
      recent_changes = [sc for sc in metrics["state_changes"] 
                       if current_time - sc["timestamp"] < 3600000]  # Last hour
      
      flapping = len(recent_changes) > 10  # More than 10 changes in an hour
      
      # Store updated metrics
      state_stores.put("circuit_breaker_metrics", service_name, metrics)
      
      return {
        "metric_type": "circuit_breaker",
        "service_name": service_name,
        "current_state": new_state,
        "open_circuits": metrics["open_count"],
        "total_opens": metrics["total_opens"],
        "flapping": flapping,
        "recent_state_changes": len(recent_changes),
        "timestamp": current_time,
        "alert": new_state == "OPEN" or flapping
      }
    resultType: json
    stores:
      - circuit_breaker_metrics
  
  # Track retry metrics
  track_retry_metrics:
    type: valueTransformer
    globalCode: |
      import time
      
      MAX_HEALTHY_RETRY_RATE = 0.3  # 30% retry rate is concerning
    code: |
      event = value
      
      # Check if this is a retry event
      if event.get("type") not in ["retry_attempted", "retry_succeeded", "retry_exhausted"]:
        return {"skip": True}
      
      operation = event.get("operation_name", "unknown")
      event_type = event.get("type")
      current_time = int(time.time() * 1000)
      
      # Get retry metrics
      metrics = state_stores.get("retry_metrics", operation) or {
        "attempts": 0,
        "successes": 0,
        "exhausted": 0,
        "last_updated": current_time,
        "retry_counts": {}  # Distribution of retry counts
      }
      
      # Update metrics
      if event_type == "retry_attempted":
        metrics["attempts"] += 1
        retry_count = event.get("retry_count", 0)
        metrics["retry_counts"][str(retry_count)] = metrics["retry_counts"].get(str(retry_count), 0) + 1
      
      elif event_type == "retry_succeeded":
        metrics["successes"] += 1
      
      elif event_type == "retry_exhausted":
        metrics["exhausted"] += 1
      
      metrics["last_updated"] = current_time
      
      # Calculate statistics
      total_operations = metrics["attempts"] + metrics["successes"]
      retry_rate = metrics["attempts"] / total_operations if total_operations > 0 else 0
      success_rate = metrics["successes"] / metrics["attempts"] if metrics["attempts"] > 0 else 0
      exhaustion_rate = metrics["exhausted"] / metrics["attempts"] if metrics["attempts"] > 0 else 0
      
      # Determine if alerting is needed
      alert_conditions = []
      if retry_rate > MAX_HEALTHY_RETRY_RATE:
        alert_conditions.append(f"High retry rate: {retry_rate:.2%}")
      if exhaustion_rate > 0.1:  # More than 10% exhausted
        alert_conditions.append(f"High exhaustion rate: {exhaustion_rate:.2%}")
      
      # Store updated metrics
      state_stores.put("retry_metrics", operation, metrics)
      
      return {
        "metric_type": "retry",
        "operation": operation,
        "retry_rate": retry_rate,
        "success_rate": success_rate,
        "exhaustion_rate": exhaustion_rate,
        "total_attempts": metrics["attempts"],
        "retry_distribution": metrics["retry_counts"],
        "alert": len(alert_conditions) > 0,
        "alert_reasons": alert_conditions,
        "timestamp": current_time
      }
    resultType: json
    stores:
      - retry_metrics
  
  # Monitor DLQ metrics
  monitor_dlq_metrics:
    type: valueTransformer
    globalCode: |
      import time
      
      DLQ_SIZE_THRESHOLD = 100
      DLQ_RATE_THRESHOLD = 10  # messages per minute
    code: |
      event = value
      
      # Check if this is a DLQ event
      if event.get("type") not in ["dlq_message_added", "dlq_message_reprocessed", "dlq_message_expired"]:
        return {"skip": True}
      
      topic = event.get("topic_name", "unknown")
      event_type = event.get("type")
      current_time = int(time.time() * 1000)
      
      # Get DLQ metrics
      metrics = state_stores.get("dlq_metrics", topic) or {
        "current_size": 0,
        "total_added": 0,
        "total_reprocessed": 0,
        "total_expired": 0,
        "add_timestamps": [],
        "error_categories": {}
      }
      
      # Update metrics
      if event_type == "dlq_message_added":
        metrics["current_size"] += 1
        metrics["total_added"] += 1
        metrics["add_timestamps"].append(current_time)
        
        # Track error categories
        error_type = event.get("error_type", "unknown")
        metrics["error_categories"][error_type] = metrics["error_categories"].get(error_type, 0) + 1
      
      elif event_type == "dlq_message_reprocessed":
        metrics["current_size"] = max(0, metrics["current_size"] - 1)
        metrics["total_reprocessed"] += 1
      
      elif event_type == "dlq_message_expired":
        metrics["current_size"] = max(0, metrics["current_size"] - 1)
        metrics["total_expired"] += 1
      
      # Clean old timestamps and calculate rate
      one_minute_ago = current_time - 60000
      metrics["add_timestamps"] = [ts for ts in metrics["add_timestamps"] if ts > one_minute_ago]
      dlq_rate = len(metrics["add_timestamps"])
      
      # Determine alerts
      alert_conditions = []
      if metrics["current_size"] > DLQ_SIZE_THRESHOLD:
        alert_conditions.append(f"DLQ size exceeds threshold: {metrics['current_size']}")
      if dlq_rate > DLQ_RATE_THRESHOLD:
        alert_conditions.append(f"High DLQ rate: {dlq_rate} messages/minute")
      
      # Calculate reprocess success rate
      reprocess_success_rate = (metrics["total_reprocessed"] / 
                               (metrics["total_reprocessed"] + metrics["total_expired"]) 
                               if (metrics["total_reprocessed"] + metrics["total_expired"]) > 0 else 0)
      
      # Store updated metrics
      state_stores.put("dlq_metrics", topic, metrics)
      
      return {
        "metric_type": "dlq",
        "topic": topic,
        "current_size": metrics["current_size"],
        "dlq_rate": dlq_rate,
        "total_added": metrics["total_added"],
        "reprocess_success_rate": reprocess_success_rate,
        "error_distribution": metrics["error_categories"],
        "alert": len(alert_conditions) > 0,
        "alert_reasons": alert_conditions,
        "timestamp": current_time
      }
    resultType: json
    stores:
      - dlq_metrics
  
  # Generate comprehensive error dashboard
  generate_error_dashboard:
    type: valueTransformer
    globalCode: |
      import time
    code: |
      current_time = int(time.time() * 1000)
      
      # Aggregate all metrics
      dashboard = {
        "timestamp": current_time,
        "error_rates": {},
        "circuit_breakers": {},
        "retry_operations": {},
        "dlq_status": {},
        "overall_health": "healthy",
        "active_alerts": []
      }
      
      # Get error rates
      for window in ["1min", "5min", "15min", "1hour"]:
        metric_key = f"error_rate_{window}"
        window_data = state_stores.get("error_rate_window", metric_key)
        if window_data:
          rate = window_data["error_count"] / window_data["total_count"] if window_data["total_count"] > 0 else 0
          dashboard["error_rates"][window] = {
            "rate": rate,
            "count": window_data["error_count"],
            "total": window_data["total_count"]
          }
      
      # Get circuit breaker states
      # In production, this would scan all services
      sample_services = ["payment-service", "inventory-service", "shipping-service"]
      open_circuits = 0
      
      for service in sample_services:
        cb_metrics = state_stores.get("circuit_breaker_metrics", service)
        if cb_metrics:
          is_open = cb_metrics.get("open_count", 0) > 0
          if is_open:
            open_circuits += 1
          dashboard["circuit_breakers"][service] = {
            "is_open": is_open,
            "total_opens": cb_metrics.get("total_opens", 0)
          }
      
      # Get retry metrics summary
      sample_operations = ["process_payment", "update_inventory", "send_notification"]
      high_retry_operations = []
      
      for operation in sample_operations:
        retry_metrics = state_stores.get("retry_metrics", operation)
        if retry_metrics:
          total = retry_metrics["attempts"] + retry_metrics["successes"]
          retry_rate = retry_metrics["attempts"] / total if total > 0 else 0
          if retry_rate > 0.3:
            high_retry_operations.append(operation)
          dashboard["retry_operations"][operation] = {
            "retry_rate": retry_rate,
            "exhaustion_rate": retry_metrics["exhausted"] / retry_metrics["attempts"] if retry_metrics["attempts"] > 0 else 0
          }
      
      # Get DLQ summary
      sample_topics = ["orders", "payments", "inventory"]
      total_dlq_size = 0
      
      for topic in sample_topics:
        dlq_metrics = state_stores.get("dlq_metrics", topic)
        if dlq_metrics:
          size = dlq_metrics.get("current_size", 0)
          total_dlq_size += size
          dashboard["dlq_status"][topic] = {
            "size": size,
            "rate": len([ts for ts in dlq_metrics.get("add_timestamps", []) 
                        if ts > current_time - 60000])
          }
      
      # Determine overall health
      if any(rate.get("rate", 0) > 0.2 for rate in dashboard["error_rates"].values()):
        dashboard["overall_health"] = "degraded"
        dashboard["active_alerts"].append("High error rate detected")
      
      if open_circuits > 0:
        dashboard["overall_health"] = "degraded"
        dashboard["active_alerts"].append(f"{open_circuits} circuit breakers open")
      
      if high_retry_operations:
        dashboard["overall_health"] = "degraded"
        dashboard["active_alerts"].append(f"High retry rate for: {', '.join(high_retry_operations)}")
      
      if total_dlq_size > 100:
        dashboard["overall_health"] = "degraded"
        dashboard["active_alerts"].append(f"High DLQ size: {total_dlq_size} messages")
      
      if len(dashboard["active_alerts"]) > 3:
        dashboard["overall_health"] = "critical"
      
      return dashboard
    resultType: json
    stores:
      - error_rate_window
      - circuit_breaker_metrics
      - retry_metrics
      - dlq_metrics
  
  # Check SLA compliance
  check_sla_compliance:
    type: valueTransformer
    globalCode: |
      import time
      
      # SLA definitions
      SLA_DEFINITIONS = {
        "error_rate": {
          "target": 0.01,  # 99% success rate
          "measurement_window": 300000  # 5 minutes
        },
        "circuit_breaker_uptime": {
          "target": 0.99,  # 99% uptime
          "measurement_window": 3600000  # 1 hour
        },
        "dlq_size": {
          "target": 50,  # Max 50 messages
          "measurement_window": 60000  # 1 minute
        }
      }
    code: |
      current_time = int(time.time() * 1000)
      violations = []
      
      # Check error rate SLA
      error_window = state_stores.get("error_rate_window", "error_rate_5min")
      if error_window:
        error_rate = error_window["error_count"] / error_window["total_count"] if error_window["total_count"] > 0 else 0
        if error_rate > SLA_DEFINITIONS["error_rate"]["target"]:
          violations.append({
            "sla_type": "error_rate",
            "current_value": error_rate,
            "target_value": SLA_DEFINITIONS["error_rate"]["target"],
            "violation_percentage": ((error_rate - SLA_DEFINITIONS["error_rate"]["target"]) / 
                                   SLA_DEFINITIONS["error_rate"]["target"] * 100)
          })
      
      # Check circuit breaker uptime SLA
      total_open_time = 0
      services_checked = 0
      
      for service in ["payment-service", "inventory-service"]:
        cb_metrics = state_stores.get("circuit_breaker_metrics", service)
        if cb_metrics:
          services_checked += 1
          if cb_metrics.get("open_count", 0) > 0 and cb_metrics.get("last_opened"):
            # Currently open
            open_duration = current_time - cb_metrics["last_opened"]
            total_open_time += open_duration
      
      if services_checked > 0:
        avg_uptime = 1 - (total_open_time / (services_checked * SLA_DEFINITIONS["circuit_breaker_uptime"]["measurement_window"]))
        if avg_uptime < SLA_DEFINITIONS["circuit_breaker_uptime"]["target"]:
          violations.append({
            "sla_type": "circuit_breaker_uptime",
            "current_value": avg_uptime,
            "target_value": SLA_DEFINITIONS["circuit_breaker_uptime"]["target"],
            "violation_percentage": ((SLA_DEFINITIONS["circuit_breaker_uptime"]["target"] - avg_uptime) / 
                                   SLA_DEFINITIONS["circuit_breaker_uptime"]["target"] * 100)
          })
      
      # Check DLQ size SLA
      total_dlq_size = 0
      for topic in ["orders", "payments", "inventory"]:
        dlq_metrics = state_stores.get("dlq_metrics", topic)
        if dlq_metrics:
          total_dlq_size += dlq_metrics.get("current_size", 0)
      
      if total_dlq_size > SLA_DEFINITIONS["dlq_size"]["target"]:
        violations.append({
          "sla_type": "dlq_size",
          "current_value": total_dlq_size,
          "target_value": SLA_DEFINITIONS["dlq_size"]["target"],
          "violation_percentage": ((total_dlq_size - SLA_DEFINITIONS["dlq_size"]["target"]) / 
                                 SLA_DEFINITIONS["dlq_size"]["target"] * 100)
        })
      
      return {
        "timestamp": current_time,
        "sla_compliant": len(violations) == 0,
        "violations": violations,
        "severity": "critical" if len(violations) > 2 else "high" if len(violations) > 0 else "none"
      }
    resultType: json
    stores:
      - error_rate_window
      - circuit_breaker_metrics
      - dlq_metrics

# Main monitoring pipelines
pipelines:
  error_rate_monitor:
    from: application_events
    via:
      - type: transformValue
        mapper: calculate_error_rates
      
      - type: branch
        branches:
          - name: has_alerts
            predicate:
              expression: value.get("has_alerts", False)
          - name: metrics_only
            predicate:
              expression: not value.get("has_alerts", False)
    
    branch:
      has_alerts:
        via:
          - type: transformValue
            mapper:
              expression: |
                alerts = []
                for metric in value.get("metrics", []):
                  if metric.get("alert"):
                    alerts.append({
                      "alert_type": "error_rate",
                      "window": metric["window"],
                      "error_rate": metric["error_rate"],
                      "threshold": metric["threshold"],
                      "severity": "high" if metric["error_rate"] > metric["threshold"] * 1.5 else "medium"
                    })
                return {
                  "alerts": alerts,
                  "timestamp": value["metrics"][0]["timestamp"] if value.get("metrics") else int(time.time() * 1000)
                }
              resultType: json
        to: error_alerts
      
      metrics_only:
        via:
          - type: transformValue
            mapper:
              expression: value.get("metrics", [])
              resultType: json
          - type: forEach
            action:
              expression: pass  # Metrics recorded in state stores

  circuit_breaker_monitor:
    from: error_events
    via:
      - type: transformValue
        mapper: monitor_circuit_breakers
      
      - type: filter
        predicate:
          expression: not value.get("skip", False)
      
      - type: branch
        branches:
          - name: alert
            predicate:
              expression: value.get("alert", False)
          - name: metric
            predicate:
              expression: not value.get("alert", False)
    
    branch:
      alert:
        to:
          - error_alerts
          - error_metrics
      metric:
        to: error_metrics

  retry_monitor:
    from: error_events
    via:
      - type: transformValue
        mapper: track_retry_metrics
      
      - type: filter
        predicate:
          expression: not value.get("skip", False)
      
      - type: branch
        branches:
          - name: alert
            predicate:
              expression: value.get("alert", False)
          - name: metric
            predicate:
              expression: not value.get("alert", False)
    
    branch:
      alert:
        to:
          - error_alerts
          - error_metrics
      metric:
        to: error_metrics

  dlq_monitor:
    from: error_events
    via:
      - type: transformValue
        mapper: monitor_dlq_metrics
      
      - type: filter
        predicate:
          expression: not value.get("skip", False)
      
      - type: branch
        branches:
          - name: alert
            predicate:
              expression: value.get("alert", False)
          - name: metric
            predicate:
              expression: not value.get("alert", False)
    
    branch:
      alert:
        to:
          - error_alerts
          - error_metrics
      metric:
        to: error_metrics

  # Dashboard generator (runs periodically in production)
  dashboard_generator:
    from: application_events
    via:
      # Sample every 100th event to generate dashboard
      - type: filter
        predicate:
          expression: |
            import random
            random.random() < 0.01  # 1% sampling
      
      - type: transformValue
        mapper: generate_error_dashboard
    
    to: error_dashboard

  # SLA compliance checker
  sla_checker:
    from: application_events
    via:
      # Check SLA every 1000th event
      - type: filter
        predicate:
          expression: |
            import random
            random.random() < 0.001  # 0.1% sampling
      
      - type: transformValue
        mapper: check_sla_compliance
      
      - type: filter
        predicate:
          expression: not value.get("sla_compliant", True)
    
    to: sla_violations