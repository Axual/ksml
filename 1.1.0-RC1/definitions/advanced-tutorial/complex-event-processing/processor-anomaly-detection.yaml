# Processor for anomaly detection using statistical analysis

streams:
  sensor_metrics:
    topic: sensor_metrics
    keyType: string
    valueType: json

stores:
  stats_store:
    type: keyValue
    keyType: string
    valueType: string
    persistent: true

functions:
  detect_anomalies:
    type: valueTransformer
    stores:
      - stats_store
    code: |
      import json
      
      # Extract fields from JSON metric
      current_value = value.get("value")
      timestamp = value.get("timestamp")
      sensor_id = value.get("sensor_id")
      metric_id = value.get("metric_id")
      
      if current_value is None or timestamp is None:
        return None
      
      # Get or initialize statistics
      stats_json = stats_store.get(key)
      if stats_json:
        stats = json.loads(stats_json)
      else:
        stats = {"count": 0, "sum": 0, "sum_sq": 0, "min": current_value, "max": current_value}
      
      # Calculate running statistics
      n = stats["count"]
      if n > 10:  # Need enough samples
        mean = stats["sum"] / n
        variance = (stats["sum_sq"] / n) - (mean * mean)
        std_dev = variance ** 0.5 if variance > 0 else 1
        
        # Detect anomalies (values outside 3 standard deviations)
        z_score = abs(current_value - mean) / std_dev if std_dev > 0 else 0
        
        if z_score > 3:
          log.warn("ANOMALY detected for {}: value={}, mean={:.2f}, z_score={:.2f}", 
                   key, current_value, mean, z_score)
          anomaly_type = "spike" if current_value > mean else "drop"
          result = {
            "anomaly_type": "STATISTICAL_ANOMALY",
            "status": "DETECTED",
            "pattern": anomaly_type,
            "sensor_id": sensor_id,
            "metric_id": metric_id,
            "anomaly_value": current_value,
            "statistical_analysis": {
              "mean": round(mean, 2),
              "std_dev": round(std_dev, 2),
              "z_score": round(z_score, 2),
              "threshold": 3.0,
              "sample_count": n + 1
            },
            "detection_timestamp": timestamp,
            "sensor_metadata": {
              "sensor_type": value.get("sensor_type"),
              "location": value.get("location"),
              "unit": value.get("unit")
            },
            "severity": "HIGH" if z_score > 5 else "MEDIUM"
          }
        else:
          result = None
      else:
        result = None
      
      # Update statistics
      stats["count"] = n + 1
      stats["sum"] += current_value
      stats["sum_sq"] += current_value * current_value
      stats["min"] = min(stats["min"], current_value)
      stats["max"] = max(stats["max"], current_value)
      
      # Store updated stats
      stats_store.put(key, json.dumps(stats))
      
      return result
      
    expression: result if result else None
    resultType: json

pipelines:
  anomaly_detection_pipeline:
    from: sensor_metrics
    via:
      - type: mapValues
        mapper: detect_anomalies
      - type: filter
        if:
          expression: value is not None
    to:
      topic: anomalies_detected
      keyType: string
      valueType: json