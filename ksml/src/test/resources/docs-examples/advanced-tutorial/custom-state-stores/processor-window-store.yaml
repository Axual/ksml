# Processor demonstrating window store for time-based aggregations

streams:
  server_metrics:
    topic: server_metrics
    keyType: string
    valueType: json

stores:
  metrics_window_store:
    type: keyValue
    keyType: string
    valueType: string
    persistent: true
    caching: true

functions:
  aggregate_metrics:
    type: valueTransformer
    stores:
      - metrics_window_store
    code: |
      import json
      import time
      
      # Extract fields from JSON metric
      if not value:
        return None
      
      metric_name = value.get("metric_name")
      metric_value = value.get("value")
      timestamp = value.get("timestamp")
      metric_id = value.get("metric_id")
      unit = value.get("unit")
      datacenter = value.get("datacenter")
      environment = value.get("environment")
      service = value.get("service")
      alerting = value.get("alerting", {})
      metadata = value.get("metadata", {})
      
      if not metric_name or metric_value is None or not timestamp:
        return None
      
      # Create window key (5-minute windows)
      window_size_ms = 5 * 60 * 1000  # 5 minutes
      window_start = (timestamp // window_size_ms) * window_size_ms
      window_key = f"{key}:{metric_name}:{window_start}"
      
      # Get existing window data
      window_data_str = metrics_window_store.get(window_key)
      if window_data_str:
        window_data = json.loads(window_data_str)
      else:
        window_data = {
          "server_id": key,
          "metric_name": metric_name,
          "unit": unit,
          "window_start": window_start,
          "window_end": window_start + window_size_ms,
          "window_size_ms": window_size_ms,
          "count": 0,
          "sum": 0,
          "min": metric_value,
          "max": metric_value,
          "first_timestamp": timestamp,
          "last_timestamp": timestamp,
          "values": [],
          "datacenters": set(),
          "environments": set(),
          "services": set()
        }
        # Convert sets to lists for JSON serialization
        window_data["datacenters"] = []
        window_data["environments"] = []
        window_data["services"] = []
      
      # Update window aggregates
      window_data["count"] += 1
      window_data["sum"] += metric_value
      window_data["min"] = min(window_data["min"], metric_value)
      window_data["max"] = max(window_data["max"], metric_value)
      window_data["last_timestamp"] = timestamp
      
      # Track categorical data
      if datacenter and datacenter not in window_data["datacenters"]:
        window_data["datacenters"].append(datacenter)
      if environment and environment not in window_data["environments"]:
        window_data["environments"].append(environment)
      if service and service not in window_data["services"]:
        window_data["services"].append(service)
      
      # Keep recent sample values for analysis (last 10 for memory efficiency)
      window_data["values"].append({
        "value": metric_value,
        "timestamp": timestamp,
        "metric_id": metric_id
      })
      if len(window_data["values"]) > 10:
        window_data["values"] = window_data["values"][-10:]
      
      # Calculate statistics
      avg = window_data["sum"] / window_data["count"]
      window_duration = window_data["last_timestamp"] - window_data["first_timestamp"]
      
      # Check against alerting thresholds
      alert_status = "normal"
      if alerting.get("enabled", False):
        if avg >= alerting.get("threshold_critical", float('inf')):
          alert_status = "critical"
        elif avg >= alerting.get("threshold_high", float('inf')):
          alert_status = "warning"
      
      # Store updated window
      metrics_window_store.put(window_key, json.dumps(window_data))
      
      # Generate comprehensive window aggregation result
      result = {
        "aggregation_type": "WINDOW_AGGREGATION",
        "server_id": key,
        "metric_name": metric_name,
        "unit": unit,
        "window": {
          "start_timestamp": window_start,
          "end_timestamp": window_start + window_size_ms,
          "duration_ms": window_size_ms,
          "actual_duration_ms": window_duration,
          "window_key": window_key
        },
        "statistics": {
          "count": window_data["count"],
          "average": round(avg, 2),
          "minimum": round(window_data["min"], 2),
          "maximum": round(window_data["max"], 2),
          "sum": round(window_data["sum"], 2),
          "range": round(window_data["max"] - window_data["min"], 2)
        },
        "alerting": {
          "status": alert_status,
          "thresholds": alerting,
          "current_average": round(avg, 2)
        },
        "distribution": {
          "datacenters": window_data["datacenters"],
          "environments": window_data["environments"],
          "services": window_data["services"]
        },
        "sampling": {
          "recent_values": window_data["values"][-3:],  # Show last 3 values
          "total_samples": window_data["count"]
        },
        "processing_info": {
          "window_updated": True,
          "latest_metric_id": metric_id,
          "processing_timestamp": int(time.time() * 1000)
        }
      }
      
      return result
      
    expression: result if result else None
    resultType: json

pipelines:
  window_aggregation_pipeline:
    from: server_metrics
    via:
      - type: mapValues
        mapper: aggregate_metrics
      - type: filter
        if:
          expression: value is not None
    to:
      topic: windowed_metrics
      keyType: string
      valueType: json