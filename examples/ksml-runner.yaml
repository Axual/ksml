ksml:
  # The examples directory is mounted to /ksml in the Docker container
  configDirectory: .              # When not set defaults to the working directory
  schemaDirectory: .              # When not set defaults to the config directory
  storageDirectory: /tmp          # When not set defaults to the default JVM temp directory

  # This section defines if a REST endpoint is opened on the KSML runner, through which
  # state stores and/or readiness / liveness probes can be accessed.
  applicationServer:
    enabled: false                # Set to true to enable, or false to disable
    host: 0.0.0.0                 # IP address to bind the REST server to
    port: 8080                    # Port number to listen on

  # This section defines whether a Prometheus endpoint is opened to allow metric scraping.
  prometheus:
    enabled: true                 # Set to true to enable, or false to disable
    host: 0.0.0.0                 # IP address to bind the Prometheus agent server to
    port: 9999                    # Port number to listen on

  # This section enables error handling or error ignoring for certain types of errors.
  errorHandling:
    consume:                      # Error handling definitions for consume errors
      log: true                   # Log errors true/false
      logPayload: true            # Upon error, should the payload of the message be dumped to the log file.
      loggerName: ConsumeError    # Definition of the error logger name.
      handler: stopOnFail         # How to proceed after encountering the error. Either continueOnFail or stopOnFail.
    process:
      log: true                   # Log errors true/false
      logPayload: true            # Upon error, should the payload of the message be dumped to the log file.
      loggerName: ProcessError    # Definition of the error logger name.
      handler: stopOnFail     # How to proceed after encountering the error. Either continueOnFail or stopOnFail.
    produce:
      log: true                   # Log errors true/false
      logPayload: true            # Upon error, should the payload of the message be dumped to the log file.
      loggerName: ProduceError    # Definition of the error logger name.
      handler: continueOnFail     # How to proceed after encountering the error. Either continueOnFail or stopOnFail.

  # This section tells KSML with which serializers / deserializers to handle certain notation types.
  notations:
    avro:                          # Definition for "avro" notation
      type: confluent_avro         # For AVRO there are two implementations: apicurio_avro and confluent_avro
      ## Below this line, specify properties to be passed into Confluent's KafkaAvroSerializer and KafkaAvroDeserializer
      config:
        # Link to Apicurio Confluent Compatibility URL
        schema.registry.url: http://schema-registry:8081/apis/ccompat/v7
        auto.register.schemas: true
        normalize.schemas: true
        # Below is an example SSL configuration for Confluent Serialization library
        # schema.registry.ssl.protocol: TLSv1.3
        # schema.registry.ssl.enabled.protocols: TLSv1.3,TLSv1.2
        # schema.registry.ssl.endpoint.identification.algorithm: ""
        # schema.registry.ssl.keystore.location: /path/to/keystore.jks
        # schema.registry.ssl.keystore.type: JKS
        # schema.registry.ssl.key.password: password
        # schema.registry.ssl.keystore.password: password
        # schema.registry.ssl.truststore.location: /path/to/truststore.jks
        # schema.registry.ssl.truststore.type: JKS
        # schema.registry.ssl.truststore.password: password

    # Definition for "other_avro" notation. With this config, you can use a type like otherAvro:SchenaName in your
    # KSML definition, which then uses the Apicurio implementation for AVRO.
    otherAvro:
      type: apicurio_avro         # For AVRO there are two implementations: apicurio_avro and confluent_avro
      ## Below this line, specify properties to be passed into Apicurio's AvroKafkaSerializer and AvroKafkaDeserializer
      config:
        apicurio.registry.url: http://schema-registry:8081/apis/registry/v2
        # Register schema if it does not exist
        apicurio.registry.auto-register: "true"
        # Apicurio Avro serialisation settings
        # apicurio.registry.avro.encoding: "BINARY"
        # apicurio.registry.headers.enabled: "true"
        # apicurio.registry.serde.IdHandler: "io.apicurio.registry.serde.Default4ByteIdHandler"
        # apicurio.registry.schema-resolver: "io.apicurio.registry.resolver.DefaultSchemaResolver"

        # Below is an example SSL configuration for Apicurio Serialization library from Apicurio documentation
        # apicurio.registry.request.ssl.keystore.location: /path/to/keystore.jks
        # apicurio.registry.request.ssl.keystore.type: JKS
        # apicurio.registry.request.ssl.keystore.password: password
        # apicurio.registry.request.ssl.key.password: password
        # apicurio.registry.request.ssl.truststore.location: /path/to/truststore.jks
        # apicurio.registry.request.ssl.truststore.type: JKS
        # apicurio.registry.request.ssl.truststore.password: password

  enableProducers: true           # Set to true to allow producer definitions to be parsed in the KSML definitions and be executed.
  enablePipelines: true           # Set to true to allow pipeline definitions to be parsed in the KSML definitions and be executed.

  # Section where you specify which KSML definitions to load, parse and execute.
  definitions:
    # Format is <namespace>: <ksml_definition_filename>
    inspect: 01-example-inspect.yaml
#    copy: 02-example-copy.yaml
#    filter: 03-example-filter.yaml
#    branch: 04-example-branch.yaml
#    route: 05-example-route.yaml
#    duplicate: 06-example-duplicate.yaml
#    convert: 07-example-convert.yaml
#    count: 08-example-count.yaml
#    aggregate: 09-example-aggregate.yaml
#    queryable_table: 10-example-queryable-table.yaml
#    field_modification: 11-example-field-modification.yaml
#    byte_manipulation: 12-example-byte-manipulation.yaml
#    join: 13-example-join.yaml
#    manual_state_store: 14-example-manual-state-store.yaml
#    pipeline_linking: 15-example-pipeline-linking.yaml
#    transform_metadata: 16-example-transform-metadata.yaml
#    inspect_with_metrics: 17-example-inspect-with-metrics.yaml
#    timestamp_extractor: 18-example-timestamp-extractor.yaml
#    performance-measurement: 19-example-performance-measurement.yaml

# This setup connects to the Kafka broker and schema registry started with the example docker-compose file
# These examples are intended to run from a inside a container on the same network
kafka:
  application.id: io.ksml.example.streaming
  # The group instance id is used to identify a single instance of the application, allowing for
  # faster rebalances and less partition reassignments. The name must be unique for each member in the group.
  group.instance.id: example-instance

  bootstrap.servers: broker:9093
  security.protocol: PLAINTEXT
  auto.offset.reset: earliest
  acks: all

  # These are Kafka SSL configuration properties. Check the documentation at1
  # Check the documentation at https://kafka.apache.org/documentation/#producerconfigs for more properties

  #  security.protocol: SSL
  #  ssl.protocol: TLSv1.3
  #  ssl.enabled.protocols: TLSv1.3,TLSv1.2
  #  ssl.endpoint.identification.algorithm: ""
  #  ssl.keystore.type: JKS
  #  ssl.truststore.type: JKS
  #  ssl.key.password: xxx
  #  ssl.keystore.password: xxx
  #  ssl.keystore.location: /path/to/ksml.keystore.jks
  #  ssl.truststore.password: xxx
  #  ssl.truststore.location: /path/to/ksml.truststore.jks

  # Use these configuration properties when connecting to a cluster using the Axual naming patterns.
  # These patterns are resolved into the actual name used on Kafka using the values in this configuration map
  # and the topic names specified in the definition YAML files
  #  tenant: "ksmldemo"
  #  instance: "dta"
  #  environment: "dev"

  # Results in Kafka topic ksmldemo-dta-dev-<topic name from KSML definition YAML>
#  topic.pattern: "{tenant}-{instance}-{environment}-{topic}"
  # Results in Kafka topic ksmldemo-dta-dev-<consumer group id>
#  group.id.pattern: "{tenant}-{instance}-{environment}-{group.id}"
  # Results in Kafka topic ksmldemo-dta-dev-<transactional id>
#  transactional.id.pattern: "{tenant}-{instance}-{environment}-{transactional.id}"
