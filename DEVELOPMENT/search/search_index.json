{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KSML Documentation","text":"<p>Welcome to the KSML documentation. KSML allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code.</p>"},{"location":"#what-is-ksml","title":"What is KSML?","text":"<p>KSML (Kafka Streams for Machine Learning) is a declarative language that makes stream processing accessible to data analysts, data engineers, and other professionals - no Java expertise required.</p> <p>Simply define your data processing pipelines in easy-to-read YAML files and write custom logic using Python. KSML handles all the complexity of Kafka Streams behind the scenes, translating your definitions into production-ready streaming applications.</p> <p>Whether you're filtering data, transforming events, performing aggregations, or building real-time analytics, KSML lets you focus on your business logic without worrying about Java code, compilation, or complex build pipelines.</p>"},{"location":"#1-getting-started","title":"1. Getting Started","text":"<ul> <li>Quick Start</li> <li>Understanding KSML</li> <li>Schema Validation Setup</li> </ul>"},{"location":"#2-tutorials","title":"2. Tutorials","text":"<ul> <li>Overview</li> <li>KSML Basics Tutorial</li> <li>Beginner Tutorials<ul> <li>Filtering and Transforming</li> <li>Different Data Formats</li> <li>Logging and Monitoring</li> </ul> </li> <li>Intermediate Tutorials<ul> <li>Branching</li> <li>Aggregations</li> <li>Joins</li> <li>Windowing</li> <li>Error Handling</li> <li>State Stores</li> </ul> </li> <li>Advanced Tutorials<ul> <li>Complex Event Processing</li> <li>Custom State Stores</li> <li>Performance Optimization</li> <li>Integration with External Systems</li> </ul> </li> </ul>"},{"location":"#3-use-case-guides","title":"3. Use Case Guides","text":"<ul> <li>Overview</li> <li>Data Transformation</li> <li>Event-Driven Applications</li> <li>IoT Data Processing</li> <li>Real-time Analytics</li> <li>Fraud Detection</li> </ul>"},{"location":"#4-references","title":"4. References","text":"<ul> <li>Overview</li> <li>KSML Definition Reference</li> <li>Pipelines</li> <li>Function Reference</li> <li>Operation Reference</li> <li>Data Types and Notations Reference</li> <li>State Store Reference</li> <li>Configuration Reference</li> </ul>"},{"location":"#latest-release","title":"Latest Release","text":"<ul> <li>Release Notes</li> </ul>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#releases","title":"Releases","text":"<ul> <li>Release Notes<ul> <li>Releases<ul> <li>1.1.0 (2025-09-18)</li> <li>1.0.8 (2025-06-20)</li> <li>1.0.7 (2025-06-09)</li> <li>1.0.6 (2025-03-24)</li> <li>1.0.5 (2025-01-14)</li> <li>1.0.4 (2024-11-22)</li> <li>1.0.3 (2024-10-18)</li> <li>1.0.2 (2024-09-20)</li> <li>1.0.1 (2024-07-17)</li> <li>1.0.0 (2024-06-28)</li> <li>0.8.0 (2024-03-08)</li> <li>0.9.1 (2024-06-21)</li> <li>0.9.0 (2024-06-05)</li> <li>0.2.2 (2024-01-30)</li> <li>0.2.1 (2023-12-20)</li> <li>0.2.0 (2023-12-07)</li> <li>0.1.0 (2023-03-15)</li> <li>0.0.4 (2022-12-02)</li> <li>0.0.3 (2021-07-30)</li> <li>0.0.2 (2021-06-28)</li> <li>0.0.1 (2021-04-30)</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/#110-2025-09-18","title":"1.1.0 (2025-09-18)","text":""},{"location":"release-notes/#breaking-changes","title":"BREAKING CHANGES","text":""},{"location":"release-notes/#state-store-configuration-136","title":"State Store Configuration (#136)","text":"<p>The <code>store</code> property in aggregate operations is now required.</p> <p>Migration Required: <pre><code># Before (1.0.8)\n- type: aggregate\n  initializer:\n    expression: 0\n    resultType: long\n  aggregator:\n    expression: aggregatedValue+1\n    resultType: long\n\n# After (1.1.0) - store is now required\n- type: aggregate\n  store:                    # REQUIRED\n    type: window\n    windowSize: 10m\n    retention: 1h\n  initializer:\n    expression: 0\n    resultType: long\n  aggregator:\n    expression: aggregatedValue+1\n    resultType: long\n</code></pre> See example: 09-example-aggregate.yaml</p>"},{"location":"release-notes/#key-new-features","title":"KEY NEW FEATURES","text":""},{"location":"release-notes/#jsonschema-support","title":"JsonSchema Support","text":"<p>Full JsonSchema support added for better data validation and schema management. Follows JSON Schema specification where missing <code>additionalProperties</code> defaults to <code>true</code> (allowing additional fields).</p> <p>Tutorial: Working with JsonSchema Data</p>"},{"location":"release-notes/#notation-and-schema-registry-configuration","title":"Notation and Schema Registry Configuration","text":"<p>New configuration system for data serialization and schema registries. Configure different implementations for each notation type (Avro, Protobuf, JsonSchema) and schema registry connections.</p> <p>Tutorial: Working with JsonSchema Data</p> <p>Example: ksml-runner.yaml <pre><code>ksml:\n  schemaRegistries:\n    confluent:\n      config:\n        schema.registry.url: http://confluent-schema-registry:8081\n    apicurio:\n      config:\n        apicurio.registry.url: http://apicurio-registry:8080\n\n  notations:\n    avro:\n      type: confluent_avro\n      schemaRegistry: confluent\n    jsonschema:\n      type: apicurio_json\n      schemaRegistry: apicurio\n    protobuf:\n      type: apicurio_protobuf\n      schemaRegistry: apicurio\n</code></pre></p>"},{"location":"release-notes/#protobuf-support-beta","title":"Protobuf Support (BETA)","text":"<p>Added experimental Protocol Buffers support with both Confluent and Apicurio registry compatibility.</p> <p>Example: 00-example-generate-sensordata-protobuf.yaml <pre><code>producers:\n  sensordata_protobuf_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: ksml_sensordata_protobuf\n      keyType: string\n      valueType: protobuf:sensor_data  # New protobuf type\n</code></pre></p>"},{"location":"release-notes/#enhanced-type-system","title":"Enhanced Type System","text":"<ul> <li>List and Tuple Types (#285)</li> <li>Map Type (#269)</li> <li>Multiple message generation support for producers</li> </ul>"},{"location":"release-notes/#kubernetes-monitoring","title":"Kubernetes &amp; Monitoring","text":"<ul> <li>Helm Charts for production deployments (#85)</li> <li>Kafka Streams Metrics Reporter with KSML tag enrichment</li> <li>Health Probes: Separate liveness, readiness, and startup probes</li> <li>NetworkPolicy and PrometheusRules support</li> </ul>"},{"location":"release-notes/#improvements","title":"IMPROVEMENTS","text":""},{"location":"release-notes/#syntax-enhancements-133","title":"Syntax Enhancements (#133)","text":"<p>Enhanced \"to\" operation with clearer definitions. Both syntaxes continue to work: <pre><code># Simple syntax (still supported)\nto: my-topic\n</code></pre> <pre><code># Detailed syntax (recommended for clarity)\nto:\n  topic: my-topic\n  keyType: string\n  valueType: json\n</code></pre></p>"},{"location":"release-notes/#bug-fixes","title":"BUG FIXES","text":"<ul> <li>Fixed AVRO CharSequence crash with nested objects (#163)</li> <li>Resolved excessive CPU usage issue (#157)</li> <li>Fixed multiple join operation issues (#136, #143, #225)</li> <li>Storage and state management improvements</li> <li>Apicurio Registry 2.x compatibility</li> </ul>"},{"location":"release-notes/#documentation-examples","title":"DOCUMENTATION &amp; EXAMPLES","text":""},{"location":"release-notes/#improved-documentation-with-docker-compose-examples","title":"Improved Documentation with Docker Compose Examples","text":"<p>Updated KSML documentation with:</p> <ul> <li>Working examples that run in Docker Compose</li> <li>Step-by-step tutorials from beginner to advanced</li> <li>Real-world use cases with complete implementations</li> <li>Interactive testing - all examples can be run locally</li> </ul> <p>Key Resources:</p> <ul> <li>Getting Started - Quick start guide with Docker Compose</li> <li>Tutorials - From basics to advanced patterns</li> <li>Examples - 20+ working examples</li> <li>Reference - Complete KSML language reference</li> </ul>"},{"location":"release-notes/#infrastructure-updates","title":"INFRASTRUCTURE UPDATES","text":"<ul> <li>Upgraded to Kafka 4.0.0 (#151)</li> <li>Java 23 Security Manager support</li> <li>Multi-architecture Docker builds (ARM64/AMD64)</li> <li>Configurable GraalVM security options</li> </ul>"},{"location":"release-notes/#108-2025-06-20","title":"1.0.8 (2025-06-20)","text":"<p>KSML</p> <ul> <li>Axual header cleaning interceptor to KSML</li> </ul> <p>Helm charts</p> <ul> <li>Run as Job</li> <li>Prometheus alert rules</li> <li>Network policies</li> </ul>"},{"location":"release-notes/#107-2025-06-09","title":"1.0.7 (2025-06-09)","text":"<ul> <li>Fix KSML crash when topic creation is needed and pattern configurations were provided</li> </ul>"},{"location":"release-notes/#106-2025-03-24","title":"1.0.6 (2025-03-24)","text":"<ul> <li>Fix storage issues, Persistent Volumes always created and never cleaned</li> <li>Fix slow and failing builds with multiple architectures</li> </ul>"},{"location":"release-notes/#105-2025-01-14","title":"1.0.5 (2025-01-14)","text":"<ul> <li>Fix store serde regression</li> </ul>"},{"location":"release-notes/#104-2024-11-22","title":"1.0.4 (2024-11-22)","text":"<ul> <li>Fix crash when using Avro CharSequence encodings and nested objects</li> </ul>"},{"location":"release-notes/#103-2024-10-18","title":"1.0.3 (2024-10-18)","text":"<ul> <li>Fix high CPU usage</li> <li>Upgrade to Avro 1.11.4 to fix CVE-2024-47561</li> </ul>"},{"location":"release-notes/#102-2024-09-20","title":"1.0.2 (2024-09-20)","text":"<p>KSML</p> <ul> <li>Upgrade to Kafka Streams 3.8.0</li> <li>Avro Schema Registry settings no longer required if Avro not used </li> <li>Add missing object in KSML Json Schema</li> <li>Fix serialisation and list handling issues</li> </ul> <p>Helm charts</p> <ul> <li>Use liveness and readiness and startup probes to fix state issues</li> <li>Fix conflicting default configuration Prometheus export and ServiceMonitor</li> </ul>"},{"location":"release-notes/#101-2024-07-17","title":"1.0.1 (2024-07-17)","text":"<ul> <li>Topology Optimization can be applied</li> <li>Runtime dependencies, like LZ4 compression support, are back in the KSML image</li> <li>Fix parse error messages during join</li> <li>Fix windowed aggregation flow errors</li> <li>Update windowed object support in multiple operations and functions</li> </ul>"},{"location":"release-notes/#100-2024-06-28","title":"1.0.0 (2024-06-28)","text":"<ul> <li>Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language   specification. This allows for better syntax checking in IDEs.</li> <li>Lots of small fixes and completion modifications.</li> </ul>"},{"location":"release-notes/#091-2024-06-21","title":"0.9.1 (2024-06-21)","text":"<ul> <li>Fix failing test in GitHub Actions during release</li> <li>Unified build workflows</li> </ul>"},{"location":"release-notes/#090-2024-06-05","title":"0.9.0 (2024-06-05)","text":"<ul> <li>Collectable metrics</li> <li>New topology test suite</li> <li>Python context hardening</li> <li>Improved handling of Kafka tombstones</li> <li>Added flexibility to producers (single shot, n-shot, or user condition-based)</li> <li>JSON Logging support</li> <li>Bumped GraalVM to 23.1.2</li> <li>Bumped several dependency versions</li> <li>Several fixes and security updates</li> </ul>"},{"location":"release-notes/#080-2024-03-08","title":"0.8.0 (2024-03-08)","text":"<ul> <li>Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification:<ul> <li>docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness.</li> <li>examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion.</li> </ul> </li> <li>Improved schema handling:<ul> <li>Better compatibility checking between schema fields.</li> </ul> </li> <li>Improved support for state stores:<ul> <li>Update to state store typing and handling.</li> <li>Manual state stores can be defined and referenced in pipelines.</li> <li>Manual state stores are also available in Python functions.</li> <li>State stores can be used 'side-effect-free' (e.g. no Avro schema registration)</li> </ul> </li> <li>Python function improvements:<ul> <li>Automatic variable assignment for state stores.</li> <li>Every Python function can use a Java Logger, integrating Python output with KSML log output.</li> <li>Type inference in situations where parameters or result types can be derived from the context.</li> </ul> </li> <li>Lots of small language updates:<ul> <li>Improve readability for store types, filter operations and windowing operations</li> <li>Introduction of the \"as\" operation, which allows for pipeline referencing and chaining.</li> </ul> </li> <li>Better data type handling:<ul> <li>Separation of data types and KSML core, allowing for easier addition of new data types in the future.</li> <li>Automatic conversion of data types, removing common pipeline failure scenarios.</li> <li>New implementation for CSV handling.</li> </ul> </li> <li>Merged the different runners into a single runner.<ul> <li>KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies).</li> <li>Removal of Kafka and Axual backend distinctions.</li> </ul> </li> <li>Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace).</li> <li>Examples updated to reflect the latest definition format.</li> <li>Documentation updated.</li> </ul>"},{"location":"release-notes/#022-2024-01-30","title":"0.2.2 (2024-01-30)","text":"<ul> <li>Fix KSML java process not stopping on exception</li> <li>Fix stream-stream join validation and align other checks</li> <li>Bump logback to 1.4.12</li> <li>Fix to enable Streams optimisations to be applied to topology</li> <li>Fix resolving admin client issues causing warning messages</li> </ul>"},{"location":"release-notes/#021-2023-12-20","title":"0.2.1 (2023-12-20)","text":"<ul> <li>Fixed an issue with Avro and field validations</li> </ul>"},{"location":"release-notes/#020-2023-12-07","title":"0.2.0 (2023-12-07)","text":"<ul> <li>Optimized Docker build</li> <li>Merged KSML Runners into one module, optimize Docker builds and workflow</li> <li>KSML documentation updates</li> <li>Docker image, GraalPy venv, install and GU commands fail</li> <li>Update GitHub Actions</li> <li>Small robustness improvements</li> <li>Issue #72 - Fix build failures when trying to use venv and install python packages</li> <li>Manual state store support, Kafka client cleanups and configuration changes</li> <li>Update and clean up dependencies</li> <li>Update documentation to use new runner configurations</li> <li>Update to GraalVM for JDK 21 Community</li> </ul>"},{"location":"release-notes/#010-2023-03-15","title":"0.1.0 (2023-03-15)","text":"<ul> <li>Added XML/SOAP support</li> <li>Added data generator</li> <li>Added Automatic Type Conversion</li> <li>Added Schema Support for XML, Avro, JSON, Schema</li> <li>Added Basic Error Handling</li> </ul>"},{"location":"release-notes/#004-2022-12-02","title":"0.0.4 (2022-12-02)","text":"<ul> <li>Update to Kafka 3.2.3</li> <li>Update to Java 17</li> <li>Support multiple architectures in KSML, linux/amd64 and linux/arm64</li> <li>Refactored internal typing system, plus some fixes to store operations</li> <li>Introduce queryable state stores</li> <li>Add better handling of NULL keys and values from Kafka</li> <li>Implement schema support</li> <li>Added Docker multistage build</li> <li>Bug fix for windowed objects</li> <li>Store improvements</li> <li>Support Liberica NIK</li> <li>Switch from Travis CI to GitHub workflow</li> <li>Build snapshot Docker image on pull request merged</li> </ul>"},{"location":"release-notes/#003-2021-07-30","title":"0.0.3 (2021-07-30)","text":"<ul> <li>Support for Python 3 through GraalVM</li> <li>improved data structuring</li> <li>bug fixes</li> </ul>"},{"location":"release-notes/#002-2021-06-28","title":"0.0.2 (2021-06-28)","text":"<ul> <li>Added JSON support, Named topology and name store supported</li> </ul>"},{"location":"release-notes/#001-2021-04-30","title":"0.0.1 (2021-04-30)","text":"<ul> <li>First alpha release </li> </ul>"},{"location":"getting-started/basics-tutorial/","title":"KSML Basics Tutorial","text":"<p>This tutorial will guide you through creating your first KSML data pipeline. By the end, you'll understand the basic components of KSML and how to create a simple but functional data processing application.</p>"},{"location":"getting-started/basics-tutorial/#what-youll-build","title":"What You'll Build","text":"<p>In this tutorial, you'll build a simple data pipeline that:</p> <ol> <li>Reads temperature sensor data from a Kafka topic</li> <li>Filters out readings below a certain threshold</li> <li>Transforms the data by converting Fahrenheit to Celsius</li> <li>Writes the processed data to another Kafka topic</li> <li>Logs information about the processed messages</li> </ol> <p>Here's a visual representation of what we'll build:</p> <pre><code>Input Topic \u2192 Filter \u2192 Transform \u2192 Output Topic\n                \u2193\n              Logging\n</code></pre>"},{"location":"getting-started/basics-tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Completed the Installation and Setup guide with Docker Compose running</li> <li>Basic understanding of YAML syntax</li> <li>Your KSML environment running (<code>docker compose ps</code> should show all services as \"Up\")</li> </ul>"},{"location":"getting-started/basics-tutorial/#choose-your-setup-method","title":"Choose Your Setup Method","text":"<p>Option A: Quick Start (Recommended) Download the pre-configured tutorial files and start immediately:</p> <ol> <li>Download and extract: local-docker-compose-setup-basics-tutorial.zip</li> <li>Navigate to the extracted folder</li> <li>Run <code>docker compose up -d</code> (if not already running)</li> <li>Skip to Step 5: Run Your Pipeline</li> </ol> <p>Option B: Step-by-Step Tutorial Use the docker-compose.yml from the Quick Start Tutorial and manually create/modify the producer and processor definitions as described below to learn each component step by step.</p>"},{"location":"getting-started/basics-tutorial/#understanding-the-ksml-file-structure","title":"Understanding the KSML File Structure","text":"<p>A KSML definition file consists of three main sections:</p> <ol> <li>Streams: Define the input and output Kafka topics</li> <li>Functions: Define reusable code snippets</li> <li>Pipelines: Define the data processing flow</li> </ol> <p>Let's create each section step by step.</p>"},{"location":"getting-started/basics-tutorial/#step-1-define-your-streams","title":"Step 1: Define Your Streams","text":"<p>Note: Skip Steps 1-4 if you chose Option A above.</p> <p>First, let's create a new file <code>tutorial.yaml</code> and start by defining the input and output streams for our pipeline:</p> Input and output streams for our pipeline (click to expand) <pre><code>streams:\n  input_stream:\n    topic: temperature_data\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: temperature_data_converted\n    keyType: string\n    valueType: json\n</code></pre> <p>This defines:</p> <ul> <li>An input stream reading from the <code>temperature_data</code> topic with string keys and JSON values</li> <li>An output stream writing to the <code>temperature_data_converted</code> topic with the same data types</li> </ul>"},{"location":"getting-started/basics-tutorial/#understanding-stream-definitions","title":"Understanding Stream Definitions","text":"<p>Each KSML stream has:</p> <ul> <li>A unique name (e.g., <code>input_stream</code>)</li> <li>The Kafka topic it connects to</li> <li>The data types for keys and values</li> </ul> <p>KSML supports various data types and notations including:</p> <ul> <li><code>string</code>: For text data</li> <li><code>json</code>: For JSON-formatted data</li> <li><code>avro</code>: For Avro-formatted data (requires schema)</li> <li><code>binary</code>: For raw binary data</li> <li>And more</li> </ul>"},{"location":"getting-started/basics-tutorial/#step-2-create-a-simple-function","title":"Step 2: Create a Simple Function","text":"<p>Next, let's add functions to filter, transform and log messages as they flow through our pipeline:</p> Functions to filter, transform and log messages to filter (click to expand) <pre><code>functions:\n  temperature_above_threshold:\n    type: predicate\n    expression: value.get('temperature', 0) &gt; 70\n\n  fahrenheit_to_celsius:\n    type: valueTransformer\n    expression: |\n      {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9}\n    resultType: struct\n\n  log_message:\n    type: forEach\n    parameters:\n      - name: prefix\n        type: string\n    code: |\n      msg = prefix + \" message\" if isinstance(prefix, str) and prefix != \"\" else \"Message\"\n      log.info(\"{}: key={}, value={}\", msg, key, value)\n</code></pre> <p>We defined three uniquely named functions:</p> <p><code>temperature_above_threshold</code> function:</p> <ul> <li>Is of type <code>predicate</code>, which means it always gets a <code>key</code> and   <code>value</code> as its parameters and needs to return a <code>boolean</code> value</li> <li>Uses the <code>expression</code> tag to return a <code>True</code> if the <code>temperature</code> field in the value (zero if it does not exist) is   above 70, <code>False</code> otherwise.</li> </ul> <p><code>fahrenheit_to_celsius</code> function:</p> <ul> <li>Is of type <code>valueTransformer</code>, which means it always gets   two parameters <code>key</code> and <code>value</code>, and returns a (modified, or transformed) value for the next processing step</li> <li>Uses the <code>expression</code> tag to define the value to return, in this case renaming <code>temperature</code> to <code>temp_fahrenheit</code> and   adding a field called <code>temp_celsius</code></li> </ul> <p><code>log_message</code> function:</p> <ul> <li>Is of type <code>forEach</code>, which means it always gets two parameters   <code>key</code> and <code>value</code>, and does not return a value</li> <li>Takes an extra parameter <code>prefix</code> of type <code>string</code></li> <li>Checks if the <code>prefix</code> variable is of the correct type, and not empty, and if so, uses it to compose a description for   the log message</li> <li>Uses the built-in <code>log</code> object to output information at <code>info</code> level</li> </ul>"},{"location":"getting-started/basics-tutorial/#understanding-functions-in-ksml","title":"Understanding Functions in KSML","text":"<p>Functions in KSML:</p> <ul> <li>Can be reused across multiple operations in your pipelines</li> <li>Are written in Python</li> <li>Have access to pre-defined parameters based on function type</li> <li>Can take additional parameters for more flexibility</li> <li>Must return a value if required by the function type</li> </ul>"},{"location":"getting-started/basics-tutorial/#step-3-build-your-pipeline","title":"Step 3: Build Your Pipeline","text":"<p>Now, let's add the pipeline that processes our data:</p> Adding Pipelines (click to expand) <pre><code>pipelines:\n  tutorial_pipeline:\n    from: input_stream\n    via:\n      - type: filter\n        if: temperature_above_threshold\n      - type: transformValue\n        mapper: fahrenheit_to_celsius\n      - type: peek\n        forEach:\n          code: |\n            # Manually call log_message to pass in additional parameters\n            log_message(key, value, prefix=\"Processed\")\n    to: output_stream\n</code></pre> <p>This pipeline:</p> <ol> <li>Reads from <code>input_stream</code></li> <li>Filters out messages where the temperature is 70\u00b0F or lower</li> <li>Transforms the values to include both Fahrenheit and Celsius temperatures</li> <li>Logs each processed message</li> <li>Writes the results to <code>output_stream</code></li> </ol>"},{"location":"getting-started/basics-tutorial/#understanding-pipeline-operations","title":"Understanding Pipeline Operations","text":"<p>Let's break down each operation:</p>"},{"location":"getting-started/basics-tutorial/#filter-operation","title":"Filter Operation","text":"Adding Filters (click to expand) <pre><code>      - type: filter\n        if: temperature_above_threshold\n</code></pre> <p>The filter operation:</p> <ul> <li>Evaluates the expression for each message</li> <li>Only passes messages where the expression returns <code>True</code></li> <li>Discards messages where the expression returns <code>False</code></li> </ul>"},{"location":"getting-started/basics-tutorial/#transform-value-operation","title":"Transform Value Operation","text":"Adding Transforming Value Operation (click to expand) <pre><code>      - type: transformValue\n        mapper: fahrenheit_to_celsius\n</code></pre> <p>The transformValue operation:</p> <ul> <li>Transforms the value of each message</li> <li>Keeps the original key unchanged</li> <li>Creates a new value based on the expression</li> <li>In this case, creates a new JSON object with the original temperature and a calculated Celsius value</li> </ul> <p>Note that we put the expression on a new line in this example to force the KSML YAML parser to interpret the expression as a literal string for Python, instead of parsing it as part of the YAML syntax. Another way of achieving the same would be to surround the '{...}' with quotes, but in that case, be aware of consistent single/double quoting to not confuse the KSML parser and/or the Python interpreter. We generally recommend using the above notation for readability purposes.</p>"},{"location":"getting-started/basics-tutorial/#peek-operation","title":"Peek Operation","text":"Adding Peek Operation (click to expand) <pre><code>      - type: peek\n        forEach:\n          code: |\n            # Manually call log_message to pass in additional parameters\n            log_message(key, value, prefix=\"Processed\")\n</code></pre> <p>The peek operation:</p> <ul> <li>Executes the provided code for each message</li> <li>Doesn't modify the message</li> <li>Allows the message to continue through the pipeline</li> <li>Is useful for logging, metrics, or other side effects</li> </ul>"},{"location":"getting-started/basics-tutorial/#step-4-put-it-all-together","title":"Step 4: Put It All Together","text":"<p>Let's combine all the sections into a complete KSML definition file. In the Quick Start guide you created a directory structure containing an <code>examples/</code> directory; in this directory create a file called <code>tutorial.yaml</code> and copy the following content:</p> Full KSML processing definition (click to expand) <pre><code>streams:\n  input_stream:\n    topic: temperature_data\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: temperature_data_converted\n    keyType: string\n    valueType: json\n\nfunctions:\n  temperature_above_threshold:\n    type: predicate\n    expression: value.get('temperature', 0) &gt; 70\n\n  fahrenheit_to_celsius:\n    type: valueTransformer\n    expression: |\n      {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9}\n    resultType: struct\n\n  log_message:\n    type: forEach\n    parameters:\n      - name: prefix\n        type: string\n    code: |\n      msg = prefix + \" message\" if isinstance(prefix, str) and prefix != \"\" else \"Message\"\n      log.info(\"{}: key={}, value={}\", msg, key, value)\n\npipelines:\n  tutorial_pipeline:\n    from: input_stream\n    via:\n      - type: filter\n        if: temperature_above_threshold\n      - type: transformValue\n        mapper: fahrenheit_to_celsius\n      - type: peek\n        forEach:\n          code: |\n            # Manually call log_message to pass in additional parameters\n            log_message(key, value, prefix=\"Processed\")\n    to: output_stream\n</code></pre> <p>Save the file. We also need to make the KSML Runner aware of the new pipeline. In the <code>ksml-runner.yaml</code> you created before, there is a section containing the definitions; modify this part so that it looks like this:</p> KSML Runner Configuration Update (click to expand) <pre><code>ksml:\n  definitions:\n    # format is: &lt;namespace&gt;: &lt;filename&gt; \n    tutorial: tutorial.yaml\n</code></pre> <p>You can either replace the line containing <code>helloworld</code> or add the tutorial, in the latter case both pipelines will be run.</p>"},{"location":"getting-started/basics-tutorial/#step-5-run-your-pipeline-definitions","title":"Step 5: Run Your Pipeline definition(s)","text":"<p>Now let's run the pipeline using our Docker Compose setup. If the compose was still running, you can just restart the KSML runner to make it aware of the new definition:</p> <pre><code>docker compose restart ksml\n</code></pre> <p>Or, if you stopped the setup, you can start the complete compose as before:</p> <pre><code>docker compose up -d \n</code></pre> <p>Check the logs to verify your pipeline(s) started:</p> <pre><code>docker compose logs ksml\n</code></pre>"},{"location":"getting-started/basics-tutorial/#step-51-test-with-sample-data","title":"Step 5.1: Test with Sample Data","text":"<p>Produce some test messages to the input topic:</p> <pre><code>docker compose exec broker kafka-console-producer.sh --bootstrap-server broker:9093 --topic temperature_data --property \"parse.key=true\" --property \"key.separator=:\"\n</code></pre> <p>Then enter messages in the format <code>key:value</code>:</p> <pre><code>sensor1:{\"temperature\": 75}\nsensor2:{\"temperature\": 65}\nsensor3:{\"temperature\": 80}\n</code></pre> <p>Press  after each record, and press Ctrl+C to exit the producer."},{"location":"getting-started/basics-tutorial/#step-52-view-the-results","title":"Step 5.2: View the Results","text":"<p>Consume messages from the output topic to see the results:</p> <pre><code>docker compose exec broker kafka-console-consumer.sh --bootstrap-server broker:9093 --topic temperature_data_converted --from-beginning\n</code></pre> <p>You can also view the topics and messages in the Kafka UI at http://localhost:8080.</p> <p>You should see messages like:</p> <pre><code>{\"sensor\":\"sensor1\",temp_fahrenheit\":75,\"temp_celsius\":23.88888888888889}\n{\"sensor\":\"sensor3\",temp_fahrenheit\":80,\"temp_celsius\":26.666666666666668}\n</code></pre> <p>Notice that:</p> <ul> <li>The message with temperature 65\u00b0F was filtered out (below our 70\u00b0F threshold)</li> <li>The remaining messages have been transformed to include both Fahrenheit and Celsius temperatures</li> <li>You can see processing logs in the KSML container logs: <code>docker compose logs ksml</code></li> </ul>"},{"location":"getting-started/basics-tutorial/#understanding-whats-happening","title":"Understanding What's Happening","text":"<p>When you run your KSML definition:</p> <ol> <li>The KSML runner parses your <code>tutorial.yaml</code> definition</li> <li>It creates a Kafka Streams topology based on your pipeline definition</li> <li>The topology starts consuming from the input topic</li> <li>Each message flows through the operations you defined:<ul> <li>The filter operation drops messages with temperatures \u2264 70\u00b0F</li> <li>The mapValues operation transforms the remaining messages</li> <li>The peek operation logs each message</li> <li>The messages are written to the output topic</li> </ul> </li> </ol>"},{"location":"getting-started/basics-tutorial/#using-ksml-to-produce-messages","title":"Using KSML to produce messages","text":"<p>While you can manually produce the above messages, KSML can also generate messages for you automatically.</p> <p>Create a new file called <code>producer.yaml</code> in your <code>examples/</code> directory:</p> Producer Definition - producer.yaml (click to expand) <pre><code>functions:\n  generate_temperature_message:\n    type: generator\n    globalCode: |\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\" + str(sensorCounter)         # Simulate 10 sensors \"sensor0\" to \"sensor9\"\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      value = {\"temperature\": random.randrange(150)}\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Indicate the type of key and value\n\nproducers:\n  # Produce a temperature message every 3 seconds\n  temperature_producer:\n    generator: generate_temperature_message\n    interval: 3s\n    to:\n      topic: temperature_data\n      keyType: string\n      valueType: json\n</code></pre> <p>Now update your <code>ksml-runner.yaml</code> to include the producer definition:</p> KSML Runner Configuration Update (click to expand) <pre><code>ksml:\n  definitions:\n    tutorial: tutorial.yaml\n    producer: producer.yaml     # Add this line\n</code></pre> <p>Restart the KSML Runner to load the new producer:</p> <pre><code>docker compose restart ksml\n</code></pre> <p>You can check the runner logs (<code>docker compose logs ksml</code>) or go to the Kafka UI at http://localhost:8080 to verify that new messages are generated every 3 seconds in the <code>temperature_data</code> topic. The filtered and converted messages will appear on the <code>temperature_data_converted</code> topic.</p>"},{"location":"getting-started/basics-tutorial/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've built your first KSML data pipeline. Here are some ways to expand on what you've learned:</p>"},{"location":"getting-started/basics-tutorial/#try-these-modifications","title":"Try These Modifications:","text":"<ol> <li>Add another filter condition (e.g., filter by sensor name)</li> <li>Add more fields to the transformed output</li> <li>Create a second pipeline that processes the data differently</li> </ol>"},{"location":"getting-started/basics-tutorial/#continue-your-learning-journey","title":"Continue Your Learning Journey:","text":"<ul> <li>Check out the beginner tutorials for more guided examples</li> <li>Dive into the reference documentation to learn about all available   operations</li> </ul>"},{"location":"getting-started/introduction/","title":"Understanding KSML","text":"<p>Now that you've seen KSML in action with the Quick Start, let's understand what just happened and why KSML makes stream processing so much simpler.</p>"},{"location":"getting-started/introduction/#what-you-just-saw","title":"What You Just Saw","text":"<p>In the Quick Start, you set up a complete stream processing pipeline with just:</p> <ul> <li>A simple Docker Compose YAML configuration file </li> <li>A YAML KSML definition file that describes the Kafka Streams topology</li> <li>A YAML KSML configuration file that configures the KSML container</li> <li>No Java code, no compilation, no complex setup</li> </ul> <p>That pipeline was:</p> <ol> <li>Reading temperature messages from a Kafka topic</li> <li>Processing each message through your KSML pipeline  </li> <li>Writing the results to another topic</li> </ol> <p>Behind the scenes, KSML converted your YAML definition into a full Kafka Streams application.</p>"},{"location":"getting-started/introduction/#why-ksml-exists","title":"Why KSML Exists","text":"<p>Kafka Streams is at the heart of many organizations' event sourcing, analytical processing, real-time, batch or ML workloads. It has a beautiful Java DSL that allows developers to focus on what their applications need to do, not how to do it. But the Java requirement also holds people back. If you don't know Java, does that mean you cannot use Kafka Streams?</p> <p>Enter KSML.</p> <p>KSML is a wrapper language and interpreter around Kafka Streams that lets you express any topology in a simple YAML syntax. Simply define your topology as a processing pipeline with a series of steps that your data passes through. Your custom functions can be expressed inline in Python. KSML reads your definition and constructs the topology dynamically via the Kafka Streams DSL.</p>"},{"location":"getting-started/introduction/#what-is-ksml","title":"What is KSML?","text":"<p>KSML is a declarative language that allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code. KSML makes stream processing accessible to a wider audience by removing the need for Java development skills and complex build pipelines.</p> <p>With KSML, you can:</p> <ul> <li>Define streaming data pipelines using simple YAML syntax</li> <li>Process data using Python functions embedded directly in your KSML definitions</li> <li>Deploy applications without compiling Java code</li> <li>Rapidly prototype and iterate on streaming applications</li> </ul>"},{"location":"getting-started/introduction/#why-use-ksml","title":"Why Use KSML?","text":""},{"location":"getting-started/introduction/#simplified-development","title":"Simplified Development","text":"<p>KSML dramatically reduces the complexity of building Kafka Streams applications. Instead of writing hundreds of lines of Java code, you can define your streaming logic in a concise YAML file with embedded Python functions.</p>"},{"location":"getting-started/introduction/#no-java-required","title":"No Java Required","text":"<p>While Kafka Streams is a powerful Java library, KSML eliminates the need to write Java code. This opens up Kafka Streams to data engineers, analysts, and other professionals who may not have Java expertise.</p>"},{"location":"getting-started/introduction/#rapid-prototyping","title":"Rapid Prototyping","text":"<p>KSML allows you to quickly prototype and test streaming applications. Changes can be made to your KSML definition and deployed immediately, without a compile-package-deploy cycle.</p>"},{"location":"getting-started/introduction/#full-access-to-kafka-streams-capabilities","title":"Full Access to Kafka Streams Capabilities","text":"<p>KSML provides access to the full power of Kafka Streams, including:</p> <ul> <li>Stateless operations (map, filter, etc.)</li> <li>Stateful operations (aggregate, count, etc.)</li> <li>Windowing operations</li> <li>Stream and table joins</li> <li>And more</li> </ul>"},{"location":"getting-started/introduction/#key-concepts-and-terminology","title":"Key Concepts and Terminology","text":""},{"location":"getting-started/introduction/#streams","title":"Streams","text":"<p>In KSML, a stream represents a flow of data from or to a Kafka topic. Streams are defined with a name, a topic, and data types for keys and values.</p> <pre><code>streams:\n  my_input_stream:\n    topic: input-topic\n    keyType: string\n    valueType: json\n</code></pre>"},{"location":"getting-started/introduction/#functions","title":"Functions","text":"<p>Functions in KSML are reusable pieces of Python code that can be called from your pipelines. They can transform data, filter messages, or perform side effects like logging.</p> <pre><code>functions:\n  log_message:\n    type: forEach\n    code: |\n      log.info(\"Processing message with key: {}\", key)\n</code></pre>"},{"location":"getting-started/introduction/#pipelines","title":"Pipelines","text":"<p>Pipelines define the flow and processing of data. A pipeline starts with a source stream, applies a series of operations, and typically ends with a sink operation that writes to another stream or performs a terminal action.</p> <pre><code>pipelines:\n  my_pipeline:\n    from: my_input_stream\n    via:\n      - type: filter\n        if:\n          expression: value.get('temperature') &gt; 70\n      - type: mapValues\n        mapper:\n          expression: |\n            {\"temp_celsius\": (value.get('temperature') - 32) * 5/9}\n    to: my_output_stream\n</code></pre>"},{"location":"getting-started/introduction/#operations","title":"Operations","text":"<p>Operations are the building blocks of pipelines. They define how data is processed as it flows through a pipeline. KSML supports a wide range of operations, including:</p> <ul> <li>Stateless operations: filter, map, flatMap, peek</li> <li>Stateful operations: aggregate, count, reduce</li> <li>Joining operations: join, leftJoin, outerJoin</li> <li>Windowing operations: windowedBy</li> <li>Sink operations: to, forEach, branch</li> </ul>"},{"location":"getting-started/introduction/#how-ksml-relates-to-kafka-streams","title":"How KSML Relates to Kafka Streams","text":"<p>KSML is built on top of Kafka Streams and translates your YAML definitions into Kafka Streams topologies. This means:</p> <ol> <li>You get all the benefits of Kafka Streams (exactly-once processing, fault tolerance, scalability)</li> <li>Your KSML applications can integrate with existing Kafka Streams applications</li> <li>Performance characteristics are similar to native Kafka Streams applications</li> </ol> <p>When you run a KSML definition, the KSML runner:</p> <ol> <li>Parses your YAML definition</li> <li>Translates it into a Kafka Streams topology</li> <li>Executes the topology using the Kafka Streams runtime</li> </ol> <p>This translation happens automatically, allowing you to focus on your business logic rather than the details of a normal Kafka Streams implementation.</p>"},{"location":"getting-started/introduction/#next-steps","title":"Next Steps","text":"<p>Now that you understand what KSML is and its key concepts, you can:</p> <ul> <li>Set up Schema Validation in your IDE for better development experience</li> <li>Explore the Reference Documentation in more detail</li> <li>Follow the KSML Basics Tutorial to build your first application</li> </ul>"},{"location":"getting-started/producer-tutorial/","title":"Producer Tutorial","text":"<p>KSML producers let you quickly send test messages to Kafka in different formats like JSON, Avro, Protobuf, XML, CSV, and Binary. Write simple Python code to generate your data, and KSML handles the rest - no building or compiling needed.</p> <p>In this tutorial, you'll learn all the properties and features of KSML producers.</p>"},{"location":"getting-started/producer-tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Complete the Quick Start Guide</li> <li>Have the Docker Compose environment running</li> <li>Add the following topic to your <code>kafka-setup</code> service in <code>docker-compose.yml</code> to run the examples:</li> </ul> Topic creation commands (click to expand) <pre><code># Producer Tutorial Topics\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic my_topic \\\n</code></pre>"},{"location":"getting-started/producer-tutorial/#basic-producer-structure","title":"Basic Producer Structure","text":"<p>A KSML producer definition consists of two main parts:</p> <ol> <li>Functions: Define the generator function that creates messages</li> <li>Producers: Configure how and when messages are produced</li> </ol> <p>Here's the simplest possible producer:</p> Simplest Producer (click to expand) <pre><code>functions:\n  simple_generator:\n    type: generator\n    code: |\n      key = \"key1\"\n      value = {\"message\": \"Hello, Kafka!\"}\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  my_producer:\n    generator: simple_generator\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>This producer will generate one JSON message and stop, because no <code>interval</code>, <code>count</code>, or <code>until</code> condition is specified.</p>"},{"location":"getting-started/producer-tutorial/#producer-properties-reference","title":"Producer Properties Reference","text":"<p>Let's explore all available properties for producer definitions:</p>"},{"location":"getting-started/producer-tutorial/#required-properties","title":"Required Properties","text":""},{"location":"getting-started/producer-tutorial/#generator-required","title":"<code>generator</code> (required)","text":"<p>The function that generates messages. Must be a function of type <code>generator</code>.</p> <pre><code>producers:\n  my_producer:\n    generator: my_generator_function  # References a function defined in the functions section\n    to: my_topic\n</code></pre> <p>The generator function must return:</p> <ul> <li>A tuple <code>(key, value)</code> for a single message, or</li> <li>A list of tuples <code>[(key1, value1), (key2, value2), ...]</code> for multiple messages</li> </ul> <p>For detailed information about generator functions, see the Generator Function Reference.</p>"},{"location":"getting-started/producer-tutorial/#to-required","title":"<code>to</code> (required)","text":"<p>The target topic where messages will be produced. Can be specified as a simple topic name or with detailed configuration.</p> <p>Simple format: <pre><code>producers:\n  my_producer:\n    generator: my_generator\n    to: my_topic \n</code></pre></p> <p>Detailed format: <pre><code>producers:\n  my_producer:\n    generator: my_generator\n    to:\n      topic: sensor_data\n      keyType: string\n      valueType: json\n</code></pre></p>"},{"location":"getting-started/producer-tutorial/#optional-properties","title":"Optional Properties","text":""},{"location":"getting-started/producer-tutorial/#interval","title":"<code>interval</code>","text":"<p>The time to wait between generator calls. Supports various duration formats based on KSML's duration specification:</p> <p>Duration Format: <code>&lt;number&gt;&lt;unit&gt;</code> where unit is optional</p> <ul> <li><code>100</code> - 100 milliseconds (default unit)</li> <li><code>500ms</code> - 500 milliseconds</li> <li><code>3s</code> - 3 seconds</li> <li><code>5m</code> - 5 minutes</li> <li><code>2h</code> - 2 hours</li> <li><code>1d</code> - 1 day</li> <li><code>4w</code> - 4 weeks</li> </ul> <p>Default: If not specified, and no <code>count</code> or <code>until</code> is provided, the producer enters \"once mode\" and generates exactly one message then stops.</p> <p>Important: Specifying <code>interval</code> (even as <code>0</code> or <code>1ms</code>) is different from omitting it entirely. See the behavior table below.</p> Interval Examples (click to expand) <pre><code>functions:\n  my_generator:\n    type: generator\n    code: |\n      key = \"key1\"\n      value = {\"message\": \"Hello\"}\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  # Produce every 3 seconds (indefinitely)\n  slow_producer:\n    generator: my_generator\n    interval: 3s\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n\n  # Produce as fast as possible for 100 messages\n  rapid_producer:\n    generator: my_generator\n    interval: 1ms   # Very fast (can be 0 for no delay)\n    count: 100      # Must specify count or until to avoid infinite loop\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre>"},{"location":"getting-started/producer-tutorial/#count","title":"<code>count</code>","text":"<p>The total number of messages to produce before stopping.</p> <p>Default: If not specified (and <code>until</code> is also not specified), produces indefinitely.</p> Count Example (click to expand) <pre><code>functions:\n  my_generator:\n    type: generator\n    code: |\n      key = \"key1\"\n      value = {\"message\": \"Hello\"}\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  # Produce exactly 10 messages\n  limited_producer:\n    generator: my_generator\n    interval: 1s\n    count: 10\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>Note: The producer will stop when either <code>count</code> is reached OR the <code>until</code> condition becomes true, whichever comes first.</p>"},{"location":"getting-started/producer-tutorial/#batchsize","title":"<code>batchSize</code>","text":"<p>The number of messages to generate in each call to the generator. This is useful for performance optimization.</p> <ul> <li>Default: <code>1</code> (one message per generator call)</li> <li>Range: Must be between 1 and 1000</li> <li>Validation: Values outside this range will trigger a warning and default to 1</li> </ul> Batch Size Example (click to expand) <pre><code>functions:\n  batch_generator:\n    type: generator\n    globalCode: |\n      counter = 0\n    code: |\n      global counter\n      # Generate multiple messages in one call\n      messages = []\n      for i in range(10):  # Generate 10 messages\n        key = f\"sensor{counter}\"\n        value = {\"id\": counter, \"reading\": counter * 10}\n        messages.append((key, value))\n        counter += 1\n      return messages  # Return list of tuples\n    resultType: list(tuple(string, json))\n\nproducers:\n  batch_producer:\n    generator: batch_generator\n    interval: 5s\n    batchSize: 10  # Matches the generator's batch size\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>Performance Note: Using batching can significantly improve throughput when producing large volumes of data.</p>"},{"location":"getting-started/producer-tutorial/#condition","title":"<code>condition</code>","text":"<p>A predicate function that validates whether a generated message should be produced. If the condition returns <code>false</code>, the message is discarded and the generator is called again.</p> Condition Example (click to expand) <pre><code>functions:\n  generate_random_value:\n    type: generator\n    globalCode: |\n      import random\n    code: |\n      value = random.randint(0, 100)\n      return (\"sensor1\", {\"value\": value})\n    resultType: (string, json)\n\n  only_high_values:\n    type: predicate\n    expression: value.get(\"value\", 0) &gt; 50\n\nproducers:\n  filtered_producer:\n    generator: generate_random_value\n    condition: only_high_values  # Only produce if value &gt; 50\n    interval: 1s\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>Use Cases:</p> <ul> <li>Filtering out invalid or unwanted messages</li> <li>Implementing probabilistic message generation</li> </ul>"},{"location":"getting-started/producer-tutorial/#until","title":"<code>until</code>","text":"<p>A predicate function that determines when to stop producing. When this function returns <code>true</code>, the producer stops immediately.</p> Until Example (click to expand) <pre><code>functions:\n  generate_sequence:\n    type: generator\n    globalCode: |\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n      return (f\"key{counter}\", {\"count\": counter})\n    resultType: (string, json)\n\n  stop_at_10:\n    type: predicate\n    expression: value.get(\"count\", 0) &gt;= 10\n\nproducers:\n  sequence_producer:\n    generator: generate_sequence\n    until: stop_at_10  # Stop when count reaches 10\n    interval: 1s\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>Alternative with global state:</p> Until with Global State Example (click to expand) <pre><code>functions:\n  generate_with_stop:\n    type: generic\n    resultType: boolean\n    globalCode: |\n      counter = 0\n      done = False\n\n      def next_message():\n        global counter, done\n        counter += 1\n        if counter &gt;= 10:\n          done = True\n        return (f\"key{counter}\", {\"count\": counter})\n\nproducers:\n  smart_producer:\n    generator:\n      code: |\n        return next_message()\n      resultType: (string, json)\n    until:\n      expression: done  # Access global variable\n    interval: 1s\n    to:\n      topic: my_topic\n      keyType: string\n      valueType: json\n</code></pre> <p>Use Cases:</p> <ul> <li>Producing a predetermined dataset exactly once</li> <li>Implementing time-based or condition-based stopping logic</li> </ul>"},{"location":"getting-started/producer-tutorial/#producer-behavior-summary","title":"Producer Behavior Summary","text":"<p>Understanding when a producer starts and stops is crucial. Here's the complete behavior table:</p> interval count until Behavior Not specified Not specified Not specified Produces 1 message then stops (\"once mode\") Specified (any value) Not specified Not specified Produces indefinitely at the specified interval Specified Specified Not specified Produces <code>count</code> messages at the specified interval, then stops Specified Not specified Specified Produces indefinitely until <code>until</code> returns true, checking at each interval Specified Specified Specified Produces until <code>count</code> is reached OR <code>until</code> returns true (whichever comes first) <p>Key Points:</p> <ul> <li>\"Not specified\" means the property is completely omitted from the YAML</li> <li>\"Specified\" means the property is included, even if set to a minimal value like <code>interval: 1ms</code></li> <li>Using <code>interval</code> without <code>count</code> or <code>until</code> will produce messages indefinitely.</li> <li>When both <code>count</code> and <code>until</code> are specified, the producer stops when either condition is met first</li> </ul>"},{"location":"getting-started/producer-tutorial/#working-with-different-data-formats","title":"Working with Different Data Formats","text":"<p>To learn about producing messages in different formats like JSON, Avro, Protobuf, XML, CSV, and Binary, see the Different Data Formats tutorial.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start with KSML","text":"<p>Get KSML running in 5 minutes! This guide shows you how to create your first real-time analytics application using only YAML - no Java required. You'll build an IoT analytics system that processes sensor data in real-time.</p>"},{"location":"getting-started/quick-start/#what-well-build","title":"What We'll Build","text":"<p>A real-time device health monitoring system that:</p> <ul> <li>Filters offline devices from processing</li> <li>Analyzes device health from battery, signal, and error metrics</li> <li>Categorizes devices as \"healthy\" or \"needs attention\"  </li> <li>Alerts on specific issues like low battery or poor signal</li> </ul> <p>All with just YAML configuration and clear Python logic.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>You'll need:</p> <ul> <li>Docker Compose installed (installation guide)</li> <li>5 minutes of your time</li> </ul>"},{"location":"getting-started/quick-start/#choose-your-setup-method","title":"Choose Your Setup Method","text":"<p>Option A: Quick Start (Recommended) Download the pre-configured setup and run immediately:</p> <ol> <li>Download and extract: local-docker-compose-setup-quick-start.zip</li> <li>Navigate to the extracted folder</li> <li>Run <code>docker compose up -d</code></li> <li>Skip to Step 6: See It In Action!</li> </ol> <p>Option B: Step-by-Step Setup Follow the detailed instructions below to create everything from scratch.</p>"},{"location":"getting-started/quick-start/#step-1-set-up-your-environment","title":"Step 1: Set Up Your Environment","text":"<p>Note: Skip this step if you chose Option A above.</p> <ul> <li>Create a new directory for your KSML project:</li> </ul> <pre><code>mkdir my-ksml-project\ncd my-ksml-project\nmkdir examples\n</code></pre> <ul> <li>Create a <code>docker-compose.yml</code> file:</li> </ul> Docker Compose Configuration (click to expand) <pre><code>networks:\n  ksml:\n    driver: bridge\n\nservices:\n  broker:\n    image: apache/kafka:3.8.0\n    hostname: broker\n    container_name: broker\n    ports:\n      - \"9092:9092\"\n    networks:\n      - ksml\n    restart: always\n    environment:\n      KAFKA_PROCESS_ROLES: 'controller,broker'\n      KAFKA_BROKER_ID: 0\n      KAFKA_NODE_ID: 0\n      KAFKA_CONTROLLER_QUORUM_VOTERS: '0@broker:9090'\n      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n      KAFKA_ADVERTISED_LISTENERS: 'INNER://broker:9093,OUTER://localhost:9092'\n      KAFKA_LISTENERS: 'INNER://broker:9093,OUTER://broker:9092,CONTROLLER://broker:9090'\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INNER:PLAINTEXT,OUTER:PLAINTEXT,CONTROLLER:PLAINTEXT'\n      KAFKA_LOG_CLEANUP_POLICY: delete\n      KAFKA_LOG_RETENTION_MINUTES: 10070\n      KAFKA_INTER_BROKER_LISTENER_NAME: INNER\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'\n      KAFKA_MIN_INSYNC_REPLICAS: 1\n      KAFKA_NUM_PARTITIONS: 1\n      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'\n    # If the kafka topics can list data, the broker is healthy\n    healthcheck:\n      test: /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9093 --list\n      interval: 5s\n      timeout: 10s\n      retries: 10\n      start_period: 5s\n\n  ksml:\n    image: registry.axual.io/opensource/images/axual/ksml:1.1.0\n    networks:\n      - ksml\n    container_name: ksml\n    working_dir: /ksml\n    volumes:\n      - ./examples:/ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n      kafka-setup:\n        condition: service_completed_successfully\n\n  kafka-ui:\n    image: quay.io/cloudhut/kowl:master\n    platform: linux/amd64\n    container_name: kowl\n    restart: always\n    ports:\n      - 8080:8080\n    volumes:\n      - ./kowl-ui-config.yaml:/config/kowl-ui-config.yaml\n    environment:\n      CONFIG_FILEPATH:  \"/config/kowl-ui-config.yaml\"\n    depends_on:\n      broker:\n        condition: service_healthy\n    networks:\n      - ksml\n\n  # This \"container\" is a workaround to pre-create topics, because setting KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE to true results in Kafka Streams errors\n  kafka-setup:\n    image: apache/kafka:3.8.0\n    hostname: kafka-setup\n    networks:\n      - ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n    restart: on-failure\n    command: \"bash -c 'echo Creating tutorial topics... &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic tutorial_input &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic temperature_data &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic temperature_data_converted &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic iot_sensor_data &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic device_health_alerts'\"\n</code></pre> <ul> <li>Create <code>kowl-ui-config.yaml</code> for Kafka UI:</li> </ul> Kafka UI Configuration (click to expand) <pre><code>server:\n  listenPort: 8080\n  listenAddress: 0.0.0.0\n\nkafka:\n  brokers:\n    - broker:9093\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-docker-services","title":"Step 2: Start Docker Services","text":"<p>Note: Skip this step if you chose Option A above.</p> <pre><code>docker compose up -d\n</code></pre> <p>This starts:</p> <ul> <li>Kafka broker (port 9092)</li> <li>KSML runner for your stream processing  </li> <li>Kowl (Kafka UI) for monitoring (port 8080)</li> <li>Automatic topic creation</li> </ul>"},{"location":"getting-started/quick-start/#step-3-verify-everything-started","title":"Step 3: Verify Everything Started","text":"<pre><code>docker compose ps\n</code></pre> <p>You should see:</p> <ul> <li>Kafka broker and Kafka UI are running  </li> <li>Topic setup container has exited successfully (this creates the required topics)</li> <li>KSML runner will have exited (missing config - we'll fix this next)</li> </ul> <p>Check the Kafka UI at http://localhost:8080 to see your topics.</p>"},{"location":"getting-started/quick-start/#step-4-create-your-first-ksml-application","title":"Step 4: Create Your First KSML Application","text":"<p>Note: If you chose Option A (zip download), these files already exist - you can skip to Step 5.</p> <p>Now let's create a smart device health monitoring application!</p> <p>In the <code>examples/</code> directory, create <code>iot-analytics.yaml</code>:</p> KSML Device Health Monitoring processing definition (click to expand) <p>This is a definition for a demo KSML application showing device health monitoring. It is not required at this point to understand this YAML &amp; Python syntax, we will explain what it does and how it works later.</p> <pre><code>functions:\n  add_health_status:\n    type: valueTransformer\n    code: |\n      # Get device metrics\n      battery = value.get('battery_percent', 100)\n      signal = value.get('signal_strength', 100)\n      errors = value.get('error_count', 0)\n\n      # Check each condition separately for clarity\n      has_low_battery = battery &lt; 20\n      has_poor_signal = signal &lt; 50  \n      has_high_errors = errors &gt; 5\n\n      # Determine if device needs attention\n      if has_low_battery or has_poor_signal or has_high_errors:\n        health_status = \"needs_attention\"\n\n        # Determine the specific reason\n        if has_low_battery:\n          reason = \"low_battery\"\n        elif has_poor_signal:\n          reason = \"poor_signal\"\n        else:\n          reason = \"high_errors\"\n      else:\n        health_status = \"healthy\"\n        reason = \"ok\"\n\n      return {\n        \"device_id\": key,\n        \"battery_percent\": battery,\n        \"signal_strength\": signal, \n        \"error_count\": errors,\n        \"health_status\": health_status,\n        \"issue_reason\": reason\n      }\n    resultType: json\n\npipelines:\n  device_health_monitor:\n    from:\n      topic: iot_sensor_data\n      keyType: string\n      valueType: json\n      offsetResetPolicy: latest\n    via:\n      # Step 1: Only process online devices\n      - type: filter\n        if:\n          expression: value is not None and value.get('status') == 'online'\n\n      # Step 2: Add health status\n      - type: transformValue\n        mapper: add_health_status\n\n      # Step 3: Log device health\n      - type: peek\n        forEach:\n          code: |\n            device = key\n            status = value.get('health_status')\n            reason = value.get('issue_reason')\n            battery = value.get('battery_percent')\n\n            if status == \"healthy\":\n              print(f\"{device}: Healthy (Battery: {battery}%)\")\n            else:\n              print(f\"{device}: Needs attention - {reason}\")\n    to:\n      topic: device_health_alerts\n      keyType: string\n      valueType: json\n</code></pre> <p>Now create the KSML runner configuration file <code>ksml-runner.yaml</code>:</p> KSML Runner Configuration (click to expand) <pre><code>kafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.example.producer\n  security.protocol: PLAINTEXT\n  acks: all\n\nksml:\n  definitions:\n    iot: iot-analytics.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-start-your-application","title":"Step 5: Start Your Application","text":"<p>Restart the KSML runner:</p> <pre><code>docker compose restart ksml\n</code></pre> <p>Check the logs to see when KSML is ready to receive messages:</p> <pre><code>docker compose logs ksml -f\n</code></pre> <p>KSML is ready to receive messages when you see a message:</p> <pre><code>Pipeline processing state change. Moving from old state 'REBALANCING' to new state 'RUNNING'\n</code></pre>"},{"location":"getting-started/quick-start/#step-6-see-it-in-action","title":"Step 6: See It In Action!","text":"<p>Send some test data to see your pipeline work:</p> <pre><code>docker compose exec broker kafka-console-producer.sh --bootstrap-server broker:9093 --topic iot_sensor_data --property \"parse.key=true\" --property \"key.separator=:\"\n</code></pre> <p>Paste these device health metrics (press Enter after each):</p> <pre><code>sensor-001:{\"battery_percent\":15,\"signal_strength\":80,\"error_count\":1,\"status\":\"online\"}\nsensor-002:{\"battery_percent\":75,\"signal_strength\":30,\"error_count\":2,\"status\":\"online\"}\nsensor-003:{\"battery_percent\":90,\"signal_strength\":85,\"error_count\":0,\"status\":\"online\"}\nsensor-004:{\"battery_percent\":60,\"signal_strength\":70,\"error_count\":8,\"status\":\"online\"}\nsensor-005:{\"battery_percent\":50,\"signal_strength\":60,\"error_count\":1,\"status\":\"offline\"}\n</code></pre> <p>Press <code>Ctrl+C</code> to exit.</p>"},{"location":"getting-started/quick-start/#what-just-happened","title":"What Just Happened?","text":"<p>Your KSML pipeline performed real-time device health monitoring in just a few lines of YAML:</p> <ol> <li>Filtered offline devices (only processes devices with status \"online\")</li> <li>Analyzed device health from battery, signal strength, and error metrics</li> <li>Categorized devices as \"healthy\" or \"needs attention\" </li> <li>Identified specific issues (low battery, poor signal, high errors)</li> </ol> <p>Check the KSML logs to see the real-time analysis:</p> <pre><code>docker compose logs ksml -f\n</code></pre> <p>You'll see device health reports like: <pre><code>sensor-003: Healthy (Battery: 90%)\nsensor-001: Needs attention - low_battery\nsensor-002: Needs attention - poor_signal\nsensor-004: Needs attention - high_errors\n</code></pre></p> <p>This is real-time device monitoring! Each device status is instantly analyzed and categorized as data flows through the pipeline.</p>"},{"location":"getting-started/quick-start/#example-messages","title":"Example messages","text":"<p>INPUT (to <code>iot_sensor_data</code> topic):</p> <ul> <li>key: <code>sensor-001</code></li> <li>value: <pre><code>{\n  \"battery_percent\": 15,\n  \"signal_strength\": 80,\n  \"error_count\": 1,\n  \"status\": \"online\"\n}\n</code></pre></li> </ul> <p>OUTPUT (to <code>device_health_alerts</code> topic):</p> <ul> <li>key: <code>sensor-001</code></li> <li>value: <pre><code>{\n  \"device_id\": \"sensor-001\",\n  \"battery_percent\": 15,\n  \"signal_strength\": 80,\n  \"error_count\": 1,\n  \"health_status\": \"needs_attention\",\n  \"issue_reason\": \"low_battery\"\n}\n</code></pre></li> </ul> <p>Open http://localhost:8080 to explore your topics and see the transformed data!</p>"},{"location":"getting-started/quick-start/#congratulations","title":"Congratulations!","text":"<p>You just built a real-time device health monitoring system with:</p> <ul> <li>Smart device analysis with battery, signal, and error monitoring</li> <li>Real-time filtering and categorization of device health status</li> <li>No compilation or complex infrastructure needed</li> </ul>"},{"location":"getting-started/quick-start/#what-makes-ksml-powerful","title":"What Makes KSML Powerful?","text":"<p>Traditional stream processing requires:</p> <ul> <li>Java/Scala expertise for Kafka Streams</li> <li>Complex filtering and transformation code</li> <li>Build and deployment pipelines</li> </ul> <p>With KSML you get:</p> <ul> <li>Simple YAML configuration for data processing</li> <li>Built-in filtering and transformation operations</li> <li>Optional Python for custom business logic</li> <li>Instant deployment with containers</li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Ready to learn more?</p> <ol> <li>Schema Validation - Set up IDE validation for error-free development</li> <li>Understanding KSML - Learn the concepts</li> <li>KSML Basics Tutorial - Build more advanced pipelines</li> </ol>"},{"location":"getting-started/schema-validation/","title":"KSML Schema Validation","text":"<p>KSML provides two JSON Schema specification files that enable IDE validation, autocompletion, and error checking for your KSML files. These schemas help you write correct syntax and catch errors early in development.</p> <p>The KSML project is available at: https://github.com/Axual/ksml</p>"},{"location":"getting-started/schema-validation/#understanding-ksml-schemas","title":"Understanding KSML Schemas","text":"<p>KSML provides two separate JSON schemas for different types of configuration files:</p>"},{"location":"getting-started/schema-validation/#1-ksml-language-specification-schema","title":"1. KSML Language Specification Schema","text":"<p>File: <code>docs/ksml-language-spec.json</code> Validates: KSML definition files (e.g., <code>processor.yaml</code>, <code>producer.yaml</code>, <code>pipeline.yaml</code>)</p> <p>This schema validates KSML stream processing definitions by ensuring:</p> <ul> <li>Proper nesting and organization of streams, tables, functions, and pipelines</li> <li>Valid property names for operations like transformValue, filter, aggregate, and join</li> <li>Correct data types for each field (string, object, array, or boolean)</li> <li>Required properties are present in each component</li> <li>Pipeline operations and transformations follow proper formatting</li> </ul> Example KSML definition file <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  temperature_filtered:\n    type: predicate\n    expression: value.get('sensors', {}).get('temperature') &gt; 20 and value.get('sensors', {}).get('humidity') &lt; 80 and value.get('sensors', {}).get('location') == 'warehouse'\n  log_message:\n    type: forEach\n    code: |\n      log.info(\"Processed message: key={}, value={}\", key, value)\n\npipelines:\n  filtering_pipeline:\n    from: input_stream\n    via:\n      - type: filter\n        if: temperature_filtered\n      - type: peek\n        forEach:\n          code: |\n            log_message(key, value)\n    to: output_stream\n</code></pre>"},{"location":"getting-started/schema-validation/#2-ksml-runner-configuration-schema","title":"2. KSML Runner Configuration Schema","text":"<p>File: <code>docs/ksml-runner-spec.json</code> Validates: KSML Runner configuration files (e.g., <code>ksml-runner.yaml</code>)</p> <p>This schema validates the runner configuration that controls how KSML executes:</p> <ul> <li>Bootstrap servers, application IDs, and Kafka consumer/producer settings</li> <li>Definition file locations, storage directories, and feature toggles</li> <li>Error handling strategies for consume, produce, and process operations</li> <li>Prometheus metrics and REST API configuration for health checks and queries</li> <li>Data format serializers and deserializers (Avro, JSON, Protobuf, etc.)</li> <li>Connections to Confluent or Apicurio schema registry instances</li> <li>Security permissions and sandboxing for Python function execution</li> </ul> Example KSML Runner configuration file <pre><code>ksml:\n  definitions:\n    producer: producer-avro.yaml\n    processor: processor-avro-transform.yaml\n  schemaRegistries:\n    my_schema_registry:\n      config:\n        schema.registry.url: http://schema-registry:8081/apis/ccompat/v7\n  notations:\n    avro:\n      type: confluent_avro\n      schemaRegistry: my_schema_registry\n      config:\n        normalize.schemas: true\n        auto.register.schemas: true\n  storageDirectory: /ksml/state\n  createStorageDirectory: true\n\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.schema.registry.test\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre>"},{"location":"getting-started/schema-validation/#schema-benefits","title":"Schema Benefits","text":"<p>Using KSML schemas in your IDE provides:</p> <ol> <li>Immediate feedback on syntax errors and invalid configurations</li> <li>Smart suggestions for properties, operations, and valid values</li> <li>Inline help text explaining each field and operation as you type</li> <li>Verification that YAML structure is correct and required fields are present</li> <li>Early detection of configuration mistakes before deployment</li> <li>Type checking for strings, numbers, booleans, objects, and arrays</li> <li>Valid value suggestions for enumerated fields like error handlers (<code>stopOnFail</code>, <code>continueOnFail</code>, <code>retryOnFail</code>)</li> </ol>"},{"location":"getting-started/schema-validation/#setting-up-schema-validation","title":"Setting Up Schema Validation","text":"<p>There are two ways to enable schema validation:</p> <ol> <li>Inline schema declaration (recommended) - Add a comment at the top of each file</li> <li>IDE configuration - Configure schema mappings in your IDE settings</li> </ol>"},{"location":"getting-started/schema-validation/#method-1-inline-schema-declaration-recommended","title":"Method 1: Inline Schema Declaration (Recommended)","text":"<p>Add a schema declaration comment at the top of your YAML files. This method works across IDEs without configuration.</p>"},{"location":"getting-started/schema-validation/#for-ksml-definition-files","title":"For KSML Definition Files","text":"<p>Add this line at the top of your pipeline/processor/producer files:</p> <pre><code># yaml-language-server: $schema=https://axual.github.io/ksml/latest/ksml-language-spec.json\n\nstreams:\n  my-stream:\n    topic: \"my-topic\"\n    # ... rest of your definition\n</code></pre>"},{"location":"getting-started/schema-validation/#for-ksml-runner-configuration-files","title":"For KSML Runner Configuration Files","text":"<p>Add this line at the top of your <code>ksml-runner.yaml</code> file:</p> <pre><code># yaml-language-server: $schema=https://axual.github.io/ksml/latest/ksml-runner-spec.json\n\nkafka:\n  bootstrap.servers: localhost:9092\n  # ... rest of your configuration\n</code></pre>"},{"location":"getting-started/schema-validation/#how-inline-schemas-work","title":"How Inline Schemas Work","text":"<p>The special comment <code># yaml-language-server: $schema=URL</code> tells your IDE:</p> <ol> <li>VS Code (YAML extension): Automatically detects and applies the schema from the URL</li> <li>IntelliJ IDEA: Recognizes the declaration and fetches the schema</li> <li>Other editors: Most modern YAML-aware editors support this convention</li> </ol> <p>Benefits: - No IDE configuration needed - Schema association travels with the file - Works for all team members immediately - Version-controlled alongside your code - Public URL means always up-to-date schemas</p> <p>Note: The schemas are published to GitHub Pages at <code>https://axual.github.io/ksml/latest/</code>, so an internet connection is required for initial download. IDEs typically cache schemas locally after the first fetch.</p> <p>For local development without internet access, you can use relative paths: <pre><code># yaml-language-server: $schema=../../docs/ksml-language-spec.json\n</code></pre></p>"},{"location":"getting-started/schema-validation/#method-2-ide-configuration","title":"Method 2: IDE Configuration","text":"<p>If you prefer to configure schema mappings in your IDE settings, or if you're working offline with local schema files, follow these steps.</p>"},{"location":"getting-started/schema-validation/#intellij-idea-setup","title":"IntelliJ IDEA Setup","text":"<p>Step 1: Access JSON Schema Settings</p> <ol> <li>Open IntelliJ IDEA</li> <li>Go to File \u2192 Preferences (on macOS) or File \u2192 Settings (on Windows/Linux)</li> <li>Navigate to Languages &amp; Frameworks \u2192 Schemas and DTDs \u2192 JSON Schema Mappings</li> </ol> <p>Step 2: Add KSML Language Specification Schema</p> <p>Configure validation for KSML definition files (streams, pipelines, producers):</p> <ol> <li>Click the + (plus) button to add a new schema mapping</li> <li>Configure the mapping:</li> <li>Name: <code>KSML Language Specification</code></li> <li>Schema file or URL: Browse to <code>docs/ksml-language-spec.json</code> in your KSML project directory</li> <li> <p>Schema version: Select JSON Schema version Draft 2019-09</p> </li> <li> <p>Add file mappings by clicking + in the mappings section:</p> </li> <li>For specific files: <code>processor.yaml</code>, <code>producer.yaml</code>, <code>pipeline.yaml</code></li> <li>For directory patterns: <code>**/definitions/**/*.yaml</code>, <code>**/pipelines/**/*.yaml</code></li> <li>For file patterns: <code>*-pipeline.yaml</code>, <code>*-processor.yaml</code></li> </ol> <p>Step 3: Add KSML Runner Configuration Schema</p> <p>Configure validation for KSML Runner configuration files:</p> <ol> <li>Click the + (plus) button again to add another schema mapping</li> <li>Configure the mapping:</li> <li>Name: <code>KSML Runner Configuration</code></li> <li>Schema file or URL: Browse to <code>docs/ksml-runner-spec.json</code> in your KSML project directory</li> <li> <p>Schema version: Select JSON Schema version Draft 2019-09</p> </li> <li> <p>Add file mappings by clicking + in the mappings section:</p> </li> <li>For specific files: <code>ksml-runner.yaml</code>, <code>application.yaml</code></li> <li>For file patterns: <code>ksml-runner*.yaml</code>, <code>*-runner.yaml</code></li> </ol> <p>Important: Make sure the file patterns for each schema don't overlap. KSML definition files should map to the Language Specification schema, while runner configuration files should map to the Runner Configuration schema.</p>"},{"location":"getting-started/schema-validation/#visual-studio-code-setup","title":"Visual Studio Code Setup","text":"<p>Note: If you're using the inline schema declaration method (recommended), no configuration is needed in VS Code. The YAML extension automatically recognizes the <code># yaml-language-server: $schema=URL</code> comments.</p> <p>For workspace-wide configuration without inline declarations:</p>"},{"location":"getting-started/schema-validation/#step-1-install-yaml-extension","title":"Step 1: Install YAML Extension","text":"<ol> <li>Install the YAML extension by Red Hat from the Extensions Marketplace</li> </ol>"},{"location":"getting-started/schema-validation/#step-2-configure-schema-associations","title":"Step 2: Configure Schema Associations","text":"<p>You need to map different file patterns to the appropriate schema.</p>"},{"location":"getting-started/schema-validation/#option-a-user-settings","title":"Option A: User Settings","text":"<ol> <li>Open Settings (Ctrl/Cmd + ,)</li> <li>Search for \"yaml schemas\"</li> <li>Add the following to your <code>settings.json</code>:</li> </ol> <pre><code>{\n  \"yaml.schemas\": {\n    \"file:///path/to/ksml/docs/ksml-language-spec.json\": [\n      \"**/definitions/**/*.yaml\",\n      \"**/pipelines/**/*.yaml\",\n      \"*-pipeline.yaml\",\n      \"*-processor.yaml\",\n      \"*-producer.yaml\"\n    ],\n    \"file:///path/to/ksml/docs/ksml-runner-spec.json\": [\n      \"**/ksml-runner.yaml\",\n      \"**/*-runner.yaml\",\n      \"**/application.yaml\"\n    ]\n  }\n}\n</code></pre>"},{"location":"getting-started/schema-validation/#option-b-workspace-configuration","title":"Option B: Workspace Configuration","text":"<p>Create a <code>.vscode/settings.json</code> file in your project root for project-specific configuration:</p> <pre><code>{\n  \"yaml.schemas\": {\n    \"./docs/ksml-language-spec.json\": [\n      \"definitions/**/*.yaml\",\n      \"pipelines/**/*.yaml\",\n      \"examples/**/processor.yaml\",\n      \"examples/**/producer.yaml\"\n    ],\n    \"./docs/ksml-runner-spec.json\": [\n      \"**/ksml-runner.yaml\",\n      \"examples/**/ksml-runner.yaml\"\n    ]\n  }\n}\n</code></pre> <p>Note: Workspace configuration (Option B) is recommended as it's portable and version-controlled with your project.</p>"},{"location":"getting-started/schema-validation/#verifying-schema-validation","title":"Verifying Schema Validation","text":"<p>Once configured, you should see:</p>"},{"location":"getting-started/schema-validation/#valid-ksml-syntax","title":"Valid KSML Syntax","text":"<ul> <li>Green underlines or checkmarks indicating correct syntax</li> <li>Autocompletion suggestions appearing as you type</li> <li>Inline documentation when hovering over fields</li> <li>Type hints showing expected data types</li> </ul>"},{"location":"getting-started/schema-validation/#invalid-syntax-detection","title":"Invalid Syntax Detection","text":"<ul> <li>Red squiggly underlines highlighting errors</li> <li>Clear error messages explaining the problem</li> <li>Suggested corrections for common mistakes</li> <li>Validation of enumerated values with all valid options</li> </ul>"},{"location":"getting-started/schema-validation/#example-1-testing-ksml-definition-validation","title":"Example 1: Testing KSML Definition Validation","text":"<p>Create a test KSML definition file (<code>test-pipeline.yaml</code>) with an intentional error:</p> <pre><code>streams:\n  input:\n    topic: \"input-topic\"\n    keyType: string\n    valueType: json\n\npipelines:\n  test:\n    from: \"input\"\n    via:\n      - type: \"invalidOperation\"  # This should show an error\n    to:\n      topic: \"output-topic\"\n</code></pre> <p>The schema validator will:</p> <ul> <li>Highlight <code>invalidOperation</code> as an invalid operation type</li> <li>Suggest valid alternatives like <code>transformValue</code>, <code>filter</code>, and <code>aggregate</code></li> <li>Display documentation for each operation when hovering</li> </ul>"},{"location":"getting-started/schema-validation/#example-2-testing-ksml-runner-configuration-validation","title":"Example 2: Testing KSML Runner Configuration Validation","text":"<p>Create a test runner configuration file (<code>test-runner.yaml</code>) with intentional errors:</p> <pre><code>kafka:\n  bootstrap.servers: localhost:9092\n  # Missing required field: application.id\n\nksml:\n  applicationServer:\n    enabled: true\n    port: 99999  # Invalid port (exceeds maximum 65535)\n  errorHandling:\n    consume:\n      handler: \"invalidHandler\"  # Invalid enum value\n</code></pre> <p>The schema validator will:</p> <ul> <li>Highlight the missing required field <code>application.id</code></li> <li>Show that port <code>99999</code> exceeds the maximum allowed value of <code>65535</code></li> <li>Suggest valid handler values: <code>stopOnFail</code>, <code>continueOnFail</code>, or <code>retryOnFail</code></li> <li>Display descriptions for each configuration option when hovering</li> </ul>"},{"location":"getting-started/schema-validation/#generating-schema-files","title":"Generating Schema Files","text":"<p>KSML schemas are automatically generated during the build process, but you can also generate them manually when needed.</p>"},{"location":"getting-started/schema-validation/#generating-both-schemas","title":"Generating Both Schemas","text":"<p>To generate both the KSML Language Specification and Runner Configuration schemas:</p> <pre><code>mvn package -DskipTests -pl ksml-runner -am\n</code></pre> <p>This builds the module with its dependencies and generates both schema files:</p> <ul> <li><code>docs/ksml-language-spec.json</code> for KSML definitions</li> <li><code>docs/ksml-runner-spec.json</code> for runner configuration</li> </ul> <p>Note: The <code>-am</code> (also-make) flag is required to build all dependencies needed for schema generation.</p>"},{"location":"getting-started/schema-validation/#generating-individual-schemas","title":"Generating Individual Schemas","text":"<p>If you've already built the project and want to generate schemas individually, you can run the JAR directly:</p> <p>To generate only the KSML Language Specification schema:</p> <pre><code>java -jar ksml-runner/target/ksml-runner-*.jar --schema docs/ksml-language-spec.json\n</code></pre> <p>To generate only the KSML Runner Configuration schema:</p> <pre><code>java -jar ksml-runner/target/ksml-runner-*.jar --runner-schema docs/ksml-runner-spec.json\n</code></pre> <p>Note: These commands require the project to be built first with <code>mvn clean package</code>.</p>"},{"location":"getting-started/schema-validation/#build-integration","title":"Build Integration","text":"<p>Schemas are automatically regenerated when running:</p> <ul> <li><code>mvn clean package</code> for a full build (recommended)</li> <li><code>mvn package -DskipTests -pl ksml-runner -am</code> for quick schema generation without tests</li> <li>Any Maven build that includes the <code>process-classes</code> phase for <code>ksml-runner</code></li> </ul> <p>The schemas are always kept in sync with the codebase, ensuring your IDE validation matches the current KSML version.</p>"},{"location":"getting-started/schema-validation/#schema-file-locations","title":"Schema File Locations","text":"<p>Both KSML schemas are located in the <code>docs/</code> directory:</p>"},{"location":"getting-started/schema-validation/#ksml-language-specification","title":"KSML Language Specification","text":"<pre><code>docs/ksml-language-spec.json\n</code></pre> <p>Purpose: Validates KSML definition files (streams, pipelines, functions, producers) Schema Version: JSON Schema Draft 2019-09</p>"},{"location":"getting-started/schema-validation/#ksml-runner-configuration","title":"KSML Runner Configuration","text":"<pre><code>docs/ksml-runner-spec.json\n</code></pre> <p>Purpose: Validates KSML Runner configuration files (Kafka settings, error handling, observability) Schema Version: JSON Schema Draft 2019-09</p>"},{"location":"getting-started/schema-validation/#schema-characteristics","title":"Schema Characteristics","text":"<p>Both schema files share these characteristics:</p> <ul> <li>Updated and version-controlled with each KSML release</li> <li>Comprehensive coverage of all features and configuration options</li> <li>Built-in descriptions for every property and field</li> <li>Automatically generated from Java code annotations using Jackson and Jakarta Validation</li> </ul>"},{"location":"getting-started/schema-validation/#next-steps","title":"Next Steps","text":"<p>With schema validation configured:</p> <ol> <li>Follow the KSML Basics Tutorial to build your first validated application</li> <li>Explore the examples in the <code>docs/</code> directory with full IDE support</li> <li>Configure runner settings confidently using the Runner Configuration schema</li> </ol>"},{"location":"reference/","title":"Reference Documentation","text":"<p>Welcome to the KSML Reference Documentation! This section provides comprehensive technical details about all aspects of KSML. It serves as a complete reference for KSML syntax, operations, functions, data types, and configuration options.</p> <p>Understanding these fundamental components will give you a solid foundation for building effective stream processing applications with KSML, regardless of the specific use case or complexity level.</p>"},{"location":"reference/#reference-sections","title":"Reference Sections","text":""},{"location":"reference/#ksml-definition-reference","title":"KSML Definition Reference","text":"<p>Complete documentation for writing KSML definitions:</p> <ul> <li>YAML structure and formatting</li> <li>Stream types (KStream, KTable, GlobalKTable)</li> <li>Definition file organization</li> <li>Syntax rules and conventions</li> <li>Data types and schemas</li> <li>Best practices</li> </ul>"},{"location":"reference/#pipeline-reference","title":"Pipeline Reference","text":"<p>Comprehensive guide to pipeline structure and data flow in KSML:</p> <ul> <li>Pipeline structure and components</li> <li>Input and output configurations</li> <li>Connecting and branching pipelines</li> <li>Best practices for pipeline design</li> <li>Duration specifications and patterns</li> </ul>"},{"location":"reference/#data-types-and-notations-reference","title":"Data Types and Notations Reference","text":"<p>Comprehensive guide to data types and notation formats in KSML:</p> <ul> <li>Primitive data types (boolean, int, string, etc.)</li> <li>Complex data types (enum, list, struct, tuple, union, windowed)</li> <li>Notation formats (JSON, Avro, CSV, XML, Binary, SOAP, Protobuf)</li> <li>Schema management (local files vs schema registry)</li> <li>Type conversion and format conversion</li> <li>Best practices for data handling</li> <li>Common patterns and examples</li> </ul>"},{"location":"reference/#function-reference","title":"Function Reference","text":"<p>Discover how to use Python functions in your KSML applications:</p> <ul> <li>Types of functions in KSML</li> <li>Writing Python functions</li> <li>Function parameters and return types</li> <li>Reusing functions across pipelines</li> <li>Function execution context and limitations</li> <li>Function types (forEach, mapper, predicate, etc.)</li> <li>Python code integration</li> <li>Built-in functions</li> <li>Best practices for function implementation</li> </ul>"},{"location":"reference/#operation-reference","title":"Operation Reference","text":"<p>Learn about the various operations you can perform on your data:</p> <ul> <li>Stateless operations (map, filter, etc.)</li> <li>Stateful operations (aggregate, count, etc.)</li> <li>Windowing operations</li> <li>Joining streams and tables</li> <li>Sink operations</li> <li>Each operation includes:</li> <li>Syntax and parameters</li> <li>Return values</li> <li>Examples</li> <li>Common use cases</li> <li>Performance considerations</li> </ul>"},{"location":"reference/#state-store-reference","title":"State Store Reference","text":"<p>Understand how to work with stateful processing in KSML:</p> <ul> <li>State store types (KeyValue, Window, Session)</li> <li>Store configuration options</li> <li>Persistence and caching</li> <li>Using stores in functions</li> <li>Store queries and management</li> <li>Performance tuning</li> <li>Best practices for stateful operations</li> </ul>"},{"location":"reference/#configuration-reference","title":"Configuration Reference","text":"<p>Complete documentation of KSML configuration options:</p> <ul> <li>Runner configuration</li> <li>Kafka client configuration</li> <li>Schema Registry configuration</li> <li>Performance tuning options</li> <li>Security settings</li> <li>Logging configuration</li> <li>Environment variables</li> </ul>"},{"location":"reference/#how-to-use-this-reference","title":"How to Use This Reference","text":"<p>You can read through these reference topics in order for a comprehensive understanding, or jump to specific topics as needed:</p> <ol> <li>Start with KSML Definition Reference to understand the basic structure, stream types, and data model</li> <li>Study Pipeline Reference to learn how data flows through your application</li> <li>Explore Functions to see how to implement custom logic in Python</li> <li>Learn about Operations to understand all the ways you can process your data</li> <li>Review Data Types and Notations to learn about data handling and notation formats</li> <li>Understand State Stores for stateful processing and data persistence</li> <li>Review advanced tutorials for production-ready applications</li> <li>Finish with Configuration Reference for deployment settings</li> </ol>"},{"location":"reference/configuration-reference/","title":"Configuration Reference","text":"<p>This document provides a comprehensive reference for configuring the KSML Runner application through the <code>ksml-runner.yaml</code> configuration file.</p>"},{"location":"reference/configuration-reference/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The KSML Runner configuration consists of two main sections:</p> <pre><code>ksml:\n  # KSML-specific configuration\n  configDirectory: .\n  definitions:\n    my_app: app-definition.yaml\n\nkafka:\n  # Kafka client configuration (standard Kafka properties)\n  bootstrap.servers: \"localhost:9092\"\n  application.id: \"my-ksml-app\"\n</code></pre>"},{"location":"reference/configuration-reference/#minimal-working-configurations","title":"Minimal Working Configurations","text":""},{"location":"reference/configuration-reference/#without-schema-registry","title":"Without Schema Registry","text":"<p>For applications that don't use schema-based formats (Avro, Protobuf, JSON Schema):</p> Minimal Configuration - No Schema Registry (click to expand) <pre><code>ksml:\n  # Section where you specify which KSML definitions to load, parse and execute.\n  definitions:\n    # Format is &lt;namespace&gt;: &lt;ksml_definition_filename&gt;\n    producer: producer.yaml\n    processor: processor.yaml\n  storageDirectory: /ksml/state  # Directory for state stores (inside /ksml which is writable because /ksml is the mounted working directory)\n  createStorageDirectory: true    # Create the directory if it doesn't exist\n\n# This setup connects to the Kafka broker and schema registry started with the example docker-compose file\n# These examples are intended to run from a inside a container on the same network\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.example.producer\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre>"},{"location":"reference/configuration-reference/#with-schema-registry","title":"With Schema Registry","text":"<p>For applications using schema-based formats (Avro, Protobuf, JSON Schema):</p> Minimal Configuration - With Schema Registry (click to expand) <pre><code>ksml:\n  definitions:\n    producer: producer.yaml\n    processor: processor.yaml\n  schemaRegistries:\n    my_schema_registry:\n      config:\n        schema.registry.url: http://schema-registry:8081/apis/ccompat/v7\n  notations:\n    avro:  \n      type: confluent_avro         # For Avro there are two implementations: apicurio_avro and confluent_avro\n      schemaRegistry: my_schema_registry\n      ## Below this line, specify properties to be passed into Confluent's KafkaAvroSerializer and KafkaAvroDeserializer\n      config:\n        normalize.schemas: true\n        auto.register.schemas: true\n\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.example.producer\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre>"},{"location":"reference/configuration-reference/#ksml-configuration-section","title":"KSML Configuration Section","text":"<p>The <code>ksml</code> section contains all configuration specific to the KSML Runner application:</p>"},{"location":"reference/configuration-reference/#directory-configuration","title":"Directory Configuration","text":"Property Type Default Description <code>configDirectory</code> String Working directory Directory containing KSML definition files <code>schemaDirectory</code> String Same as configDirectory Directory containing schema files <code>storageDirectory</code> String System temp directory Directory for Kafka Streams state stores <code>createStorageDirectory</code> Boolean false Create storage directory if it doesn't exist <pre><code>ksml:\n  configDirectory: .\n  schemaDirectory: ./schemas\n  storageDirectory: /tmp/kafka-streams\n  createStorageDirectory: true\n</code></pre>"},{"location":"reference/configuration-reference/#application-server-configuration","title":"Application Server Configuration","text":"<p>Enables a REST API for state store queries and health checks:</p> Property Type Default Description <code>enabled</code> Boolean false Enable/disable the REST server <code>host</code> String 0.0.0.0 IP address to bind the server to <code>port</code> Integer 8080 Port number for the REST API <pre><code>ksml:\n  applicationServer:\n    enabled: true\n    host: 0.0.0.0\n    port: 8080\n</code></pre>"},{"location":"reference/configuration-reference/#prometheus-metrics-configuration","title":"Prometheus Metrics Configuration","text":"<p>Enables Prometheus metrics endpoint:</p> Property Type Default Description <code>enabled</code> Boolean false Enable/disable Prometheus metrics <code>host</code> String 0.0.0.0 IP address to bind the metrics server to <code>port</code> Integer 9999 Port number for metrics endpoint <pre><code>ksml:\n  prometheus:\n    enabled: true\n    host: 0.0.0.0\n    port: 9999\n</code></pre>"},{"location":"reference/configuration-reference/#error-handling-configuration","title":"Error Handling Configuration","text":"<p>Configure how different types of errors are handled:</p> <p>Each error type (<code>consume</code>, <code>process</code>, <code>produce</code>) supports these properties:</p> Property Type Default Description <code>log</code> Boolean true Whether to log errors <code>logPayload</code> Boolean false Whether to include message payload in error logs <code>loggerName</code> String Auto-generated Custom logger name for this error type <code>handler</code> String stop Error handling strategy (<code>continueOnFail</code>, <code>stopOnFail</code>, <code>retryOnFail</code>) <pre><code>ksml:\n  errorHandling:\n    consume:\n      log: true\n      logPayload: false\n      loggerName: ConsumeError\n      handler: stopOnFail\n    process:\n      log: true\n      logPayload: false\n      loggerName: ProcessError\n      handler: continueOnFail\n    produce:\n      log: true\n      logPayload: false\n      loggerName: ProduceError\n      handler: continueOnFail\n</code></pre>"},{"location":"reference/configuration-reference/#feature-enablement","title":"Feature Enablement","text":"<p>Control which KSML features are enabled:</p> Property Type Default Description <code>enableProducers</code> Boolean true Enable producer definitions in KSML files <code>enablePipelines</code> Boolean true Enable pipeline definitions in KSML files <pre><code>ksml:\n  enableProducers: false  # Disable producers for pipeline-only applications\n  enablePipelines: true\n</code></pre>"},{"location":"reference/configuration-reference/#python-context-configuration","title":"Python Context Configuration","text":"<p>Control Python execution security and permissions:</p> Property Type Default Description <code>allowHostFileAccess</code> Boolean false Allow Python code to access host file system <code>allowHostSocketAccess</code> Boolean false Allow Python code to open network sockets <code>allowNativeAccess</code> Boolean false Allow Graal native access / JNI <code>allowCreateProcess</code> Boolean false Allow Python code to execute external processes <code>allowCreateThread</code> Boolean false Allow Python code to create new Java threads <code>inheritEnvironmentVariables</code> Boolean false Inherit JVM process environment in Python context <pre><code>ksml:\n  pythonContext:\n    allowHostFileAccess: false\n    allowHostSocketAccess: false\n    allowNativeAccess: false\n    allowCreateProcess: false\n    allowCreateThread: false\n    inheritEnvironmentVariables: false\n</code></pre>"},{"location":"reference/configuration-reference/#schema-registry-configuration","title":"Schema Registry Configuration","text":"<p>Configure connections to schema registries:</p>"},{"location":"reference/configuration-reference/#basic-configuration","title":"Basic Configuration","text":"<pre><code>ksml:\n  schemaRegistries:\n    # Confluent Schema Registry\n    confluent:\n      config:\n        schema.registry.url: \"http://schema-registry:8081\"\n\n    # Apicurio Schema Registry\n    apicurio:\n      config:\n        apicurio.registry.url: \"http://apicurio:8080/apis/registry/v2\"\n</code></pre>"},{"location":"reference/configuration-reference/#ssl-enabled-schema-registry","title":"SSL-Enabled Schema Registry","text":"<pre><code>ksml:\n  schemaRegistries:\n    confluent_secure:\n      config:\n        schema.registry.url: \"https://schema-registry:8081\"\n        schema.registry.ssl.protocol: TLSv1.3\n        schema.registry.ssl.keystore.location: /path/to/keystore.jks\n        schema.registry.ssl.keystore.type: JKS\n        schema.registry.ssl.keystore.password: \"${KEYSTORE_PASSWORD}\"\n        schema.registry.ssl.truststore.location: /path/to/truststore.jks\n        schema.registry.ssl.truststore.type: JKS\n        schema.registry.ssl.truststore.password: \"${TRUSTSTORE_PASSWORD}\"\n\n    apicurio_secure:\n      config:\n        apicurio.registry.url: \"https://apicurio:8080/apis/registry/v2\"\n        apicurio.registry.request.ssl.keystore.location: /path/to/keystore.jks\n        apicurio.registry.request.ssl.keystore.type: JKS\n        apicurio.registry.request.ssl.keystore.password: \"${KEYSTORE_PASSWORD}\"\n        apicurio.registry.request.ssl.truststore.location: /path/to/truststore.jks\n        apicurio.registry.request.ssl.truststore.type: JKS\n        apicurio.registry.request.ssl.truststore.password: \"${TRUSTSTORE_PASSWORD}\"\n</code></pre>"},{"location":"reference/configuration-reference/#notation-configuration","title":"Notation Configuration","text":"<p>Configure data format serializers and deserializers. Each notation entry defines:</p> Property Type Required Description <code>type</code> String Yes Serializer implementation type <code>schemaRegistry</code> String No Schema registry to use (if applicable) <code>config</code> Object No Additional properties for the serializer <pre><code>ksml:\n  notations:\n    # Avro with Confluent\n    avro:\n      type: confluent_avro\n      schemaRegistry: confluent\n      config:\n        normalize.schemas: true\n        auto.register.schemas: false\n\n    # Avro with Apicurio\n    apicurio_avro:\n      type: apicurio_avro\n      schemaRegistry: apicurio\n      config:\n        apicurio.registry.auto-register: true\n\n    # JSON Schema with Apicurio\n    jsonschema:\n      type: apicurio_jsonschema\n      schemaRegistry: apicurio\n      config:\n        apicurio.registry.auto-register: true\n\n    # Protobuf with Apicurio\n    protobuf:\n      type: apicurio_protobuf\n      schemaRegistry: apicurio\n      config:\n        apicurio.registry.auto-register: false\n</code></pre> <p>Available serializer types:</p> Serializer Type Notation Schema Registry Description <code>confluent_avro</code> avro Confluent Avro with Confluent SR <code>apicurio_avro</code> avro Apicurio Avro with Apicurio SR <code>confluent_jsonschema</code> jsonschema Confluent JSON Schema with Confluent SR <code>apicurio_jsonschema</code> jsonschema Apicurio JSON Schema with Apicurio SR <code>confluent_protobuf</code> protobuf Confluent Protobuf with Confluent SR <code>apicurio_protobuf</code> protobuf Apicurio Protobuf with Apicurio SR <p>Built-in serializers (no configuration needed):</p> <ul> <li><code>json</code>: Schemaless JSON</li> <li><code>csv</code>: Comma-separated values</li> <li><code>xml</code>: XML format</li> <li><code>soap</code>: SOAP messages</li> </ul>"},{"location":"reference/configuration-reference/#ksml-definition-loading","title":"KSML Definition Loading","text":"<p>Specify which KSML definition files to load and execute:</p> <pre><code>ksml:\n  definitions:\n    # Format: &lt;namespace&gt;: &lt;filename&gt;\n    my_producer: producer-definition.yaml\n    my_processor: processor-definition.yaml\n    order_pipeline: order-processing.yaml\n</code></pre>"},{"location":"reference/configuration-reference/#schema-file-loading","title":"Schema File Loading","text":"<p>Specify schema files to load (Avro, JSON Schema, XSD, CSV schemas):</p> <pre><code>ksml:\n  schemas:\n    # Format: &lt;name&gt;: &lt;filename&gt;\n    SensorData.avsc: SensorData.avsc\n    SensorData.json: SensorData.json\n    SensorData.xsd: SensorData.xsd\n    SensorData.csv: SensorData.csv\n</code></pre>"},{"location":"reference/configuration-reference/#kafka-configuration-section","title":"Kafka Configuration Section","text":"<p>The <code>kafka</code> section contains standard Kafka client configuration properties. All Kafka Streams and Kafka client properties are supported.</p>"},{"location":"reference/configuration-reference/#essential-properties","title":"Essential Properties","text":"Property Type Required Description <code>bootstrap.servers</code> String Yes Comma-separated list of Kafka brokers <code>application.id</code> String Yes Unique identifier for the Kafka Streams app <p>!!! note \"Application ID Aliases\"     You can use any of these property names for the application ID:     - <code>application.id</code> (standard Kafka property)     - <code>applicationId</code> (camelCase variant)     - <code>app.id</code> (KSML shorthand)</p>"},{"location":"reference/configuration-reference/#common-properties","title":"Common Properties","text":"Property Type Default Description <code>group.instance.id</code> String - Static member ID for faster rebalancing <code>security.protocol</code> String PLAINTEXT Security protocol (PLAINTEXT, SSL, SASL_*) <code>auto.offset.reset</code> String latest Offset reset policy (earliest, latest, none) <code>acks</code> String 1 Producer acknowledgment mode <pre><code>kafka:\n  # Essential configuration\n  bootstrap.servers: \"kafka1:9092,kafka2:9092,kafka3:9092\"\n  application.id: \"my-ksml-application\"\n  group.instance.id: \"instance-1\"\n\n  # Security configuration\n  security.protocol: PLAINTEXT\n  auto.offset.reset: earliest\n  acks: all\n\n  # Performance tuning\n  batch.size: 16384\n  linger.ms: 5\n  buffer.memory: 33554432\n</code></pre>"},{"location":"reference/configuration-reference/#ssl-configuration","title":"SSL Configuration","text":"<p>For secure connections to Kafka:</p> <pre><code>kafka:\n  security.protocol: SSL\n  ssl.protocol: TLSv1.3\n  ssl.enabled.protocols: TLSv1.3,TLSv1.2\n  ssl.endpoint.identification.algorithm: \"\"\n  ssl.keystore.type: JKS\n  ssl.keystore.location: /path/to/ksml.keystore.jks\n  ssl.keystore.password: \"${KEYSTORE_PASSWORD}\"\n  ssl.key.password: \"${KEY_PASSWORD}\"\n  ssl.truststore.type: JKS\n  ssl.truststore.location: /path/to/ksml.truststore.jks\n  ssl.truststore.password: \"${TRUSTSTORE_PASSWORD}\"\n</code></pre>"},{"location":"reference/configuration-reference/#sasl-configuration","title":"SASL Configuration","text":"<p>For SASL authentication:</p> <pre><code>kafka:\n  security.protocol: SASL_SSL\n  sasl.mechanism: PLAIN\n  sasl.jaas.config: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${KAFKA_USERNAME}\\\" password=\\\"${KAFKA_PASSWORD}\\\";\"\n</code></pre>"},{"location":"reference/configuration-reference/#axual-platform-configuration","title":"Axual Platform Configuration","text":"<p>For Axual platform with naming patterns:</p> <pre><code>kafka:\n  # Axual naming patterns\n  axual.topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\"\n  axual.group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\"\n  axual.transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\"\n\n  # Pattern variables\n  tenant: \"mytenant\"\n  instance: \"myinstance\"\n  environment: \"dev\"\n</code></pre>"},{"location":"reference/configuration-reference/#environment-variable-substitution","title":"Environment Variable Substitution","text":"<p>KSML supports environment variable substitution in configuration values using <code>${VARIABLE_NAME}</code> syntax:</p> <pre><code>kafka:\n  bootstrap.servers: \"${KAFKA_BROKERS}\"\n  application.id: \"${APP_ID:-default-app-id}\"  # With default value\n\nksml:\n  schemaRegistries:\n    confluent:\n      config:\n        schema.registry.url: \"${SCHEMA_REGISTRY_URL}\"\n        schema.registry.ssl.keystore.password: \"${KEYSTORE_PASSWORD}\"\n</code></pre>"},{"location":"reference/configuration-reference/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code>ksml:\n  configDirectory: /app/definitions\n  schemaDirectory: /app/schemas\n  storageDirectory: /app/state-stores\n  createStorageDirectory: true\n\n  applicationServer:\n    enabled: true\n    host: 0.0.0.0\n    port: 8080\n\n  prometheus:\n    enabled: true\n    host: 0.0.0.0\n    port: 9999\n\n  errorHandling:\n    consume:\n      log: true\n      logPayload: false\n      loggerName: ConsumeError\n      handler: stopOnFail\n    process:\n      log: true\n      logPayload: true\n      loggerName: ProcessError\n      handler: continueOnFail\n    produce:\n      log: true\n      logPayload: true\n      loggerName: ProduceError\n      handler: retryOnFail\n\n  enableProducers: true\n  enablePipelines: true\n\n  pythonContext:\n    allowHostFileAccess: false\n    allowHostSocketAccess: false\n    allowNativeAccess: false\n    allowCreateProcess: false\n    allowCreateThread: false\n    inheritEnvironmentVariables: false\n\n  schemaRegistries:\n    confluent:\n      config:\n        schema.registry.url: \"${SCHEMA_REGISTRY_URL}\"\n\n  notations:\n    avro:\n      type: confluent_avro\n      schemaRegistry: confluent\n      config:\n        normalize.schemas: true\n        auto.register.schemas: false\n\n  definitions:\n    order_processor: order-processing.yaml\n    user_events: user-event-pipeline.yaml\n\n  schemas:\n    UserProfile.avsc: UserProfile.avsc\n    OrderSchema.json: OrderSchema.json\n\nkafka:\n  bootstrap.servers: \"${KAFKA_BROKERS}\"\n  application.id: \"${APP_ID}\"\n  group.instance.id: \"${INSTANCE_ID}\"\n  security.protocol: \"${SECURITY_PROTOCOL:-PLAINTEXT}\"\n  auto.offset.reset: earliest\n  acks: all\n</code></pre>"},{"location":"reference/configuration-reference/#best-practices","title":"Best Practices","text":""},{"location":"reference/configuration-reference/#configuration-management","title":"Configuration Management","text":"<ul> <li>Use environment variables for environment-specific values (URLs, passwords, IDs)</li> <li>Keep sensitive information out of configuration files</li> <li>Use different configuration files for different environments (dev, staging, prod)</li> <li>Version control your configuration files (excluding secrets)</li> </ul>"},{"location":"reference/configuration-reference/#security","title":"Security","text":"<ul> <li>Use environment variables for sensitive information like passwords and API keys</li> <li>Enable SSL/TLS for production environments  </li> <li>Use proper authentication mechanisms (SASL_PLAIN, SASL_SSL, SCRAM, etc.)</li> <li>Store certificates and keystores securely</li> <li>Regularly rotate passwords and certificates</li> </ul>"},{"location":"reference/configuration-reference/#performance","title":"Performance","text":"<ul> <li>Set appropriate batch sizes and linger times for producers</li> <li>Configure adequate buffer memory for high-throughput scenarios</li> <li>Use static member IDs (<code>group.instance.id</code>) for faster rebalancing</li> <li>Monitor storage directory disk usage for stateful applications</li> <li>Choose appropriate serialization formats (Avro for efficiency, JSON for debugging)</li> </ul>"},{"location":"reference/configuration-reference/#operational","title":"Operational","text":"<ul> <li>Set unique <code>application.id</code> for each KSML application</li> <li>Enable application server for health checks and state store queries</li> <li>Enable Prometheus metrics for monitoring and alerting</li> <li>Configure appropriate error handling strategies for your use case</li> <li>Use descriptive application IDs and instance IDs</li> <li>Regularly backup state store directories for stateful applications</li> <li>Plan for disaster recovery and state store restoration</li> </ul>"},{"location":"reference/configuration-reference/#development","title":"Development","text":"<ul> <li>Start with minimal configuration and add complexity as needed</li> <li>Use <code>logPayload: true</code> during development for debugging (disable in production)</li> <li>Test with different error scenarios to validate error handling configuration</li> <li>Use schema registries to enforce data contracts between services</li> <li>Use <code>continueOnFail</code> for non-critical processing errors during testing</li> </ul>"},{"location":"reference/data-and-formats-reference/","title":"Data Types and Notations Reference","text":"<p>KSML supports a wide range of data types and formats for both keys and values in your streams. This comprehensive reference covers all data types, notation formats, and type conversion capabilities available in KSML.</p>"},{"location":"reference/data-and-formats-reference/#data-types-in-ksml","title":"Data Types in KSML","text":""},{"location":"reference/data-and-formats-reference/#primitive-types","title":"Primitive Types","text":"<p>KSML supports the following primitive types:</p> Type Description Example Java Equivalent <code>boolean</code> True or false values <code>true</code>, <code>false</code> <code>Boolean</code> <code>byte</code> 8-bit integer <code>42</code> <code>Byte</code> <code>short</code> 16-bit integer <code>1000</code> <code>Short</code> <code>int</code> 32-bit integer <code>1000000</code> <code>Integer</code> <code>long</code> 64-bit integer <code>9223372036854775807</code> <code>Long</code> <code>float</code> Single-precision floating point <code>3.14</code> <code>Float</code> <code>double</code> Double-precision floating point <code>3.141592653589793</code> <code>Double</code> <code>string</code> Text string <code>\"Hello, World!\"</code> <code>String</code> <code>bytes</code> Array of bytes Binary data <code>byte[]</code> <code>null</code> Null value <code>null</code> <code>null</code>"},{"location":"reference/data-and-formats-reference/#complex-types","title":"Complex Types","text":"<p>KSML also supports several complex types that can contain multiple values:</p>"},{"location":"reference/data-and-formats-reference/#enum","title":"Enum","text":"<p>An enumeration defines a set of allowed values.</p> <p>Syntax: <pre><code>valueType: enum(&lt;value1&gt;, &lt;value2&gt;, ...)   # Quotes optional\n</code></pre></p> <p>Example: <pre><code>streams:\n  order_status_stream:\n    topic: order-statuses\n    keyType: string\n    valueType: enum(PENDING, PROCESSING, SHIPPED, DELIVERED, CANCELLED)  # Works without quotes\n    # valueType: \"enum(PENDING, PROCESSING, SHIPPED, DELIVERED, CANCELLED)\"  # Also works with quotes\n</code></pre></p> <p>In Python code, an enum value is always represented as a string: <pre><code>functions:\n  update_status:\n    type: valueTransformer\n    code: |\n      if value.get(\"shipped\"):\n        return \"SHIPPED\"\n      elif value.get(\"processing\"):\n        return \"PROCESSING\"\n    expression: \"PENDING\"\n    resultType: string\n</code></pre></p> Producer - Enum example (click to expand) <pre><code># Demonstrates enum data type usage in KSML\nstreams:\n  order_events:\n    topic: order_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_order_event:\n    type: generator\n    resultType: (string, json)\n    code: |\n      order = nextOrder()\n      return (order[\"order_id\"], order)\n    globalCode: |\n      # Define orders with enum status values\n      orders = [\n        {\"order_id\": \"ORD001\", \"customer\": \"Alice\", \"amount\": 150.00, \"status\": \"PENDING\"},\n        {\"order_id\": \"ORD002\", \"customer\": \"Bob\", \"amount\": 75.50, \"status\": \"PROCESSING\"},\n        {\"order_id\": \"ORD003\", \"customer\": \"Charlie\", \"amount\": 200.00, \"status\": \"SHIPPED\"},\n        {\"order_id\": \"ORD004\", \"customer\": \"Diana\", \"amount\": 50.00, \"status\": \"DELIVERED\"},\n        {\"order_id\": \"ORD005\", \"customer\": \"Eve\", \"amount\": 125.00, \"status\": \"CANCELLED\"},\n      ]\n\n      index = 0\n      done = False\n\n      def nextOrder():\n        global index, done\n        if index &gt;= len(orders):\n          done = True\n          index = 0\n        order = orders[index]\n        index += 1\n        return order\n\nproducers:\n  order_producer:\n    to: order_events\n    interval: 1000\n    until:\n      expression: done\n    generator: generate_order_event\n</code></pre> Processor - Enum example (click to expand) <pre><code># Demonstrates enum data type usage and validation in KSML\nstreams:\n  order_events:\n    topic: order_events\n    keyType: string\n    valueType: json\n\n  # Stream with enum type for order status\n  # Enum values are represented as strings with validation\n  order_status_stream:\n    topic: order_statuses\n    keyType: string\n    valueType: enum(PENDING, PROCESSING, SHIPPED, DELIVERED, CANCELLED)\n\nfunctions:\n  extract_and_validate_status:\n    type: valueTransformer\n    resultType: string\n    code: |\n      # Extract status from order event\n      # In Python, enum values are always represented as strings\n      status = value.get(\"status\", \"PENDING\")\n\n      # Validate against allowed enum values\n      valid_statuses = [\"PENDING\", \"PROCESSING\", \"SHIPPED\", \"DELIVERED\", \"CANCELLED\"]\n      if status in valid_statuses:\n        return status\n      else:\n        # Return default if invalid\n        return \"PENDING\"\n\npipelines:\n  # Process orders and extract enum status\n  process_order_status:\n    from: order_events\n    via:\n      # Transform JSON value to validated enum string\n      - type: transformValue\n        mapper: extract_and_validate_status\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Order {} has status: {}\", key, value)\n    to: order_status_stream\n</code></pre>"},{"location":"reference/data-and-formats-reference/#list","title":"List","text":"<p>A list contains multiple elements of the same type.</p> <p>Syntax: <pre><code># Function notation (recommended - avoids YAML validation warnings)\nvalueType: list(&lt;element_type&gt;) # valueType: [&lt;element_type&gt;] also works\n</code></pre></p> <p>Example: <pre><code>streams:\n  tags_stream:\n    topic: tags\n    keyType: string\n    valueType: list(string)     # Function notation, valueType: [string] is also valid\n\n  categories_stream:\n    topic: categories\n    keyType: string\n    valueType: list(string)      # Alternative notation (no quotes needed)\n</code></pre></p> <p>In Python code, a list is represented as a Python list: <pre><code>functions:\n  extract_tags:\n    type: keyValueToValueListTransformer\n    expression: value.get(\"tags\", [])\n    resultType: list(string)    # Function notation, resultType: [string] is also valid\n\n  extract_categories:\n    type: keyValueToValueListTransformer\n    expression: value.get(\"categories\", [])\n    resultType: list(string)    # Alternative notation (no quotes needed)\n</code></pre></p> <p>See it in action:</p> <ul> <li>List example for predicate functions for data filtering</li> </ul>"},{"location":"reference/data-and-formats-reference/#example","title":"Example","text":"<pre><code>functions:\n  enhance_grades:\n    type: valueTransformer\n    resultType: list(int)    # Alternative to \"[int]\" - demonstrates list() syntax without quotes\n    code: |\n      grades = value.get(\"grades\", [])\n      # Add a bonus point to each grade\n      enhanced_grades = [grade + 5 for grade in grades if grade &lt; 95]\n    expression: enhanced_grades\n</code></pre> <p>This example demonstrates using <code>list(int)</code> syntax in function result types to avoid YAML validation warnings:</p> Producer - <code>list()</code> syntax example (click to expand) <pre><code># Simple producer demonstrating list() and tuple() syntax alternatives\n\nfunctions:\n  generate_grades:\n    type: generator\n    resultType: tuple(string, json)  # Alternative to \"(string, json)\" - no quotes needed\n    globalCode: |\n      import random\n    code: |\n      student_id = f\"student_{random.randint(1000, 9999)}\"\n\n      # Generate simple student grades data\n      student_data = {\n        \"grades\": [random.randint(70, 100) for _ in range(3)]\n      }\n    expression: (student_id, student_data)\n\nstreams:\n  student_grades:\n    topic: student_grades\n    keyType: string\n    valueType: json\n\nproducers:\n  grades_producer:\n    to: student_grades\n    interval: 3000\n    generator: generate_grades\n</code></pre> Processor - <code>list()</code> syntax example (click to expand) <pre><code># Simple processor demonstrating list() syntax alternative\n\nstreams:\n  student_grades:\n    topic: student_grades\n    keyType: string\n    valueType: json\n\n  enhanced_grades:\n    topic: enhanced_grades\n    keyType: string\n    valueType: json\n\nfunctions:\n  enhance_grades:\n    type: valueTransformer\n    resultType: list(int)    # Alternative to \"[int]\" - demonstrates list() syntax without quotes\n    code: |\n      grades = value.get(\"grades\", [])\n      # Add a bonus point to each grade\n      enhanced_grades = [grade + 5 for grade in grades if grade &lt; 95]\n    expression: enhanced_grades\n\npipelines:\n  process_grades:\n    from: student_grades\n    via:\n      - type: transformValue\n        mapper: enhance_grades\n      - type: convertValue\n        into: json\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"ENHANCED GRADES - Student: {}, enhanced_grades: {}\", key, value)\n    to: enhanced_grades\n</code></pre> <p>What this example does:</p> <ul> <li>Producer uses <code>resultType: tuple(string, json)</code> instead of <code>\"(string, json)\"</code> to avoid quotes</li> <li>Processor uses <code>resultType: list(int)</code> instead of <code>\"[int]\"</code> to avoid YAML validation warnings</li> <li>Functionality remains identical - the new syntax is purely for YAML compatibility</li> </ul>"},{"location":"reference/data-and-formats-reference/#map","title":"Map","text":"<p>A map contains key-value pairs where keys are always strings and values are of a specified type.</p> <p>Syntax: <pre><code>valueType: map(&lt;value_type&gt;)   # Quotes optional\n</code></pre></p> <p>Example: <pre><code>streams:\n  user_preferences:\n    topic: user-preferences\n    keyType: string\n    valueType: map(string)  # Map with string keys and string values (quotes optional)\n\n  scores:\n    topic: scores\n    keyType: string\n    valueType: map(int)     # Map with string keys and integer values, \"map(int)\" also valid\n</code></pre></p> <p>In Python code, a map is represented as a Python dictionary: <pre><code>functions:\n  create_preferences:\n    type: valueTransformer\n    code: |\n      return {\n        \"theme\": value.get(\"selected_theme\", \"default\"),\n        \"language\": value.get(\"user_language\", \"en\"),\n        \"notifications\": value.get(\"notify_enabled\", \"true\")\n      }\n    expression: result\n    resultType: map(string) # \"map(string)\" also valid\n\n  calculate_scores:\n    type: valueTransformer\n    code: |\n      return {\n        \"math\": 85,\n        \"science\": 92,\n        \"english\": 78\n      }\n    expression: result\n    resultType: map(int) # \"map(int)\" also valid \n</code></pre></p> <p>Key characteristics:</p> <ul> <li>Keys are always strings (this is enforced by the type system)</li> <li>All values must be of the same type as specified in <code>map(&lt;value_type&gt;)</code></li> <li>Useful for representing configuration objects, dictionaries, and key-value stores</li> </ul>"},{"location":"reference/data-and-formats-reference/#example_1","title":"Example","text":"<pre><code>streams:\n  user_preferences:\n    topic: user_preferences\n    keyType: string\n    valueType: map(string)  # Map with string keys and string values\n\n  user_scores:\n    topic: user_scores\n    keyType: string\n    valueType: map(int)     # Map with string keys and integer values\n</code></pre> <p>This simple example demonstrates using <code>map(string)</code> and <code>map(int)</code> types in stream definitions and function result types:</p> Producer - <code>map</code> example (click to expand) <pre><code># Simple producer demonstrating map(string) and map(int) data types\n\nfunctions:\n  generate_preferences:\n    type: generator\n    resultType: (string, map(string))  # Returns a map with string values\n    globalCode: |\n      import random\n      import time\n    code: |\n      user_id = f\"user_{random.randint(1000, 9999)}\"\n\n      # Generate preferences map with string values\n      preferences = {\n        \"theme\": random.choice([\"dark\", \"light\"]),\n        \"language\": random.choice([\"en\", \"es\", \"fr\"]),\n        \"layout\": random.choice([\"grid\", \"list\"])\n      }\n    expression: (user_id, preferences)\n\n  generate_scores:\n    type: generator\n    resultType: (string, map(int))  # Returns a map with integer values\n    globalCode: |\n      import random\n    code: |\n      user_id = f\"user_{random.randint(1000, 9999)}\"\n\n      # Generate scores map with integer values\n      scores = {\n        \"math\": random.randint(60, 100),\n        \"science\": random.randint(60, 100),\n        \"english\": random.randint(60, 100)\n      }\n    expression: (user_id, scores)\n\nstreams:\n  user_preferences:\n    topic: user_preferences\n    keyType: string\n    valueType: map(string)  # Map with string keys and string values\n\n  user_scores:\n    topic: user_scores\n    keyType: string\n    valueType: map(int)     # Map with string keys and integer values\n\nproducers:\n  preferences_producer:\n    to: user_preferences\n    interval: 3000\n    generator: generate_preferences\n\n  scores_producer:\n    to: user_scores\n    interval: 4000\n    generator: generate_scores\n</code></pre> Processor - <code>map</code> example (click to expand) <pre><code># Simple processor demonstrating map(string) and map(int) usage\n\nstreams:\n  user_preferences:\n    topic: user_preferences\n    keyType: string\n    valueType: map(string)  # Map with string keys and string values\n\n  user_scores:\n    topic: user_scores\n    keyType: string\n    valueType: map(int)     # Map with string keys and integer values\n\n  processed_preferences:\n    topic: processed_preferences\n    keyType: string\n    valueType: map(string)  # Output also as map(string)\n\n  processed_scores:\n    topic: processed_scores\n    keyType: string\n    valueType: map(int)     # Output also as map(int)\n\nfunctions:\n  enhance_preferences:\n    type: valueTransformer\n    resultType: map(string)  # Function returns map(string)\n    code: |\n      # Add a status field to the preferences map\n      enhanced = dict(value)  # Copy input map\n      enhanced[\"status\"] = \"active\"  # Add string value\n    expression: enhanced\n\n  calculate_stats:\n    type: valueTransformer\n    resultType: map(int)     # Function returns map(int)  \n    code: |\n      # Calculate some statistics from scores map\n      scores = dict(value)  # Copy input map\n      total = sum(scores.values())\n      average = total // len(scores)  # Integer division\n\n      stats = dict(scores)  # Start with original scores\n      stats[\"total\"] = total      # Add integer values\n      stats[\"average\"] = average\n    expression: stats\n\npipelines:\n  process_preferences:\n    from: user_preferences\n    via:\n      - type: transformValue\n        mapper: enhance_preferences\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"PREFERENCES MAP - User: {}, prefs: {}\", key, value)\n    to: processed_preferences\n\n  process_scores:\n    from: user_scores  \n    via:\n      - type: transformValue\n        mapper: calculate_stats\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"SCORES MAP - User: {}, total: {}, average: {}\", \n                   key, value.get(\"total\"), value.get(\"average\"))\n    to: processed_scores\n</code></pre> <p>What this example does:</p> <ul> <li>Stream definitions use <code>valueType: map(string)</code> and <code>valueType: map(int)</code> to define strongly-typed maps</li> <li>Function result types use <code>resultType: (string, map(string))</code> to return maps with type safety</li> <li>Processing functions use <code>resultType: map(string)</code> and <code>resultType: map(int)</code> to transform and validate map contents</li> <li>Demonstrates how the <code>map(valuetype)</code> syntax ensures all values in a map conform to the specified type</li> </ul>"},{"location":"reference/data-and-formats-reference/#struct","title":"Struct","text":"<p>A struct is a key-value map where all keys are strings. This is the most common complex type and is used for JSON objects, Avro records, etc.</p> <p>Syntax: <pre><code>valueType: struct\n</code></pre></p> <p>Example: <pre><code>streams:\n  user_profiles:\n    topic: user-profiles\n    keyType: string\n    valueType: struct\n</code></pre></p> <p>In Python code, a struct is represented as a dictionary: <pre><code>functions:\n  create_user:\n    type: valueTransformer\n    expression: |\n      return {\n        \"id\": value.get(\"user_id\"),\n        \"name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"),\n        \"email\": value.get(\"email\"),\n        \"age\": value.get(\"age\")\n      }\n</code></pre></p> Producer - Struct example (click to expand) <pre><code># Demonstrates struct data type usage in KSML\nstreams:\n  user_profiles:\n    topic: user_profiles\n    keyType: string\n    valueType: struct  # Using struct value type\n\nfunctions:\n  generate_user_profile:\n    type: generator\n    globalCode: |\n      import time\n\n      user_id = 1\n\n      def get_user_profile():\n        global user_id\n        # Create a struct (dictionary) for user profile\n        profile = {\n          \"user_id\": f\"USER_{user_id:03d}\",\n          \"name\": f\"User Name {user_id}\",\n          \"age\": 20 + (user_id % 50),\n          \"email\": f\"user{user_id}@example.com\",\n          \"preferences\": {\n            \"newsletter\": user_id % 2 == 0,\n            \"notifications\": user_id % 3 != 0\n          },\n          \"created_at\": int(time.time() * 1000)\n        }\n        user_id = (user_id % 5) + 1  # Cycle through 5 users\n        return profile\n    code: |\n      profile = get_user_profile()\n      return (profile[\"user_id\"], profile)\n    resultType: (string, struct)  # Returning struct type\n\nproducers:\n  profile_producer:\n    to: user_profiles\n    interval: 3000  # Generate profile every 3 seconds\n    generator: generate_user_profile\n</code></pre> Processor - Struct example (click to expand) <pre><code># Demonstrates struct data type manipulation in KSML\nstreams:\n  user_profiles:\n    topic: user_profiles\n    keyType: string\n    valueType: struct  # Input as struct type\n\n  enriched_profiles:\n    topic: enriched_profiles\n    keyType: string\n    valueType: struct  # Output as struct type\n\nfunctions:\n  enrich_user_profile:\n    type: valueTransformer\n    code: |\n      # Working with struct data (dictionary)\n      # Access nested struct fields\n      preferences = value.get(\"preferences\", {})\n\n      # Create enriched struct with additional fields\n      enriched = {\n        \"user_id\": value.get(\"user_id\"),\n        \"name\": value.get(\"name\"),\n        \"age\": value.get(\"age\"),\n        \"email\": value.get(\"email\"),\n        \"age_group\": \"young\" if value.get(\"age\", 0) &lt; 30 else \"adult\",\n        \"subscription_status\": \"active\" if preferences.get(\"newsletter\", False) else \"inactive\",\n        \"original_preferences\": preferences,\n        \"enriched_at\": int(time.time() * 1000)\n      }\n      return enriched\n    resultType: struct  # Function returns struct type\n    globalCode: |\n      import time\n\npipelines:\n  enrich_profiles:\n    from: user_profiles\n    via:\n      # Transform struct to enriched struct\n      - type: transformValue\n        mapper: enrich_user_profile\n\n      # Log the enriched struct\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Enriched profile: {} - {} ({} years, {})\", \n                     key, \n                     value.get(\"name\"),\n                     value.get(\"age\"),\n                     value.get(\"subscription_status\"))\n\n    to: enriched_profiles\n</code></pre>"},{"location":"reference/data-and-formats-reference/#tuple","title":"Tuple","text":"<p>A tuple combines multiple elements of different types into a single value.</p> <p>Syntax: <pre><code># Standard bracket notation\nvalueType: (&lt;type1&gt;, &lt;type2&gt;, ...)\n\n# Alternative function notation (avoids YAML validation warnings)\nvalueType: tuple(&lt;type1&gt;, &lt;type2&gt;, ...)\n</code></pre></p> <p>Example: <pre><code>streams:\n  sensor_stream:\n    topic: sensor-data\n    keyType: string\n    valueType: (string, avro:SensorData)     # Standard notation\n\n  coordinate_stream:\n    topic: coordinates\n    keyType: string\n    valueType: tuple(double, double)           # Alternative notation (no quotes needed)\n</code></pre></p> <p>In Python code, a tuple is represented as a Python tuple: <pre><code>functions:\n  create_user_age_pair:\n    type: keyValueTransformer\n    expression: (value.get(\"name\"), value.get(\"age\"))\n    resultType: (string, int)               # Standard notation\n\n  create_coordinate_pair:\n    type: keyValueTransformer\n    expression: (value.get(\"lat\"), value.get(\"lng\"))\n    resultType: tuple(double, double)         # Alternative notation (no quotes needed)\n</code></pre></p> <p>See it in action:</p> <ul> <li>Tuple example</li> </ul>"},{"location":"reference/data-and-formats-reference/#example_2","title":"Example","text":"<pre><code>functions:\n  generate_grades:\n    type: generator\n    resultType: tuple(string, json)  # Alternative to \"(string, json)\" - no quotes needed\n    globalCode: |\n      import random\n    code: |\n      student_id = f\"student_{random.randint(1000, 9999)}\"\n\n      # Generate simple student grades data\n      student_data = {\n        \"grades\": [random.randint(70, 100) for _ in range(3)]\n      }\n</code></pre> <p>This example demonstrates using <code>tuple(string, json)</code> syntax in function result types to avoid YAML validation warnings:</p> Producer - <code>tuple()</code> syntax example (click to expand) <pre><code># Simple producer demonstrating list() and tuple() syntax alternatives\n\nfunctions:\n  generate_grades:\n    type: generator\n    resultType: tuple(string, json)  # Alternative to \"(string, json)\" - no quotes needed\n    globalCode: |\n      import random\n    code: |\n      student_id = f\"student_{random.randint(1000, 9999)}\"\n\n      # Generate simple student grades data\n      student_data = {\n        \"grades\": [random.randint(70, 100) for _ in range(3)]\n      }\n    expression: (student_id, student_data)\n\nstreams:\n  student_grades:\n    topic: student_grades\n    keyType: string\n    valueType: json\n\nproducers:\n  grades_producer:\n    to: student_grades\n    interval: 3000\n    generator: generate_grades\n</code></pre> Processor - <code>tuple()</code> syntax example (click to expand) <pre><code># Simple processor demonstrating list() syntax alternative\n\nstreams:\n  student_grades:\n    topic: student_grades\n    keyType: string\n    valueType: json\n\n  enhanced_grades:\n    topic: enhanced_grades\n    keyType: string\n    valueType: json\n\nfunctions:\n  enhance_grades:\n    type: valueTransformer\n    resultType: list(int)    # Alternative to \"[int]\" - demonstrates list() syntax without quotes\n    code: |\n      grades = value.get(\"grades\", [])\n      # Add a bonus point to each grade\n      enhanced_grades = [grade + 5 for grade in grades if grade &lt; 95]\n    expression: enhanced_grades\n\npipelines:\n  process_grades:\n    from: student_grades\n    via:\n      - type: transformValue\n        mapper: enhance_grades\n      - type: convertValue\n        into: json\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"ENHANCED GRADES - Student: {}, enhanced_grades: {}\", key, value)\n    to: enhanced_grades\n</code></pre> <p>What this example does:</p> <ul> <li>Producer function uses <code>resultType: tuple(string, json)</code> instead of <code>\"(string, json)\"</code> to avoid quotes</li> <li>Processor function uses <code>resultType: list(int)</code> to demonstrate both new syntaxes working together</li> <li>No functional difference - the new syntax provides YAML-friendly alternatives</li> </ul>"},{"location":"reference/data-and-formats-reference/#union","title":"Union","text":"<p>A union type can be one of several possible types.</p> <p>Syntax: <pre><code>valueType: union(&lt;type1&gt;, &lt;type2&gt;, ...)\n</code></pre></p> <p>Example:</p> <p>Union types are used in two main places in KSML:</p> <p>1. In stream definitions - to specify that a stream can contain multiple types: <pre><code>streams:\n  optional_messages:\n    topic: optional-messages\n    keyType: string\n    valueType: union(null, json)  # This stream accepts either null OR a JSON object\n</code></pre></p> <p>2. In function return types - to specify that a function can return multiple types: <pre><code>functions:\n  generate_optional:\n    type: generator\n    code: |\n      # Can return either null or a message\n      if random.random() &gt; 0.5:\n        return (\"key1\", {\"data\": \"value\"})\n      else:\n        return (\"key1\", None)\n    resultType: (string, union(null, json))  # Returns a tuple with union type\n</code></pre></p> <p>What union types mean:</p> <ul> <li><code>union(null, json)</code> means the value can be either <code>null</code> OR a JSON object</li> <li>When processing union types, your code must check which type was received and handle each case</li> </ul> <p>Complete example showing both usages:</p> Producer - Union example (click to expand) <pre><code># Demonstrates union data type usage in KSML\nstreams:\n  optional_messages:\n    topic: optional_messages\n    keyType: string\n    valueType: json  # Using JSON for Kafka serialization\n\nfunctions:\n  generate_optional_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      message_id = 1\n\n      def get_optional_message():\n        global message_id\n        # Generate union type: either a message or null\n        if random.random() &gt; 0.3:  # 70% chance of message\n          message = {\n            \"id\": message_id,\n            \"content\": f\"Message {message_id}\",\n            \"timestamp\": int(time.time() * 1000)\n          }\n        else:  # 30% chance of null\n          message = None\n\n        key = f\"MSG_{message_id:04d}\"\n        message_id += 1\n        return (key, message)\n    code: |\n      return get_optional_message()\n    resultType: (string, union(null, json))  # Function returns union type\n\nproducers:\n  optional_producer:\n    to: optional_messages\n    interval: 2000  # Generate message every 2 seconds\n    generator: generate_optional_message\n</code></pre> Processor - Union example (click to expand) <pre><code># Demonstrates processing of union types in KSML\nstreams:\n  optional_messages:\n    topic: optional_messages\n    keyType: string\n    valueType: union(null, json)  # Input as union type\n\n  processed_messages:\n    topic: processed_messages\n    keyType: string\n    valueType: json  # Output as JSON\n\nfunctions:\n  # This function accepts a union type\n  process_optional:\n    type: valueTransformer\n    code: |\n      # Handle union type (null or JSON message)\n      if value is None:\n        # Handle null case\n        return {\n          \"status\": \"empty\",\n          \"message\": \"No content received\",\n          \"processed_at\": int(time.time() * 1000)\n        }\n      else:\n        # Handle JSON message case\n        return {\n          \"status\": \"processed\",\n          \"original_id\": value.get(\"id\"),\n          \"content_length\": len(value.get(\"content\", \"\")),\n          \"processed_at\": int(time.time() * 1000)\n        }\n    # Function signature shows it processes union type\n    resultType: json\n    globalCode: |\n      import time\n\npipelines:\n  process_optional_messages:\n    from: optional_messages\n    via:\n      # Transform the union type value\n      - type: transformValue\n        mapper: process_optional\n\n      # Log the processing result\n      - type: peek\n        forEach:\n          code: |\n            status = value.get(\"status\")\n            if status == \"empty\":\n              log.info(\"Received null value for key: {}\", key)\n            else:\n              log.info(\"Processed message {} for key: {}\", \n                       value.get(\"original_id\"), key)\n\n    to: processed_messages\n</code></pre>"},{"location":"reference/data-and-formats-reference/#windowed","title":"Windowed","text":"<p>Windowing operations in Kafka Streams group messages together in time-based windows. KSML provides the <code>windowed(&lt;base_type&gt;)</code> syntax to work with these windowed keys.</p> <p>Syntax: <pre><code># Without notation - requires manual transformation for Kafka output\nkeyType: windowed(&lt;base_type&gt;)\n\n# With notation - automatically serializes to the specified format\nkeyType: &lt;notation&gt;:windowed(&lt;base_type&gt;)  # e.g., json:windowed(string), avro:windowed(string)\n</code></pre></p> <p>Understanding Windowed Keys:</p> <p>After windowing operations (like <code>windowByTime</code>), Kafka Streams internally creates windowed keys that contain:</p> <ul> <li>The original key value</li> <li>Window start timestamp (milliseconds)</li> <li>Window end timestamp (milliseconds)  </li> <li>Human-readable start/end times</li> </ul> <p>Two Approaches for Handling Windowed Keys:</p> <p>1. Without Notation (Manual Transformation Required):</p> <p>When using plain <code>windowed(string)</code>, the windowed keys cannot be directly serialized to Kafka topics. You must manually transform them to a regular type:</p> <pre><code>      - type: convertKey\n        into: windowed(string)\n</code></pre> <p>2. With Notation Prefix (Automatic Serialization):</p> <p>Using a notation prefix like <code>json:windowed(string)</code> or <code>avro:windowed(string)</code> enables automatic serialization of the windowed key structure:</p> <pre><code>      - type: convertKey\n        into: json:windowed(string)  # Recommended: use notation prefix for automatic serialization\n</code></pre> <p>The notation automatically serializes the windowed key as a structured object with fields: <code>start</code>, <code>end</code>, <code>startTime</code>, <code>endTime</code>, and <code>key</code>.</p> <p>Complete Examples:</p> Producer - Generates events for windowing (click to expand) <pre><code># Demonstrates generating events for windowed processing\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json  # Using JSON for readability in Kowl UI\n\nfunctions:\n  generate_user_event:\n    type: generator\n    globalCode: |\n      import time\n      import random\n\n      # Simulate events from 5 different users\n      users = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n      event_types = [\"click\", \"view\", \"purchase\"]\n\n      def get_user_event():\n        user = random.choice(users)\n        event_type = random.choice(event_types)\n\n        event = {\n          \"user\": user,\n          \"type\": event_type,\n          \"timestamp\": int(time.time() * 1000)\n        }\n\n        return (user, event)  # Key by user for windowing\n    code: |\n      return get_user_event()\n    resultType: (string, json)\n\nproducers:\n  event_producer:\n    to: user_events\n    interval: 1000  # Generate event every second\n    generator: generate_user_event\n</code></pre> Processor - Manual transformation approach (click to expand) <p>This example shows how to manually transform windowed keys to regular strings when not using notation:</p> <pre><code># Demonstrates windowed type usage in KSML\n# \n# This example shows how to use the windowed(&lt;base_type&gt;) syntax with convertKey operation.\n# After windowing operations, KSML internally uses windowed keys. The convertKey operation\n# with 'into: windowed(string)' explicitly converts to this type for internal processing.\n#\n# IMPORTANT: While windowed types can be used internally and in stream definitions,\n# they cannot be serialized to Kafka topics. The final transformation to regular strings\n# is necessary for writing to Kafka.\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json  # Input events\n\n  # Output stream with regular string keys (transformed from windowed keys)\n  windowed_counts:\n    topic: windowed_counts  \n    keyType: string  # Regular string key after transformation\n    valueType: json  # JSON value containing count and window info\n\nstores:\n  event_counts_store:\n    type: window\n    retention: 1m  # Keep window data for 1 minute\n    keyType: string\n    valueType: long\n\npipelines:\n  count_events_by_window:\n    from: user_events\n    via:\n      # Group by key (user) for counting\n      - type: groupByKey\n\n      # Apply a 10-second tumbling window\n      - type: windowByTime\n        windowType: tumbling\n        duration: 10s\n\n      # Count events in each window\n      - type: count\n        store: event_counts_store\n\n      # Convert to stream for processing\n      - type: toStream\n\n      # DEMONSTRATES windowed(&lt;base_type&gt;) USAGE:\n      # Explicitly convert the key to windowed(string) type\n      # This shows how KSML handles windowed keys internally\n      - type: convertKey\n        into: windowed(string)\n\n      # Log the windowed counts with the windowed key type\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Windowed key type - User {} had {} events in window [{} - {}]\", \n                     key['key'], value, key['startTime'], key['endTime'])\n\n      # Transform windowed key to string for Kafka output\n      # This is necessary because Kafka topics cannot serialize windowed keys\n      - type: transformKeyValue\n        mapper:\n          resultType: (string, json)\n          code: |\n            # Extract window information from the windowed key\n            # Convert to a string key format: \"user_startTime_endTime\"\n            new_key = f\"{key['key']}_{key['start']}_{key['end']}\"\n\n            # Create a JSON value with all the information\n            new_value = {\n                \"user\": key['key'],\n                \"count\": value,\n                \"window_start\": key['start'],\n                \"window_end\": key['end'],\n                \"window_start_time\": key['startTime'],\n                \"window_end_time\": key['endTime']\n            }\n\n            log.info(\"Transformed to string key - User {} had {} events in window [{} - {}]\", \n                     key['key'], value, key['startTime'], key['endTime'])\n\n            return (new_key, new_value)\n\n    # Now we can write to the topic with regular string keys\n    to: windowed_counts\n</code></pre> Processor - Automatic serialization with notation (click to expand) <p>This example shows the simpler approach using notation for automatic serialization:</p> <pre><code># Demonstrates windowed type usage in KSML\n# \n# This example shows how to use the windowed(&lt;base_type&gt;) syntax with convertKey operation.\n# After windowing operations, KSML internally uses windowed keys.\n#\n# RECOMMENDED APPROACH:\n# Use a notation prefix like 'json:windowed(string)' or 'avro:windowed(string)' instead of\n# plain 'windowed(string)'. The notation automatically handles serialization of the windowed\n# key structure (start, end, startTime, endTime, key) to the specified format, allowing you\n# to write directly to Kafka topics without manual transformation.\n#\n# Example: \n#   - type: convertKey\n#     into: json:windowed(string)  # Automatically serializes as JSON structure\n#\n# Without notation, windowed keys cannot be serialized to Kafka topics and require manual\n# transformation to regular types.\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json  # Input events\n\n  # Output stream with JSON-serialized windowed keys\n  windowed_counts:\n    topic: windowed_counts  \n    keyType: json  # JSON notation automatically serializes the windowed key structure\n    valueType: long  # Simple count value\n\nstores:\n  event_counts_store:\n    type: window\n    retention: 1m  # Keep window data for 1 minute\n    keyType: string\n    valueType: long\n\npipelines:\n  count_events_by_window:\n    from: user_events\n    via:\n      # Group by key (user) for counting\n      - type: groupByKey\n\n      # Apply a 10-second tumbling window\n      - type: windowByTime\n        windowType: tumbling\n        duration: 10s\n\n      # Count events in each window\n      - type: count\n        store: event_counts_store\n\n      # Convert to stream for processing\n      - type: toStream\n\n      # DEMONSTRATES windowed(&lt;base_type&gt;) USAGE WITH NOTATION:\n      # Using json:windowed(string) automatically serializes windowed keys as JSON structures\n      # containing: start, end, startTime, endTime, and key fields\n      - type: convertKey\n        into: json:windowed(string)  # Recommended: use notation prefix for automatic serialization\n\n      # Log the windowed counts\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"JSON Windowed key - User {} had {} events in window [{} - {}]\", \n                     key['key'], value, key['startTime'], key['endTime'])\n\n    # Now we can write to the topic with regular string keys\n    to: windowed_counts\n</code></pre> <p>When to Use Each Approach:</p> <ul> <li> <p>Use notation prefix (<code>json:windowed(string)</code>) when you want to:</p> <ul> <li>Write windowed keys directly to Kafka topics</li> <li>Preserve the complete window structure in a standard format</li> <li>Avoid manual transformation code</li> </ul> </li> <li> <p>Use plain windowed type (<code>windowed(string)</code>) when you:</p> <ul> <li>Only need windowed keys for internal processing</li> <li>Want custom key formatting for output</li> <li>Need to extract specific window information</li> </ul> </li> </ul> <p>Key Takeaway:</p> <p>Windowed types enable time-based analytics like counting events per time window, calculating moving averages, or detecting patterns over time intervals. The notation prefix approach simplifies working with windowed data by handling serialization automatically.</p>"},{"location":"reference/data-and-formats-reference/#the-any-and-types","title":"The Any and \"?\" Types","text":"<p>KSML supports wildcard types <code>any</code> and <code>?</code> (which are equivalent) that represent unknown or variable data types. These map internally to <code>DataType.UNKNOWN</code> and can only be used for function parameters when the exact type is not known at definition time. They cannot be used for stream types or function result types due to serialization and type system requirements.</p> <p>Syntax:</p> <p>The <code>any</code> and <code>?</code> types can be used in:</p> <ul> <li>Function parameters only (<code>type: any</code> or <code>type: \"?\"</code>)</li> </ul> <pre><code># Function parameters (SUPPORTED)\nfunctions:\n  my_function:\n    type: generic\n    parameters:\n      - name: input\n        type: any      # Accepts any type\n      - name: other\n        type: \"?\"      # Alternative syntax (quote to avoid YAML issues)\n    code: |\n      # Process the input parameter of unknown type\n      return \"processed\"\n    resultType: string   # Must be a concrete type\n\n# Stream types (NOT SUPPORTED)\n# valueType: any     # \u274c This will fail with \"JSON serde not found\"\n# keyType: \"?\"       # \u274c This will fail with serialization error\n\n# Function result types (NOT SUPPORTED)\n# resultType: any    # \u274c This will fail with topology type checking error\n</code></pre> <p>Why the limitations exist:</p> <ul> <li>Stream types: Kafka requires concrete serialization formats. The <code>any</code> type cannot be serialized to Kafka topics because there's no serde for unknown data types.</li> <li>Result types: The topology type system requires concrete types for type checking and ensuring data flows correctly between operations.</li> </ul> <p>Key Use Cases:</p> <ul> <li>Generic utility functions that accept multiple data types as parameters</li> <li>Helper functions that need to handle variable input types</li> <li>Functions that process data generically before converting to concrete output types</li> </ul> Producer - Any type demonstration (click to expand) <p>This example demonstrates using the <code>?</code> type for function parameters, showing how to create generic utility functions.</p> <pre><code># Simple demonstration of 'any' and '?' types in KSML\n\nstreams:\n  any_data:\n    topic: any_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_data:\n    type: generator\n    code: |\n      import random\n\n      # Generate different JSON-compatible data structures\n      if random.choice([True, False]):\n        data = {\"type\": \"text\", \"value\": \"hello world\"}\n      else:\n        data = {\"type\": \"number\", \"value\": 42}\n\n      return (\"key1\", data)\n\n    resultType: (string, json)\n\n  # Function with '?' parameter type\n  describe_data:\n    type: generic\n    parameters:\n      - name: value\n        type: \"?\"  # Accepts any type\n    code: |\n      return \"processed\"\n\n    resultType: string\n\nproducers:\n  test_producer:\n    generator: generate_data\n    interval: 2s\n    to: any_data\n</code></pre> Processor - Any type processing (click to expand) <p>This example shows how to process data using the <code>any</code> type for function parameters, demonstrating type-agnostic helper functions.</p> <pre><code># Simple demonstration of 'any' type processing\n\nstreams:\n  any_input:\n    topic: any_data\n    keyType: string\n    valueType: json\n\n  any_output:\n    topic: processed_any_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  # Helper function with 'any' parameter type\n  process_data:\n    type: generic\n    parameters:\n      - name: data\n        type: any  # Accepts any type\n    code: |\n      return {\n        \"processed_data\": data,\n        \"status\": \"processed\"\n      }\n    resultType: json\n\n  # Function that uses the helper\n  process_any:\n    type: valueTransformer\n    code: |\n      # Use helper function that accepts 'any' type\n      return process_data(value)\n\n    resultType: json\n\npipelines:\n  process_data:\n    from: any_input\n    via:\n      - type: transformValue\n        mapper: process_any\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processed any type: {} = {}\",\n                     value.get(\"input_type\"), value.get(\"input_value\"))\n    to: any_output\n</code></pre>"},{"location":"reference/data-and-formats-reference/#notation-formats","title":"Notation Formats","text":"<p>KSML uses notations to allow reading/writing different message formats to Kafka topics. Notations are specified as a prefix to the schema name.</p>"},{"location":"reference/data-and-formats-reference/#examples","title":"Examples","text":"<p>See a working example for every data format in this tutorial:</p> <ul> <li>Data Format Examples</li> </ul>"},{"location":"reference/data-and-formats-reference/#format-selection-guide","title":"Format Selection Guide","text":"<p>The choice of notation depends on your specific requirements:</p> If you need... Consider using... Schema evolution and backward compatibility Avro or Protobuf Human-readable data for debugging JSON Integration with legacy systems XML or SOAP Simple tabular data CSV Compact binary format Avro or Protobuf Raw binary data handling Binary"},{"location":"reference/data-and-formats-reference/#avro","title":"Avro","text":"<p>Avro is a binary format that supports schema evolution.</p> <p>Syntax: <pre><code>valueType: avro:&lt;schema_name&gt;\n# or for schema registry lookup\nvalueType: avro\n</code></pre></p> <p>Example: <pre><code>streams:\n  sensor_readings:\n    topic: sensor-data\n    keyType: string\n    valueType: avro:SensorData\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#json","title":"JSON","text":"<p>JSON is a text-based, human-readable format for data transfer.</p> <p>Syntax: <pre><code># For schemaless JSON:\nvalueType: json\n# For JSON with a schema:\nvalueType: json:&lt;schema_name&gt;\n</code></pre></p> <p>Example: <pre><code>streams:\n  user_profiles:\n    topic: user-profiles\n    keyType: string\n    valueType: json\n\n  orders:\n    topic: orders\n    keyType: string\n    valueType: json:Order\n</code></pre></p> <p>Python functions can return JSON by returning a dictionary: <pre><code>functions:\n  merge_key_value_data:\n    type: valueTransformer\n    expression: { 'key': key, 'value': value }\n    resultType: json\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#json-schema","title":"JSON Schema","text":"<p>JSON Schema adds vendor-specific schema support to JSON serialization.</p> <p>Syntax: <pre><code># For schema registry lookup:\nvalueType: jsonschema\n# For JSON with a schema:\nvalueType: jsonschema:&lt;schema_name&gt;\n</code></pre></p> <p>Example: <pre><code>streams:\n  user_profiles:\n    topic: user-profiles\n    keyType: string\n    valueType: jsonschema:UserProfile\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#csv","title":"CSV","text":"<p>CSV (Comma-Separated Values) is a simple tabular data format.</p> <p>Syntax: <pre><code># For schemaless CSV:\nvalueType: csv\n# For CSV with a schema:\nvalueType: csv:&lt;schema_name&gt;\n</code></pre></p> <p>Example: <pre><code>streams:\n  sales_data:\n    topic: sales-data\n    keyType: string\n    valueType: csv\n\n  inventory_data:\n    topic: inventory-data\n    keyType: string\n    valueType: csv:InventoryRecord\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#xml","title":"XML","text":"<p>XML (Extensible Markup Language) is used for complex hierarchical data.</p> <p>Syntax: <pre><code># For schemaless XML:\nvalueType: xml\n# For XML with a schema:\nvalueType: xml:&lt;schema_name&gt;\n</code></pre></p> <p>Example: <pre><code>streams:\n  customer_data:\n    topic: customer-data\n    keyType: string\n    valueType: xml:CustomerData\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#protobuf","title":"Protobuf","text":"<p>Protobuf is a popular encoding format developed by Google.</p> <p>Syntax: <pre><code># For schema registry lookup:\nvalueType: protobuf\n# For Protobuf with a schema:\nvalueType: protobuf:&lt;schema_name&gt;\n</code></pre></p> <p>Example: <pre><code>streams:\n  user_profiles:\n    topic: user-profiles\n    keyType: string\n    valueType: protobuf:UserProfile\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#binary","title":"Binary","text":"<p>Binary data represents raw bytes for custom protocols.</p> <p>Syntax: <pre><code>valueType: binary\n</code></pre></p> <p>Example: <pre><code>streams:\n  binary_data:\n    topic: binary-messages\n    keyType: string\n    valueType: binary\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#soap","title":"SOAP","text":"<p>SOAP (Simple Object Access Protocol) is an XML-based messaging protocol.</p> <p>Syntax: <pre><code>valueType: soap\n</code></pre></p> <p>Example: <pre><code>streams:\n  service_requests:\n    topic: service-requests\n    keyType: string\n    valueType: soap\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#schema-management","title":"Schema Management","text":"<p>When working with structured data, it's important to manage your schemas effectively.</p>"},{"location":"reference/data-and-formats-reference/#examples_1","title":"Examples","text":"<p>See a working example for every type of schema in this tutorial:</p> <ul> <li>Schema Examples</li> </ul>"},{"location":"reference/data-and-formats-reference/#local-files-vs-schema-registry","title":"Local Files vs. Schema Registry","text":"<p>Local Schema Files: When a schema is specified, KSML loads the schema from a local file from the <code>schemaDirectory</code>. The notation determines the filename extension:</p> <ul> <li>Avro schemas: <code>.avsc</code> extension</li> <li>XML schemas: <code>.xsd</code> extension  </li> <li>CSV schemas: <code>.csv</code> extension</li> <li>JSON schemas: <code>.json</code> extension</li> </ul> <pre><code>streams:\n  sensor_data:\n    topic: sensor-reading\n    keyType: string\n    valueType: avro:SensorReading  # Looks for SensorReading.avsc\n</code></pre> <p>Schema Registry Lookup: When no schema is specified, KSML assumes the schema is loadable from Schema Registry:</p> <pre><code>streams:\n  sensor_data:\n    topic: sensor-reading\n    keyType: string\n    valueType: avro  # Schema fetched from registry\n</code></pre>"},{"location":"reference/data-and-formats-reference/#type-conversion","title":"Type Conversion","text":"<p>KSML handles type conversion differently depending on the context:</p> Context Conversion Type When to Use Functions Automatic When <code>resultType</code> differs from returned value Streams Explicit When input/output stream formats differ"},{"location":"reference/data-and-formats-reference/#function-type-conversion-automatic","title":"Function Type Conversion (Automatic)","text":"<p>Functions automatically convert return values to match their declared <code>resultType</code> when possible:</p> <p>Successful Conversions:</p> <ul> <li>Any type \u2192 string: Always works via automatic <code>.toString()</code> conversion</li> <li>String \u2192 numeric types (int, long, float, double): Works only if string contains a valid numeric value (e.g., \"123\" \u2192 int)</li> <li>Numeric conversions: Work between compatible numeric types (int \u2194 long, float \u2194 double)</li> <li>Complex types: Dict \u2192 JSON, lists/structs/tuples with matching schemas</li> </ul> <p>Failed Conversions:</p> <ul> <li>Invalid string \u2192 numeric: Throws exception and stops processing (e.g., \"not_a_number\" \u2192 int fails)</li> <li>Incompatible complex types: Mismatched schemas or structures</li> </ul> <p>Example: <pre><code>functions:\n  string_to_int:\n    type: valueTransformer\n    code: |\n      result = \"123\"        # Valid numeric string\n    expression: result\n    resultType: int         # \u2190 Succeeds: converts \"123\" \u2192 123\n\n  invalid_conversion:\n    type: valueTransformer\n    code: |\n      result = \"not_a_number\"  # Invalid numeric string\n    expression: result\n    resultType: int         # \u2190 Fails: throws conversion exception\n</code></pre></p> Working example - Automatic type conversion in functions <p>Producer: <pre><code># Producer for automatic type conversion example\n# Generates test data for demonstrating function result type conversion\n\nstreams:\n  sensor_data:\n    topic: sensor_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_sensor_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      cities = [\"Amsterdam\", \"Rotterdam\", \"Utrecht\", \"The Hague\"]\n    code: |\n      global counter, cities\n      counter += 1\n\n      # Create sensor data\n      data = {\n        \"sensor_id\": f\"sensor_{counter}\",\n        \"temperature\": round(random.uniform(15.0, 30.0), 1),\n        \"humidity\": round(random.uniform(40.0, 80.0), 1),\n        \"city\": random.choice(cities),\n        \"timestamp\": counter * 1000\n      }\n\n      return (data[\"sensor_id\"], data)\n    resultType: (string, json)\n\nproducers:\n  sensor_generator:\n    generator: generate_sensor_data\n    interval: 2s\n    to: sensor_data\n</code></pre></p> <p>Processor: <pre><code># Demonstrates automatic type conversion in KSML functions\n# Shows how KSML automatically converts function return values to match the declared resultType\n\nfunctions:\n  # Example 1: String to int conversion (valid numeric string)\n  string_to_int:\n    type: valueTransformer\n    code: |\n      result = \"123\"        # Return a valid numeric string\n    expression: result\n    resultType: int         # Automatic conversion: \"123\" to 123\n\n  # Example 2: Int to string conversion\n  int_to_string:\n    type: valueTransformer\n    code: |\n      result = 456          # Return an integer\n    expression: result\n    resultType: string      # Automatic conversion: 456 to \"456\"\n\n  # Example 3: Dict to string conversion (JSON serialization)\n  dict_to_string:\n    type: valueTransformer\n    code: |\n      result = {\n        \"sensor\": key,\n        \"temp_celsius\": value.get(\"temperature\", 20),\n        \"status\": \"active\"\n      }\n    expression: result\n    resultType: string      # Automatic conversion: dict to JSON string\n\npipelines:\n  demonstrate_auto_conversion:\n    from:\n      topic: sensor_data\n      keyType: string\n      valueType: json\n      offsetResetPolicy: earliest\n    via:\n      # Test 1: String to int\n      - type: transformValue\n        mapper: string_to_int\n      - type: peek\n        forEach:\n          code: 'log.info(\"String to Int: type={}, value={}\", type(value).__name__, value)'\n\n      # Test 2: Int to string\n      - type: transformValue\n        mapper: int_to_string\n      - type: peek\n        forEach:\n          code: 'log.info(\"Int to String: type={}, value={}\", type(value).__name__, value)'\n    to:\n      topic: converted_data\n      keyType: string\n      valueType: string\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#stream-format-conversion-explicit","title":"Stream Format Conversion (Explicit)","text":"<p>Streams require explicit <code>convertValue</code> operations when formats differ:</p> <pre><code>pipelines:\n  example_pipeline:\n    from: json_input      # JSON format\n    via:\n      - type: convertValue\n        into: string      # Must explicitly convert\n    to: string_output     # String format\n</code></pre> <p>Without <code>convertValue</code>, KSML will fail with a type mismatch error.</p> Working example - Explicit stream conversion <p>Producer: <pre><code># Producer for explicit format conversion example\n# Generates JSON messages that will be converted to different formats\n\nstreams:\n  json_messages:\n    topic: json_messages\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_json_data:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      counter = 0\n      cities = [\"Amsterdam\", \"Rotterdam\", \"Utrecht\", \"The Hague\", \"Eindhoven\"]\n      types = [\"temperature\", \"humidity\", \"pressure\"]\n    code: |\n      global counter, cities, types\n      counter += 1\n\n      sensor_data = {\n        \"sensor_id\": f\"sensor_{counter % 10}\",\n        \"city\": random.choice(cities),\n        \"type\": random.choice(types),\n        \"value\": round(random.uniform(0, 100), 2),\n        \"unit\": \"%\" if types == \"humidity\" else \"\u00b0C\" if types == \"temperature\" else \"hPa\",\n        \"timestamp\": int(time.time())\n      }\n\n      return (sensor_data[\"sensor_id\"], sensor_data)\n    resultType: (string, json)\n\nproducers:\n  json_generator:\n    generator: generate_json_data\n    interval: 2s\n    to: json_messages\n</code></pre></p> <p>Processor: <pre><code># Demonstrates explicit format conversion in KSML\n# \n# This example shows that KSML requires explicit conversion operations when\n# outputting to a stream with a different format type. Without explicit conversion,\n# KSML will fail with a type mismatch error at the pipeline sink.\n\nstreams:\n  json_input:\n    topic: json_messages\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  # Output stream with different format - requires explicit conversion\n  string_output:\n    topic: string_messages\n    keyType: string\n    valueType: string\n\npipelines:\n  explicit_conversion_pipeline:\n    from: json_input\n    via:\n      # Log the incoming JSON data\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"JSON input - sensor: {}, city: {}, type: {}, value: {}{}\",\n                     value.get(\"sensor_id\"), value.get(\"city\"), \n                     value.get(\"type\"), value.get(\"value\"), value.get(\"unit\"))\n\n      # REQUIRED: Explicit conversion from JSON to string\n      # Without this, KSML will fail with:\n      # \"Target topic valueType is expected to be type json, but found string\"\n      - type: convertValue\n        into: string\n\n      # Log the converted string data\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"String output - sensor: {}, data: {}\", key, value[:100] if len(value) &gt; 100 else value)\n\n    # Output to string format stream\n    to: string_output\n</code></pre></p>"},{"location":"reference/data-and-formats-reference/#chaining-multiple-conversions","title":"Chaining Multiple Conversions","text":"<p>Chain <code>convertValue</code> operations for complex transformations:</p> <pre><code>pipelines:\n  multi_conversion:\n    from: json_stream\n    via:\n      - type: convertValue\n        into: string      # JSON \u2192 String\n      - type: convertValue  \n        into: json        # String \u2192 JSON\n    to: json_output\n</code></pre> Working example - Chained conversions <p>Producer: <pre><code># Producer for multiple format conversions example\n# Generates JSON data that will be converted through multiple formats\n\nstreams:\n  multi_format_data:\n    topic: multi_format_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_data:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      data = {\n        \"id\": counter,\n        \"product\": f\"product_{counter}\",\n        \"price\": round(random.uniform(10.0, 500.0), 2),\n        \"quantity\": random.randint(1, 10),\n        \"available\": counter % 2 == 0,\n        \"timestamp\": int(time.time())\n      }\n\n      return (f\"item_{counter}\", data)\n    resultType: (string, json)\n\nproducers:\n  data_generator:\n    generator: generate_data\n    interval: 2s\n    to: multi_format_data\n</code></pre></p> <p>Processor: <pre><code># Demonstrates chaining multiple format conversions in KSML\n# \n# This example shows how to chain multiple convertValue operations\n# to transform data through several formats in a single pipeline.\n\nstreams:\n  json_input:\n    topic: multi_format_data\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  final_output:\n    topic: final_output\n    keyType: string\n    valueType: string\n\nfunctions:\n  calculate_total_value:\n    type: valueTransformer\n    resultType: json\n    globalCode: |\n      import time\n    code: |\n      # Calculate total value and add metadata\n      result = {\n        \"product_id\": f\"PRD-{value.get('id'):04d}\",\n        \"name\": value.get(\"product\", \"\").upper(),\n        \"total_value\": value.get(\"price\", 0) * value.get(\"quantity\", 0),\n        \"in_stock\": value.get(\"available\", False),\n        \"processed_at\": int(time.time())\n      }\n      return result\n\npipelines:\n  multi_format_conversion:\n    from: json_input\n    via:\n      # Starting with JSON format\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Step 1 - JSON input: id={}, product={}, price=${}\", \n                     value.get(\"id\"), value.get(\"product\"), value.get(\"price\"))\n\n      # Convert JSON to String (serialization)\n      - type: convertValue\n        into: string\n\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Step 2 - Converted to String: {}\", value[:80])\n\n      # Convert String back to JSON (parsing)\n      - type: convertValue\n        into: json\n\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Step 3 - Parsed back to JSON: available={}, quantity={}\", \n                     value.get(\"available\"), value.get(\"quantity\"))\n\n      # Transform the data while in JSON format\n      - type: transformValue\n        mapper: calculate_total_value\n\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Step 4 - Transformed data: product_id={}, total_value=${}\", \n                     value.get(\"product_id\"), value.get(\"total_value\"))\n\n      # Final conversion to String for output\n      - type: convertValue\n        into: string\n\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Step 5 - Final String output: {}\", value)\n\n    # Output as string format\n    to: final_output\n</code></pre></p> <p>Key Takeaway: Functions convert automatically, streams need explicit conversion.</p>"},{"location":"reference/data-and-formats-reference/#working-with-multiple-formats-in-a-single-pipeline","title":"Working with Multiple Formats in a Single Pipeline","text":"<p>Process different data formats within one KSML definition using separate pipelines.</p> <p>This producer generates both JSON config data and Avro sensor data:</p> Producer definition (click to expand) <pre><code>functions:\n  generate_device_config:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      configCounter = 0\n    code: |\n      global configCounter\n\n      device_id = \"sensor\" + str(configCounter)\n      configCounter = (configCounter + 1) % 10\n\n      # Generate device configuration as JSON\n      config = {\n        \"device_id\": device_id,\n        \"threshold\": random.randrange(50, 90),\n        \"alert_level\": random.choice([\"LOW\", \"MEDIUM\", \"HIGH\"]),\n        \"calibration_factor\": round(random.uniform(0.8, 1.2), 2),\n        \"last_maintenance\": str(round(time.time() * 1000))\n      }\n    expression: (device_id, config)\n    resultType: (string, json)\n\n  generate_sensor_reading:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\" + str(sensorCounter)\n      sensorCounter = (sensorCounter + 1) % 10\n\n      # Generate sensor reading data that will be output as Avro\n      reading = {\n        \"name\": key,\n        \"timestamp\": str(round(time.time() * 1000)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"PRESSURE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"value\": str(random.randrange(0, 100)),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"])\n      }\n    expression: (key, reading)\n    resultType: (string, json)  # Generate as JSON, output as Avro\n\nproducers:\n  # Produce JSON device configuration every 10 seconds\n  device_config_producer:\n    generator: generate_device_config\n    interval: 10s\n    to:\n      topic: device_config\n      keyType: string\n      valueType: json\n\n  # Produce Avro sensor readings every 3 seconds\n  sensor_reading_producer:\n    generator: generate_sensor_reading\n    interval: 3s\n    to:\n      topic: sensor_readings\n      keyType: string\n      valueType: avro:SensorData\n</code></pre> <p>This processor shows two pipelines handling different formats (Avro and JSON) and combining results:</p> Processor definition for working with multiple formats in a single pipeline (click to expand) <pre><code>streams:\n  avro_sensor_stream:\n    topic: sensor_readings\n    keyType: string\n    valueType: avro:SensorData\n    offsetResetPolicy: latest\n\n  json_config_stream:\n    topic: device_config\n    keyType: string\n    valueType: json\n    offsetResetPolicy: latest\n\n  combined_output:\n    topic: combined_sensor_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  add_avro_source:\n    type: valueTransformer\n    resultType: json\n    code: |\n      result = dict(value) if value else {}\n      result[\"source_format\"] = \"Avro\"\n      return result\n\n  transform_json_to_sensor:\n    type: valueTransformer\n    resultType: json\n    code: |\n      return {\n        \"name\": key,\n        \"type\": \"CONFIG\",\n        \"threshold\": value.get(\"threshold\"),\n        \"alert_level\": value.get(\"alert_level\"),\n        \"source_format\": \"JSON\"\n      }\n\npipelines:\n  # Pipeline 1: Process Avro data and convert to JSON\n  avro_processing:\n    from: avro_sensor_stream\n    via:\n      # Log Avro input\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Avro sensor: name={}, type={}, value={}{}\",\n                    value.get(\"name\"), value.get(\"type\"),\n                    value.get(\"value\"), value.get(\"unit\"))\n\n      # Add a source field to identify the format\n      - type: transformValue\n        mapper: add_avro_source\n\n    to: combined_output\n\n  # Pipeline 2: Process JSON config data\n  json_processing:\n    from: json_config_stream\n    via:\n      # Log JSON input\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"JSON config: device={}, threshold={}, alert={}\",\n                    key, value.get(\"threshold\"), value.get(\"alert_level\"))\n\n      # Transform to sensor-like format with source field\n      - type: transformValue\n        mapper: transform_json_to_sensor\n\n    to: combined_output\n</code></pre>"},{"location":"reference/data-and-formats-reference/#type-definition-quoting-rules","title":"Type Definition Quoting Rules","text":"<p>In KSML, quotes around type definitions are always optional. KSML can parse all type expressions correctly whether they have quotes or not. The choice to use quotes is purely a matter of style and preference.</p>"},{"location":"reference/data-and-formats-reference/#all-type-expressions-work-without-quotes","title":"All Type Expressions Work Without Quotes:","text":"<pre><code># Basic types\nkeyType: string\nvalueType: json\nresultType: int\n\n# Function-style types\nvalueType: enum(PENDING, PROCESSING, SHIPPED)\nvalueType: map(string)\nkeyType: windowed(string)\nresultType: list(int)\nresultType: tuple(string, json)\n\n# Complex expressions\nvalueType: union(null, string)\nresultType: list(tuple(string, json)) # [(string, json)] also valid\nresultType: (string, json)\n\n# Notation prefixes (with colons)\nvalueType: avro:SensorData\nkeyType: protobuf:UserProfile\n\n# With quotes (also valid)\nresultType: list(tuple(string, json)) # [(string, json)] also valid\nvalueType: enum(PENDING, SHIPPED)\n</code></pre>"},{"location":"reference/data-and-formats-reference/#yaml-syntax-highlighting-note","title":"YAML Syntax Highlighting Note","text":"<p>Some YAML syntax highlighters may incorrectly interpret bracket notation like <code>[(string, json)]</code>, expecting proper array syntax.</p> <p>For better highlighting, use quotes <code>\"[(string, json)]\"</code> or the cleaner <code>list(tuple(string, json))</code> syntax.</p>"},{"location":"reference/data-and-formats-reference/#summary","title":"Summary:","text":"<p>All type expressions work without quotes in KSML. Use quotes only if you prefer them for style, but they are never functionally required. For bracket notation, consider using the <code>list()</code> function syntax for cleaner, more readable code.</p>"},{"location":"reference/definition-reference/","title":"KSML Definition Reference","text":"<p>This reference guide covers the structure and organization of KSML definition files. A KSML definition is a YAML file that describes your complete stream processing application.</p>"},{"location":"reference/definition-reference/#ksml-file-structure","title":"KSML File Structure","text":"<p>KSML supports two main patterns for applications. Choose the pattern that matches your use case:</p>"},{"location":"reference/definition-reference/#stream-processing-applications","title":"Stream Processing Applications","text":"<p>Process data from input topics to output topics:</p> <pre><code># Application metadata (optional)\nname: \"order-processor\"         # Optional\nversion: \"1.0.0\"               # Optional  \ndescription: \"Process orders\"  # Optional\n\n# Data sources and sinks (optional - can be inlined)\nstreams:       # KStream definitions (optional)\ntables:        # KTable definitions (optional)\nglobalTables:  # GlobalKTable definitions (optional)\n\n# State storage (optional - only if needed)\nstores:        # State store definitions (optional)\n\n# Processing logic\nfunctions:     # Python function definitions (optional)\npipelines:     # Data flow pipelines (REQUIRED)\n</code></pre> <p>Required sections: <code>pipelines</code> Optional sections: All others (streams/tables can be inlined in pipelines)</p>"},{"location":"reference/definition-reference/#data-generation-applications","title":"Data Generation Applications","text":"<p>Generate and produce data to Kafka topics:</p> <pre><code># Application metadata (optional)\nname: \"sensor-data-generator\"   # Optional\nversion: \"1.0.0\"               # Optional\ndescription: \"Generate sensor data\"  # Optional\n\n# Processing logic\nfunctions:     # Python function definitions (REQUIRED - must include generator functions)\nproducers:     # Data producer definitions (REQUIRED)\n</code></pre> <p>Required sections: <code>functions</code> (with generator functions), <code>producers</code> Optional sections: <code>name</code>, <code>version</code>, <code>description</code></p> <p>Note: Data generation applications don't use streams/tables/stores sections since they only produce data.</p>"},{"location":"reference/definition-reference/#application-metadata","title":"Application Metadata","text":"<p>Optional metadata to describe your KSML application:</p> Property Type Required Description <code>name</code> String No The name of the KSML definition <code>version</code> String No The version of the KSML definition <code>description</code> String No A description of the KSML definition <pre><code>name: \"order-processing-app\"\nversion: \"1.2.3\"\ndescription: \"Processes orders and enriches them with customer data\"\n</code></pre>"},{"location":"reference/definition-reference/#data-sources-and-targets","title":"Data Sources and Targets","text":"<p>KSML supports three types of data streams based on Kafka Streams concepts. Each stream type has different characteristics and use cases for processing streaming data.</p>"},{"location":"reference/definition-reference/#streams-kstream","title":"Streams (KStream)","text":"<p>Use for: Event-based processing where each record is an independent event.</p> <pre><code>streams:\n  user_clicks:\n    topic: user-clicks\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest  # Optional\n    timestampExtractor: click_timestamp_extractor  # Optional\n    partitioner: click_partitioner  # Optional\n</code></pre> <p>Key characteristics:</p> <ul> <li>Records are immutable and processed individually</li> <li>Each record represents an independent event or fact</li> <li>Records arrive in order and are processed one at a time</li> <li>Ideal for: user actions, sensor readings, transactions, logs</li> </ul> Property Type Required Description <code>topic</code> String Yes The Kafka topic to read from or write to <code>keyType</code> String Yes The type of the record key <code>valueType</code> String Yes The type of the record value <code>offsetResetPolicy</code> String No The offset reset policy. Valid values: earliest, latest, none, by_duration: (e.g., by_duration:PT1H for 1 hour). Default: Kafka Streams default (typically latest) <code>timestampExtractor</code> String No Function name to extract timestamps from records. Default: Kafka Streams default (message timestamp, fallback to current time) <code>partitioner</code> String No Function name that determines message partitioning for this stream/table. Default: Kafka default (hash-based on key)"},{"location":"reference/definition-reference/#stream-example-with-offsetresetpolicy","title":"Stream Example with <code>offsetResetPolicy</code>","text":"<ul> <li><code>offsetResetPolicy</code> example</li> </ul>"},{"location":"reference/definition-reference/#stream-example-with-timestampextractor","title":"Stream Example with <code>timestampExtractor</code>","text":"<ul> <li><code>timestampExtractor</code> example</li> </ul>"},{"location":"reference/definition-reference/#tables-ktable","title":"Tables (KTable)","text":"<p>Use for: State-based processing where records represent updates to entities.</p> <pre><code>tables:\n  user_profiles:\n    topic: user-profiles\n    keyType: string\n    valueType: avro:UserProfile\n    store: user_profiles_store  # Optional state store name\n</code></pre> <p>Key characteristics:</p> <ul> <li>Records with the same key represent updates to the same entity</li> <li>Only the latest record for each key is retained (compacted)</li> <li>Represents a changelog stream with the current state</li> <li>Ideal for: user profiles, inventory levels, configuration settings</li> </ul> Property Type Required Description <code>topic</code> String Yes The Kafka topic to read from or write to <code>keyType</code> String Yes The type of the record key <code>valueType</code> String Yes The type of the record value <code>offsetResetPolicy</code> String No The offset reset policy. Valid values: earliest, latest, none, by_duration: (e.g., by_duration:PT1H for 1 hour). Default: Kafka Streams default (typically latest) <code>timestampExtractor</code> String No Function name to extract timestamps from records. Default: Kafka Streams default (message timestamp, fallback to current time) <code>partitioner</code> String No Function that determines message partitioning <code>store</code> String No The name of the key/value state store to use. Default: Auto-created store using topic name"},{"location":"reference/definition-reference/#table-example-without-store","title":"Table Example without <code>store</code>","text":"<ul> <li>Table example in Circuit Breaker Pattern</li> </ul>"},{"location":"reference/definition-reference/#table-example-with-store","title":"Table Example with <code>store</code>","text":"<p>This example demonstrates using a custom inline state store for a table. The table uses custom persistence and caching settings, and the processor function accesses the table to enrich streaming data.</p> <pre><code>tables:\n  user_profiles:\n    topic: user_profiles\n    keyType: string\n    valueType: json\n    store:\n      name: user_profiles_store\n      type: keyValue\n      persistent: true\n      caching: true\n      logging: false\n</code></pre> Producer - User Profile Data (click to expand) <pre><code>streams:\n  user_profiles_output:\n    topic: user_profiles\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_user_profile:\n    type: generator\n    resultType: (string, json)\n    code: |\n      import random\n      import time\n\n      # Generate realistic user profile data\n      user_id = \"user_\" + str(random.randint(1, 1000))\n      departments = [\"engineering\", \"marketing\", \"sales\", \"support\", \"hr\"]\n      locations = [\"new_york\", \"london\", \"tokyo\", \"sydney\"]\n\n      profile = {\n        \"user_id\": user_id,\n        \"name\": f\"User {user_id.split('_')[1]}\",\n        \"email\": f\"{user_id}@company.com\",\n        \"department\": random.choice(departments),\n        \"location\": random.choice(locations),\n        \"created_at\": int(time.time() * 1000),\n        \"is_active\": True\n      }\n\n      return (user_id, profile)\n\nproducers:\n  user_profile_producer:\n    to:\n      topic: user_profiles\n      keyType: string\n      valueType: json\n    generator: generate_user_profile\n    interval: 3000\n</code></pre> Processor - Enrich Activity with Profiles (click to expand) <pre><code>streams:\n  user_activity:\n    topic: user_activity\n    keyType: string\n    valueType: json\n  enriched_activity_output:\n    topic: enriched_user_activity\n    keyType: string\n    valueType: json\n\ntables:\n  user_profiles:\n    topic: user_profiles\n    keyType: string\n    valueType: json\n    store:\n      name: user_profiles_store\n      type: keyValue\n      persistent: true\n      caching: true\n      logging: false\n\nfunctions:\n  enrich_with_user_profile:\n    type: valueTransformer\n    resultType: json\n    stores:\n      - user_profiles\n    code: |\n      import time\n\n      # Look up user profile from the custom store\n      user_id = value.get(\"user_id\")\n      if not user_id:\n        log.warn(\"No user_id found in activity event\")\n        return value\n\n      # Access the table which uses the custom store\n      profile = user_profiles.get(user_id)\n      if profile:\n        # Enrich activity with user profile data\n        enriched_activity = {\n          **value,\n          \"user_name\": profile.get(\"name\", \"Unknown\"),\n          \"user_department\": profile.get(\"department\", \"Unknown\"),\n          \"user_location\": profile.get(\"location\", \"Unknown\"),\n          \"enriched_at\": int(time.time() * 1000),\n          \"store_used\": \"custom_inline_store\"\n        }\n        log.info(\"Enriched activity for user: {}, department: {}, location: {}\", \n                 user_id, profile.get(\"department\"), profile.get(\"location\"))\n        return enriched_activity\n      else:\n        log.warn(\"User profile not found for user_id: {}\", user_id)\n        return {\n          **value,\n          \"enriched_at\": int(time.time() * 1000),\n          \"enrichment_status\": \"profile_not_found\"\n        }\n\nproducers:\n  user_activity_producer:\n    to:\n      topic: user_activity\n      keyType: string\n      valueType: json\n    generator:\n      resultType: (string, json)\n      code: |\n        import random\n        import time\n\n        # Generate activity events referencing existing users\n        user_id = \"user_\" + str(random.randint(1, 1000))\n        activities = [\"login\", \"logout\", \"page_view\", \"document_upload\", \"meeting_join\"]\n\n        activity = {\n          \"user_id\": user_id,\n          \"activity_type\": random.choice(activities),\n          \"timestamp\": int(time.time() * 1000),\n          \"session_id\": \"session_\" + str(random.randint(1000, 9999))\n        }\n\n        return (user_id, activity)\n    interval: 2000\n\npipelines:\n  enrich_user_activity:\n    from: user_activity\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processing activity: {} for user: {}\", \n                     value.get(\"activity_type\"), value.get(\"user_id\"))\n      - type: transformValue\n        mapper: enrich_with_user_profile\n    to: enriched_activity_output\n</code></pre> <p>The table <code>user_profiles</code> uses an inline store definition with custom settings:</p> <ul> <li><code>persistent: true</code> - Data survives application restarts</li> <li><code>caching: true</code> - Enables local caching for better performance  </li> <li><code>logging: false</code> - Disables changelog topic creation</li> </ul> <p>The enrichment function accesses the table via <code>stores: [user_profiles]</code> declaration and performs lookups using <code>user_profiles.get(user_id)</code>.</p>"},{"location":"reference/definition-reference/#global-tables-globalktable","title":"Global Tables (GlobalKTable)","text":"<p>Use for: Reference data that needs to be available on all application instances.</p> <pre><code>globalTables:\n  product_catalog:\n    topic: product-catalog\n    keyType: string\n    valueType: avro:Product\n    store: product_catalog_store  # Optional state store name\n</code></pre> <p>Key characteristics:</p> <ul> <li>Fully replicated on each application instance (not partitioned)</li> <li>Allows joins without requiring co-partitioning</li> <li>Provides global access to reference data</li> <li>Ideal for: product catalogs, country codes, small to medium reference datasets</li> </ul> Property Type Required Description <code>topic</code> String Yes The Kafka topic to read from <code>keyType</code> String Yes The type of the record key <code>valueType</code> String Yes The type of the record value <code>offsetResetPolicy</code> String No The offset reset policy. Valid values: earliest, latest, none, by_duration: (e.g., by_duration:PT1H for 1 hour). Default: Kafka Streams default (typically latest) <code>timestampExtractor</code> String No Function name to extract timestamps from records. Default: Kafka Streams default (message timestamp, fallback to current time) <code>partitioner</code> String No Function that determines message partitioning <code>store</code> String No The name of the key/value state store to use. Default: Auto-created store using topic name"},{"location":"reference/definition-reference/#global-table-example","title":"Global Table Example","text":"<ul> <li>GlobalTable Example</li> </ul>"},{"location":"reference/definition-reference/#choosing-the-right-stream-type","title":"Choosing the Right Stream Type","text":"If you need to... Consider using... Process individual events as they occur KStream Maintain the latest state of entities KTable Join with data that's needed across all partitions GlobalKTable Process time-ordered events KStream Track changes to state over time KTable Access reference data without worrying about partitioning GlobalKTable"},{"location":"reference/definition-reference/#pipelines","title":"Pipelines","text":"<p>Define how data flows through your application:</p> <pre><code>pipelines:\n  process_orders:\n    from: orders\n    via:\n      - type: filter\n        if: is_valid_order\n      - type: mapValues\n        mapper: enrich_order\n    to: processed_orders\n</code></pre> <p>For complete pipeline documentation, see Pipeline Reference.</p>"},{"location":"reference/definition-reference/#functions","title":"Functions","text":"<p>Define reusable Python logic for processing:</p> <pre><code>functions:\n  is_valid_order:\n    type: predicate\n    expression: value.get(\"total\") &gt; 0\n</code></pre> <p>For complete function documentation, see Function Reference.</p>"},{"location":"reference/definition-reference/#operations","title":"Operations","text":"<p>Operations are the building blocks that transform, filter, and process your data within pipelines:</p> <pre><code>via:\n  - type: filter        # Keep matching records\n    if: is_valid_order\n  - type: mapValues     # Transform record values\n    mapper: enrich_order\n  - type: join          # Combine with other streams\n    with: customers\n</code></pre> <p>For complete operation documentation, see Operation Reference.</p>"},{"location":"reference/definition-reference/#state-stores","title":"State Stores","text":"<p>Define persistent state stores for stateful operations:</p> <pre><code>stores:\n  session_store:\n    type: keyValue\n    keyType: string\n    valueType: json\n    persistent: true\n    caching: true\n</code></pre> <p>For details, see State Store Reference.</p>"},{"location":"reference/definition-reference/#producers","title":"Producers","text":"<p>Define data generators for testing and simulation:</p> <pre><code>producers:\n  test_producer:\n    target: test_topic\n    generator: generate_test_data\n    interval: 1000\n</code></pre>"},{"location":"reference/definition-reference/#producer-example","title":"Producer Example","text":"<ul> <li>Producer Example</li> </ul>"},{"location":"reference/function-reference/","title":"Function Reference","text":"<p>KSML functions let you implement custom stream-processing logic in Python. They make it easier for data scientists, analysts, and developers to process streaming data  without needing Java or the Kafka Streams API.</p> <p>Functions extend built-in operations, enabling custom business logic, transformations, and processing within the KSML runtime- combining Kafka Streams\u2019 power with Python\u2019s simplicity.</p>"},{"location":"reference/function-reference/#function-definition-structure","title":"Function Definition Structure","text":"<p>Functions are defined in the <code>functions</code> section of your KSML definition file. Each function has the following properties:</p> Property Type Required Description <code>type</code> String Yes The type of function (predicate, aggregator, valueJoiner, etc.) <code>parameters</code> Array No Additional custom parameters to add to the function's built-in parameters (see note below) <code>globalCode</code> String No Python code executed once upon startup <code>code</code> String No Python code implementing the function <code>expression</code> String No An expression that the function will return as value <code>resultType</code> Data type Sometimes The data type returned by the function. Required when it cannot be derived from function type. <code>stores</code> Array No List of state stores the function can access <p>Note about parameters: Every function type has built-in parameters that are automatically provided by KSML (e.g., <code>key</code> and <code>value</code> for most function types). The <code>parameters</code> property is only needed when you want to add custom parameters beyond these built-in ones. These additional parameters can then be passed when calling the function from Python code.</p>"},{"location":"reference/function-reference/#writing-ksml-functions","title":"Writing KSML Functions","text":""},{"location":"reference/function-reference/#example-ksml-function-definition","title":"Example KSML Function Definition","text":"Example KSML Function Definition <pre><code>functions:\n  # Example of a complete function definition with all components\n  process_sensor_data:\n    type: valueTransformer\n    globalCode: |\n      # This code runs once when the application starts\n      import json\n      import time\n\n      # Initialize global variables\n      sensor_threshold = 25.0\n      alert_count = 0\n\n    code: |\n      # This code runs for each message\n      global alert_count\n\n      # Process the sensor value\n      if value is None:\n        return None\n\n      temperature = value.get(\"temperature\", 0)\n\n      # Convert Celsius to Fahrenheit\n      temperature_f = (temperature * 9/5) + 32\n\n      # Check for alerts\n      is_alert = temperature &gt; sensor_threshold\n      if is_alert:\n        alert_count += 1\n        log.warn(\"High temperature detected: {}\u00b0C\", temperature)\n\n      # Return enriched data\n      result = {\n        \"original_temp_c\": temperature,\n        \"temp_fahrenheit\": temperature_f,\n        \"is_alert\": is_alert,\n        \"total_alerts\": alert_count,\n        \"processed_at\": int(time.time() * 1000)\n      }\n\n      return result\n\n    resultType: json\n\n  # Example of a simple expression-based function\n  is_high_priority:\n    type: predicate\n    expression: value.get(\"priority\", 0) &gt; 7\n    resultType: boolean\n</code></pre> <p>KSML functions are defined in the <code>functions</code> section of your KSML definition file. A typical function definition includes:</p> <ul> <li>Type: Specifies the function's purpose and behavior</li> <li>Parameters: Input parameters the function accepts (defined by the function type)</li> <li>GlobalCode: Python code executed only once upon application start</li> <li>Code: Python code implementing the function's logic</li> <li>Expression: Shorthand for simple return expressions</li> <li>ResultType: The expected return type of the function</li> </ul>"},{"location":"reference/function-reference/#function-definition-formats","title":"Function Definition Formats","text":"<p>KSML supports two formats for defining functions:</p>"},{"location":"reference/function-reference/#expression-format","title":"Expression Format","text":"<p>For simple, one-line functions:</p> <pre><code>functions:\n  is_valid:\n    type: predicate\n    code: |\n      # Code is optional here\n    expression: value.get(\"status\") == \"ACTIVE\"\n</code></pre>"},{"location":"reference/function-reference/#code-block-format","title":"Code Block Format","text":"<p>For more complex functions:</p> <pre><code>functions:\n  process_transaction:\n    type: keyValueMapper\n    code: |\n      result = {}\n\n      # Copy basic fields\n      result[\"transaction_id\"] = value.get(\"id\")\n      result[\"amount\"] = value.get(\"amount\", 0)\n\n      # Calculate fee\n      amount = value.get(\"amount\", 0)\n      if amount &gt; 1000:\n        result[\"fee\"] = amount * 0.02\n      else:\n        result[\"fee\"] = amount * 0.03\n\n      # Add timestamp\n      result[\"processed_at\"] = int(time.time() * 1000)\n\n      return result\n    resultType: struct\n</code></pre>"},{"location":"reference/function-reference/#function-parameters","title":"Function Parameters","text":""},{"location":"reference/function-reference/#built-in-vs-custom-parameters","title":"Built-in vs Custom Parameters","text":"<p>Every function type in KSML has built-in parameters that are automatically provided by KSML. These are implicitly available in your function code without needing to declare them:</p> <p>Most function types (like <code>forEach</code>, <code>predicate</code>, <code>valueTransformer</code>) automatically receive:</p> <ul> <li><code>key</code> - The record key</li> <li><code>value</code> - The record value</li> </ul> <p>Some specialized types have different built-in parameters:</p> <ul> <li><code>aggregator</code>: receives <code>key</code>, <code>value</code>, and <code>aggregate</code></li> <li><code>merger</code>: receives <code>key</code>, <code>aggregate1</code>, and <code>aggregate2</code></li> <li><code>initializer</code>: receives no parameters</li> </ul>"},{"location":"reference/function-reference/#adding-custom-parameters","title":"Adding Custom Parameters","text":"<p>The <code>parameters</code> property allows you to add custom parameters beyond the built-in ones. This is useful when:</p> <ol> <li>Creating reusable functions that can behave differently based on configuration</li> <li>Calling functions from Python code with specific arguments</li> <li>Using the <code>generic</code> function type which has no built-in parameters</li> </ol>"},{"location":"reference/function-reference/#example-without-custom-parameters","title":"Example WITHOUT custom parameters:","text":"<pre><code>functions:\n  simple_logger:\n    type: forEach\n    # Only uses built-in key and value parameters\n    code: |\n      log.info(\"Processing: key={}, value={}\", key, value)\n</code></pre>"},{"location":"reference/function-reference/#example-with-custom-parameters","title":"Example WITH custom parameters:","text":"<pre><code>functions:\n  configurable_logger:\n    type: forEach\n    parameters: # ADDS 'prefix' to the built-in key and value\n      - name: prefix\n        type: string\n    code: |\n      log.info(\"{}: key={}, value={}\", prefix, key, value)\n</code></pre> <p>When calling this function from Python:</p> <pre><code># The custom parameter is passed along with built-in ones\nconfigurable_logger(key, value, prefix=\"DEBUG\")\n</code></pre>"},{"location":"reference/function-reference/#parameter-definition-structure","title":"Parameter Definition Structure","text":"<p>When defining custom parameters:</p> <pre><code>parameters:\n  - name: parameter_name   # Name of the parameter\n    type: parameter_type   # Data type (string, int, double, etc.)\n</code></pre> <p>Important: The <code>parameters</code> property adds to the built-in parameters - it doesn't replace them. Built-in parameters like <code>key</code> and <code>value</code> are still available in your function code.</p>"},{"location":"reference/function-reference/#function-types-overview","title":"Function Types Overview","text":"<p>Below is a table with all 21 function types in KSML.</p> Function Type Purpose Used In Functions for stateless operations forEach Process each message for side effects peek keyTransformer Convert a key to another type or value mapKey, selectKey, toStream, transformKey keyValueToKeyValueListTransformer Convert key and value to a list of key/values flatMap, transformKeyValueToKeyValueList keyValueToValueListTransformer Convert key and value to a list of values flatMapValues, transformKeyValueToValueList keyValueTransformer Convert key and value to another key and value flatMapValues, transformKeyValueToValueList predicate Return true/false based on message content filter, branch valueTransformer Convert value to another type or value mapValue, mapValues, transformValue Functions for stateful operations aggregator Incrementally build aggregated results aggregate initializer Provide initial values for aggregations aggregate merger Merge two aggregation results into one aggregate reducer Combine two values into one reduce Special Purpose Functions foreignKeyExtractor Extract a key from a join table's record join, leftJoin generator Function used in producers to generate a message producer generic Generic custom function keyValueMapper Convert key and value into a single output value groupBy, join, leftJoin keyValuePrinter Output key and value print metadataTransformer Convert Kafka headers and timestamps transformMetadata valueJoiner Combine data from multiple streams join, leftJoin, outerJoin Stream Related Functions streamPartitioner Determine which partition to send records to to timestampExtractor Extract timestamps from messages stream, table, globalTable topicNameExtractor Derive a target topic name from key and value toTopicNameExtractor"},{"location":"reference/function-reference/#functions-for-stateless-operations","title":"Functions for stateless operations","text":""},{"location":"reference/function-reference/#foreach","title":"forEach","text":"<p>Processes each message for side effects like logging, without changing the message.</p>"},{"location":"reference/function-reference/#parameters","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value","title":"Return Value","text":"<p>None (the function is called for its side effects)</p> <pre><code>functions:\n  extract_region_key:\n    type: keyTransformer\n    code: |\n      if value is None: return \"unknown\"\n      return value.get(\"region\", \"unknown\")\n    resultType: string\n</code></pre> <p>Full example for <code>forEach</code>:</p> <ul> <li>Tutorial: Filtering and Transforming Example</li> </ul>"},{"location":"reference/function-reference/#keytransformer","title":"keyTransformer","text":"<p>Transforms a key/value into a new key, which then gets combined with the original value as a new message on the output stream.</p>"},{"location":"reference/function-reference/#parameters_1","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_1","title":"Return Value","text":"<p>New key for the output message</p>"},{"location":"reference/function-reference/#example","title":"Example","text":"<p>Function Definition:</p> <pre><code>functions:\n  extract_region_key:\n    type: keyTransformer\n    code: |\n      if value is None: return \"unknown\"\n      return value.get(\"region\", \"unknown\")\n    resultType: string\n</code></pre> <p>This function extracts the region from transaction data to use as the new message key, enabling region-based partitioning.</p> <p>Complete Working Example:</p> Producer - <code>keyTransformer</code> example (click to expand) <pre><code>functions:\n  generate_region_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n      tid = f\"txn_{counter:05d}\"\n      return tid, {\n        \"transaction_id\": tid,\n        \"region\": random.choice([\"us-east\", \"us-west\", \"europe\", \"asia\"]),\n        \"amount\": round(random.uniform(10.0, 1000.0), 2),\n        \"customer_id\": f\"c{random.randint(1, 100):03d}\",\n        \"timestamp\": counter * 1000\n      }\n    resultType: (string, json)\n\nproducers:\n  region_data_producer:\n    generator: generate_region_data\n    interval: 2s\n    to:\n      topic: transaction_events\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>keyTransformer</code> example (click to expand) <pre><code>streams:\n  transaction_input:\n    topic: transaction_events\n    keyType: string\n    valueType: json\n  region_output:\n    topic: transactions_by_region\n    keyType: string\n    valueType: json\n\nfunctions:\n  extract_region_key:\n    type: keyTransformer\n    code: |\n      if value is None: return \"unknown\"\n      return value.get(\"region\", \"unknown\")\n    resultType: string\n\n  log_repartitioned:\n    type: forEach\n    code: |\n      log.info(\"Repartitioned - Key: {}, Region: {}, Amount: ${}\", \n               key, value.get(\"region\"), value.get(\"amount\"))\n\npipelines:\n  region_repartitioning:\n    from: transaction_input\n    via:\n      - type: mapKey\n        mapper: extract_region_key\n      - type: peek\n        forEach: log_repartitioned\n    to: region_output\n</code></pre> <p>Additional Example:</p> <p>Full example for <code>keyTransformer</code>: Stream Table Join Tutorial</p>"},{"location":"reference/function-reference/#keyvaluetokeyvaluelisttransformer","title":"keyValueToKeyValueListTransformer","text":"<p>Takes one message and converts it into a list of output messages, which then get sent to the output stream. Unlike <code>keyValueToValueListTransformer</code>, this function can create new keys for each output message, enabling data reshaping and repartitioning.</p>"},{"location":"reference/function-reference/#parameters_2","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_2","title":"Return Value","text":"<p>A list of key-value pairs <code>[(key1, value1), (key2, value2), ...]</code></p>"},{"location":"reference/function-reference/#example_1","title":"Example","text":"<pre><code>  split_batch_orders:\n    type: keyValueToKeyValueListTransformer\n    code: |\n      # Split batch orders into individual orders with unique keys\n      # This transforms one batch record into multiple individual order records\n      if value is None or \"orders\" not in value:\n        return []\n\n      batch_id = key\n      orders = value.get(\"orders\", [])\n      individual_records = []\n\n      for i, order in enumerate(orders):\n        # Create unique key for each individual order\n        order_key = f\"{batch_id}_order_{i+1}\"\n\n        # Create individual order record\n        order_value = {\n          \"order_id\": order_key,\n          \"batch_id\": batch_id,\n          \"product\": order.get(\"product\"),\n          \"quantity\": order.get(\"quantity\", 1),\n          \"customer_email\": order.get(\"customer_email\"),\n          \"processing_timestamp\": value.get(\"timestamp\")\n        }\n\n        individual_records.append((order_key, order_value))\n\n      log.info(\"Split batch {} into {} individual orders\", batch_id, len(individual_records))\n      return individual_records\n    resultType: list(tuple(string, json))\n</code></pre> <p>This example demonstrates splitting batch orders into individual orders with unique keys, useful for processing bulk data into individual records.</p> Producer - <code>keyvaluetokeyvaluelisttransformer</code> example (click to expand) <pre><code>functions:\n  generate_batch_orders:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n      bid = f\"b{counter:03d}\"\n      prods = [\"laptop\", \"phone\", \"tablet\"]\n      custs = [\"alice@co\", \"bob@co\", \"charlie@co\"]\n      orders = [{\"product\": random.choice(prods), \"qty\": random.randint(1,5), \n                 \"customer\": random.choice(custs)} for _ in range(random.randint(2,4))]\n      return bid, {\"batch_id\": bid, \"orders\": orders, \"total\": len(orders)}\n    resultType: (string, json)\n\nproducers:\n  batch_order_producer:\n    generator: generate_batch_orders\n    interval: 3s\n    to:\n      topic: batch_orders\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>keyvaluetokeyvaluelisttransformer</code> example (click to expand) <pre><code>streams:\n  batch_orders_input:\n    topic: batch_orders\n    keyType: string\n    valueType: json\n  individual_orders_output:\n    topic: individual_orders\n    keyType: string\n    valueType: json\n\nfunctions:\n  split_batch_orders:\n    type: keyValueToKeyValueListTransformer\n    code: |\n      # Split batch orders into individual orders with unique keys\n      # This transforms one batch record into multiple individual order records\n      if value is None or \"orders\" not in value:\n        return []\n\n      batch_id = key\n      orders = value.get(\"orders\", [])\n      individual_records = []\n\n      for i, order in enumerate(orders):\n        # Create unique key for each individual order\n        order_key = f\"{batch_id}_order_{i+1}\"\n\n        # Create individual order record\n        order_value = {\n          \"order_id\": order_key,\n          \"batch_id\": batch_id,\n          \"product\": order.get(\"product\"),\n          \"quantity\": order.get(\"quantity\", 1),\n          \"customer_email\": order.get(\"customer_email\"),\n          \"processing_timestamp\": value.get(\"timestamp\")\n        }\n\n        individual_records.append((order_key, order_value))\n\n      log.info(\"Split batch {} into {} individual orders\", batch_id, len(individual_records))\n      return individual_records\n    resultType: list(tuple(string, json))\n\npipelines:\n  split_batch_processing:\n    from: batch_orders_input\n    via:\n      - type: transformKeyValueToKeyValueList\n        mapper: split_batch_orders\n    to: individual_orders_output\n</code></pre>"},{"location":"reference/function-reference/#keyvaluetovaluelisttransformer","title":"keyValueToValueListTransformer","text":"<p>Takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream.</p>"},{"location":"reference/function-reference/#parameters_3","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_3","title":"Return Value","text":"<p>A list of values <code>[value1, value2, ...]</code> that will be combined with the original key</p>"},{"location":"reference/function-reference/#example_2","title":"Example","text":"<pre><code>  explode_order_items:\n    type: keyValueToValueListTransformer\n    code: |\n      # Split order into individual item records\n      # Key remains the same (order_id), but each item becomes a separate value\n      if value is None or \"items\" not in value:\n        return []\n\n      items = value.get(\"items\", [])\n      item_records = []\n\n      for item in items:\n        # Create individual item record with order context\n        item_record = {\n          \"order_id\": value.get(\"order_id\"),\n          \"customer_id\": value.get(\"customer_id\"),\n          \"product\": item.get(\"product\"),\n          \"quantity\": item.get(\"quantity\"),\n          \"unit_price\": item.get(\"price\"),\n          \"total_price\": item.get(\"price\", 0) * item.get(\"quantity\", 0),\n          \"order_total\": value.get(\"order_total\")\n        }\n        item_records.append(item_record)\n\n      log.info(\"Exploded order {} into {} item records\", \n               value.get(\"order_id\"), len(item_records))\n\n      return item_records\n    resultType: list(json)\n</code></pre> Producer - <code>keyValueToValueListTransformer</code> example (click to expand) <pre><code>functions:\n  generate_order_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      products = [\"laptop\", \"phone\", \"tablet\", \"headphones\", \"speaker\"]\n    code: |\n      global counter, products\n\n      # Generate order ID as key  \n      order_id = f\"order_{counter:03d}\"\n      counter += 1\n\n      # Generate order with multiple items\n      num_items = random.randint(2, 5)\n      items = []\n\n      for i in range(num_items):\n        item = {\n          \"product\": random.choice(products),\n          \"quantity\": random.randint(1, 3),\n          \"price\": round(random.uniform(50.0, 500.0), 2)\n        }\n        items.append(item)\n\n      # Create order value with items array\n      value = {\n        \"order_id\": order_id,\n        \"customer_id\": f\"cust_{random.randint(1, 50):03d}\",\n        \"items\": items,\n        \"order_total\": sum(item[\"price\"] * item[\"quantity\"] for item in items)\n      }\n\n    expression: (order_id, value)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_order_data\n    interval: 3s\n    to:\n      topic: customer_orders\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>keyValueToValueListTransformer</code> example (click to expand) <pre><code>streams:\n  orders_input:\n    topic: customer_orders\n    keyType: string\n    valueType: json\n  items_output:\n    topic: individual_items\n    keyType: string\n    valueType: json\n\nfunctions:\n  explode_order_items:\n    type: keyValueToValueListTransformer\n    code: |\n      # Split order into individual item records\n      # Key remains the same (order_id), but each item becomes a separate value\n      if value is None or \"items\" not in value:\n        return []\n\n      items = value.get(\"items\", [])\n      item_records = []\n\n      for item in items:\n        # Create individual item record with order context\n        item_record = {\n          \"order_id\": value.get(\"order_id\"),\n          \"customer_id\": value.get(\"customer_id\"),\n          \"product\": item.get(\"product\"),\n          \"quantity\": item.get(\"quantity\"),\n          \"unit_price\": item.get(\"price\"),\n          \"total_price\": item.get(\"price\", 0) * item.get(\"quantity\", 0),\n          \"order_total\": value.get(\"order_total\")\n        }\n        item_records.append(item_record)\n\n      log.info(\"Exploded order {} into {} item records\", \n               value.get(\"order_id\"), len(item_records))\n\n      return item_records\n    resultType: list(json)\n\npipelines:\n  explode_orders:\n    from: orders_input\n    via:\n      - type: transformKeyValueToValueList\n        mapper: explode_order_items\n    to: items_output\n</code></pre>"},{"location":"reference/function-reference/#keyvaluetransformer","title":"keyValueTransformer","text":"<p>Takes one message and converts it into another message, which may have different key/value types.</p>"},{"location":"reference/function-reference/#parameters_4","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_4","title":"Return Value","text":"<p>A tuple of (new_key, new_value)</p>"},{"location":"reference/function-reference/#example_3","title":"Example","text":"<pre><code>  create_external_request:\n    type: keyValueTransformer\n    code: |\n      import time\n\n      # Extract fields from JSON order event\n      if not value:\n        return None\n\n      order_id = value.get(\"order_id\")\n      customer_id = value.get(\"customer_id\")\n      status = value.get(\"status\")\n      amount = value.get(\"amount\")\n      timestamp = value.get(\"timestamp\")\n      sequence_number = value.get(\"sequence_number\")\n      order_details = value.get(\"order_details\", {})\n      customer_info = value.get(\"customer_info\", {})\n      fulfillment = value.get(\"fulfillment\", {})\n      business_context = value.get(\"business_context\", {})\n      metadata = value.get(\"metadata\", {})\n\n      # Create comprehensive request for external payment processing system\n      request_id = f\"REQ_{order_id}_{timestamp}\"\n\n      external_request = {\n        \"request_id\": request_id,\n        \"request_type\": \"PAYMENT_PROCESSING\",\n        \"original_order\": {\n          \"order_id\": order_id,\n          \"customer_id\": customer_id,\n          \"amount\": amount,\n          \"timestamp\": timestamp,\n          \"sequence_number\": sequence_number\n        },\n        \"payment_details\": {\n          \"amount\": amount,\n          \"currency\": order_details.get(\"currency\", \"USD\"),\n          \"payment_method\": order_details.get(\"payment_method\"),\n          \"customer_tier\": customer_info.get(\"customer_tier\"),\n          \"loyalty_points\": customer_info.get(\"loyalty_points\")\n        },\n        \"processing_context\": {\n          \"priority\": business_context.get(\"priority\", \"normal\"),\n          \"high_value\": business_context.get(\"high_value\", False),\n          \"customer_previous_orders\": customer_info.get(\"previous_orders\", 0),\n          \"order_source\": order_details.get(\"order_source\")\n        },\n        \"async_metadata\": {\n          \"correlation_id\": metadata.get(\"correlation_id\", f\"corr_{request_id}\"),\n          \"created_at\": int(time.time() * 1000),\n          \"timeout_ms\": 30000,  # 30 second timeout\n          \"retry_count\": 0,\n          \"expected_response_topic\": \"external_responses\"\n        },\n        \"external_system_info\": {\n          \"target_system\": \"payment_processor\",\n          \"api_version\": \"v2.1\",\n          \"request_format\": \"async_json\",\n          \"callback_required\": True\n        }\n      }\n\n      log.info(\"Created external payment request for order {}: amount=${:.2f}, priority={}\", \n               order_id, amount, business_context.get(\"priority\", \"normal\"))\n\n      return (request_id, external_request)\n\n    expression: result\n    resultType: (string, json)\n</code></pre> <p>Full example for <code>keyValueTransformer</code>:</p> <ul> <li>Async Integration Pattern with   <code>keyValueTransformer</code></li> </ul>"},{"location":"reference/function-reference/#predicate","title":"predicate","text":"<p>Returns true or false based on message content. Used for filtering and branching operations.</p>"},{"location":"reference/function-reference/#parameters_5","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_5","title":"Return Value","text":"<p>Boolean (true or false)</p>"},{"location":"reference/function-reference/#example_4","title":"Example","text":"<pre><code>  is_critical_sensor:\n    type: predicate\n    code: |\n      # Check location\n      if value.get('sensors', {}).get('location') not in ['server_room', 'data_center']:\n        return False\n\n      # Check temperature threshold based on location\n      if value.get('sensors', {}).get('location') == 'server_room' and value.get('sensors', {}).get('temperature') &gt; 20:\n        return True\n      if value.get('sensors', {}).get('location') == 'data_center' and value.get('sensors', {}).get('temperature') &gt; 30:\n        return True\n\n      return False\n</code></pre> <p>Full example for <code>predicate</code>:</p> <ul> <li>Tutorial: Filtering and Transforming   for predicate functions for data filtering</li> </ul>"},{"location":"reference/function-reference/#valuetransformer","title":"valueTransformer","text":"<p>Transforms a key/value into a new value, which is combined with the original key and sent to the output stream.</p>"},{"location":"reference/function-reference/#parameters_6","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_6","title":"Return Value","text":"<p>New value for the output message</p>"},{"location":"reference/function-reference/#example_5","title":"Example","text":"<pre><code>  convert_temperature:\n    type: valueTransformer\n    code: |\n      result = {\n        \"device_id\": value.get('device_id'),\n        \"temperature_c\": round((value.get('temperature') - 32) * 5/9, 2) if value.get('temperature') else None,\n        \"humidity\": value.get('humidity'),\n        \"timestamp\": value.get('timestamp')\n      }\n    expression: result\n    resultType: json\n</code></pre> <p>Full example for <code>valueTransformer</code>:</p> <ul> <li>Tutorial: Filtering and Transforming   for understanding valueTransformer for data enrichment</li> </ul>"},{"location":"reference/function-reference/#functions-for-stateful-operations","title":"Functions for stateful operations","text":""},{"location":"reference/function-reference/#aggregator","title":"aggregator","text":"<p>Incrementally builds aggregated results from multiple messages.</p>"},{"location":"reference/function-reference/#parameters_7","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed aggregatedValue Any The current aggregated value (can be None)"},{"location":"reference/function-reference/#return-value_7","title":"Return Value","text":"<p>New aggregated value</p>"},{"location":"reference/function-reference/#example_6","title":"Example","text":"<pre><code>    type: aggregator\n    resultType: json\n    code: |\n      # Update payment statistics\n      if value and aggregatedValue:\n        amount = value.get(\"amount\", 0.0)\n        count = aggregatedValue.get(\"count\", 0) + 1\n        total_amount = aggregatedValue.get(\"total_amount\", 0.0) + amount\n\n        current_min = aggregatedValue.get(\"min_amount\")\n        current_max = aggregatedValue.get(\"max_amount\")\n\n        if current_min is None or amount &lt; current_min:\n          min_amount = amount\n        else:\n          min_amount = current_min\n\n        if current_max is None or amount &gt; current_max:\n          max_amount = amount\n        else:\n</code></pre> <p>Full example for <code>aggregator</code>:</p> <ul> <li>Tutorial: Aggregations for comprehensive aggregator   function examples</li> </ul>"},{"location":"reference/function-reference/#initializer","title":"initializer","text":"<p>Provides initial values for aggregations.</p>"},{"location":"reference/function-reference/#parameters_8","title":"Parameters","text":"<p>None</p>"},{"location":"reference/function-reference/#return-value_8","title":"Return Value","text":"<p>Initial value for aggregation</p>"},{"location":"reference/function-reference/#example_7","title":"Example","text":"<pre><code>  init_stats:\n    type: initializer\n    resultType: json\n    code: |\n      # Initialize statistics\n      return {\n        \"count\": 0,\n        \"total_amount\": 0.0,\n        \"min_amount\": None,\n        \"max_amount\": None\n      }\n</code></pre> <p>Full example for <code>initializer</code>:</p> <ul> <li>Example for <code>initializer</code></li> </ul>"},{"location":"reference/function-reference/#merger","title":"merger","text":"<p>Merges two aggregation results into one. Used in aggregation operations to combine partial results.</p>"},{"location":"reference/function-reference/#parameters_9","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value1 Any The value of the first aggregation value2 Any The value of the second aggregation"},{"location":"reference/function-reference/#return-value_9","title":"Return Value","text":"<p>The merged aggregation result</p>"},{"location":"reference/function-reference/#example_8","title":"Example","text":"<pre><code>  merge_counts:\n    type: merger\n    code: |\n      # This function is called when two session windows need to be merged\n      # For example, when a late-arriving event connects two previously separate sessions\n\n      count1 = value1 or 0\n      count2 = value2 or 0\n      merged_total = count1 + count2\n\n      log.info(\"Merging sessions: {} + {} = {}\", count1, count2, merged_total)\n      return merged_total\n    resultType: int\n</code></pre> Producer - <code>merger</code> example (click to expand) <pre><code>functions:\n  generate_user_activity:\n    type: generator\n    globalCode: |\n      import random, time\n      counter = 0\n      last_act = {}\n      base_t = int(time.time() * 1000)\n    code: |\n      global counter, last_act, base_t\n      uid = random.choice([\"alice\", \"bob\", \"charlie\"])\n      t = base_t + (counter * 2000)\n      if uid in last_act and random.random() &lt; 0.15:\n        t = last_act[uid] + 660000  # 11min gap\n      last_act[uid] = t\n      counter += 1\n      return uid, {\n        \"user_id\": uid,\n        \"page\": random.choice([\"home\", \"products\", \"profile\"]),\n        \"timestamp\": t,\n        \"event_type\": \"page_view\"\n      }\n    resultType: (string, json)\n\nproducers:\n  activity_producer:\n    generator: generate_user_activity\n    interval: 2s\n    to:\n      topic: user_activity\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>merger</code> example (click to expand) <pre><code>streams:\n  user_activity_input:\n    topic: user_activity\n    keyType: string\n    valueType: json\n\nfunctions:\n  # Initialize session counter\n  init_counter:\n    type: initializer\n    expression: 0\n    resultType: int\n\n  # Count events in session\n  count_events:\n    type: aggregator\n    code: |\n      # Increment the counter for each event\n      return (aggregatedValue or 0) + 1\n    resultType: int\n\n  # Merge session counters when sessions are combined\n  merge_counts:\n    type: merger\n    code: |\n      # This function is called when two session windows need to be merged\n      # For example, when a late-arriving event connects two previously separate sessions\n\n      count1 = value1 or 0\n      count2 = value2 or 0\n      merged_total = count1 + count2\n\n      log.info(\"Merging sessions: {} + {} = {}\", count1, count2, merged_total)\n      return merged_total\n    resultType: int\n\npipelines:\n  session_counting:\n    from: user_activity_input\n    via:\n      - type: groupByKey\n      - type: windowBySession\n        inactivityGap: 10m  # Session ends after 10 minutes of inactivity\n        grace: 2m          # Allow 2 minutes for late events\n      - type: aggregate\n        initializer: init_counter\n        aggregator: count_events\n        merger: merge_counts  # Required for session windows - merges overlapping sessions\n        store:\n          type: session\n          retention: 1h\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"User {} session: {} events\", key, value)\n      # Convert windowed key to string for output serialization\n      - type: mapKey\n        mapper:\n          code: |\n            # Convert windowed key to readable string format\n            # The windowed key is a dictionary with 'key', 'start', 'end' fields\n            return f\"user_{key['key']}_window_{key['start']}_{key['end']}\"\n          resultType: string\n    to:\n      topic: session_counts\n      keyType: string  # Now using simple string keys after transformation\n      valueType: int\n</code></pre> <p>The merger function is specifically designed for session window aggregations where late-arriving events can merge previously separate sessions. This example demonstrates user activity tracking with session-based counting.</p> <p>What the example does:</p> <p>Simulates user activity tracking with session windows and merging:</p> <ul> <li>Groups events into 10-min inactivity sessions</li> <li>Counts events per user</li> <li>Merges sessions when late events connect them</li> <li>Producer simulates gaps to trigger merging</li> </ul> <p>Key Features:</p> <ul> <li>Automatic session windowing &amp; late data handling</li> <li>Type-safe merger (integers)</li> <li>Windowed key transformation for output</li> <li>Merge logic adds event counts</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see log messages like:</p> <ul> <li><code>\"Merging sessions: 3 + 2 = 5\"</code> - Shows the merger function combining session counts</li> <li><code>\"User alice session: 4 events\"</code> - Displays final session results after merging</li> <li>Session windows spanning different time periods for each user</li> </ul>"},{"location":"reference/function-reference/#reducer","title":"reducer","text":"<p>Combines two values into one.</p>"},{"location":"reference/function-reference/#parameters_10","title":"Parameters","text":"Parameter Type Description value1 Any The first value to combine value2 Any The second value to combine"},{"location":"reference/function-reference/#return-value_10","title":"Return Value","text":"<p>Combined value</p>"},{"location":"reference/function-reference/#example_9","title":"Example","text":"<pre><code>  sum_amounts:\n    type: reducer\n    code: |\n      # Sum two transaction amounts (in cents)\n      total = value1 + value2\n    expression: total\n    resultType: long\n</code></pre> <p>Full example for <code>reducer</code>:</p> <ul> <li>Example for <code>reducer</code> function</li> </ul>"},{"location":"reference/function-reference/#special-purpose-functions","title":"Special Purpose Functions","text":""},{"location":"reference/function-reference/#foreignkeyextractor","title":"foreignKeyExtractor","text":"<p>Extracts a key from a join table's record. Used during join operations to determine which records to join.</p>"},{"location":"reference/function-reference/#parameters_11","title":"Parameters","text":"Parameter Type Description value Any The value of the record to get a key from"},{"location":"reference/function-reference/#return-value_11","title":"Return Value","text":"<p>The key to look up in the table being joined with</p>"},{"location":"reference/function-reference/#example_10","title":"Example","text":"<pre><code>  extract_customer_id:\n    type: foreignKeyExtractor\n    code: |\n      # Extract the foreign key (customer_id) from the order value\n      # This key will be used to look up the customer in the customers table\n      if value is None:\n        return None\n\n      customer_id = value.get(\"customer_id\")\n      log.debug(\"Extracting customer_id: {} from order\", customer_id)\n      return customer_id\n    resultType: string\n</code></pre> Producer - <code>foreignKeyExtractor</code> example (click to expand) <pre><code>functions:\n  generate_orders:\n    type: generator\n    globalCode: |\n      import random, time\n      order_counter = 1\n    code: |\n      global order_counter\n      oid = f\"ord_{order_counter:03d}\"\n      order_counter += 1\n      return oid, {\n        \"order_id\": oid,\n        \"customer_id\": random.choice([\"c001\", \"c002\", \"c003\"]),\n        \"product\": random.choice([\"laptop\", \"phone\"]),\n        \"amount\": random.randint(100, 1000),\n        \"timestamp\": int(time.time() * 1000)\n      }\n    resultType: (string, json)\n\n  generate_customers:\n    type: generator\n    globalCode: |\n      import time, random\n      custs = [\n        {\"id\": \"c001\", \"name\": \"Alice\", \"tier\": \"gold\"},\n        {\"id\": \"c002\", \"name\": \"Bob\", \"tier\": \"silver\"},  \n        {\"id\": \"c003\", \"name\": \"Charlie\", \"tier\": \"bronze\"}\n      ]\n    code: |\n      global custs\n      c = random.choice(custs)\n      return c[\"id\"], {\n        \"customer_id\": c[\"id\"],\n        \"name\": c[\"name\"],\n        \"tier\": c[\"tier\"],\n        \"created_at\": int(time.time() * 1000)\n      }\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_orders\n    interval: 3s\n    to:\n      topic: orders\n      keyType: string\n      valueType: json\n\n  customer_producer:\n    generator: generate_customers\n    interval: 5s  # Create customers periodically\n    to:\n      topic: customers\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>foreignKeyExtractor</code> example (click to expand) <pre><code>streams:\n  orders_input:\n    topic: orders\n    keyType: string\n    valueType: json\n\ntables:\n  customers_table:\n    topic: customers\n    keyType: string\n    valueType: json\n\nfunctions:\n  # Extract customer_id from order value to join with customers table\n  extract_customer_id:\n    type: foreignKeyExtractor\n    code: |\n      # Extract the foreign key (customer_id) from the order value\n      # This key will be used to look up the customer in the customers table\n      if value is None:\n        return None\n\n      customer_id = value.get(\"customer_id\")\n      log.debug(\"Extracting customer_id: {} from order\", customer_id)\n      return customer_id\n    resultType: string\n\n  # Join order with customer data\n  join_order_customer:\n    type: valueJoiner\n    code: |\n      # value1 is the order, value2 is the customer\n      order = value1\n      customer = value2\n\n      if order is None:\n        return None\n\n      # Create enriched order with customer information\n      enriched_order = {\n        \"order_id\": order.get(\"order_id\"),\n        \"product\": order.get(\"product\"),\n        \"amount\": order.get(\"amount\"),\n        \"timestamp\": order.get(\"timestamp\"),\n        \"customer\": {\n          \"customer_id\": order.get(\"customer_id\"),\n          \"name\": customer.get(\"name\", \"Unknown\") if customer else \"Unknown\",\n          \"email\": customer.get(\"email\", \"Unknown\") if customer else \"Unknown\",\n          \"tier\": customer.get(\"tier\", \"Unknown\") if customer else \"Unknown\"\n        }\n      }\n\n      log.info(\"Joined order {} with customer {}\", \n               order.get(\"order_id\"), \n               customer.get(\"name\") if customer else \"Unknown\")\n\n      return enriched_order\n    resultType: json\n\npipelines:\n  enrich_orders:\n    from: orders_input\n    via:\n      # Join orders table with customers table using foreignKeyExtractor\n      - type: join\n        table: customers_table\n        foreignKeyExtractor: extract_customer_id  # Extracts customer_id from order\n        valueJoiner: join_order_customer\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Enriched order: {} for customer: {}\", \n                     value.get(\"order_id\"), \n                     value.get(\"customer\", {}).get(\"name\"))\n    to:\n      topic: enriched_orders\n      keyType: string  # Still keyed by order_id\n      valueType: json\n</code></pre> <p>The foreignKeyExtractor enables table joins where the join key is embedded within the record value rather than being the record key. This example demonstrates order enrichment by joining with customer data using a foreign key relationship.</p> <p>Example</p> <p>This example simulates an e-commerce system enriching orders with customer data:</p> <ul> <li>Orders keyed by <code>order_id</code>, referencing <code>customer_id</code></li> <li>Customer details looked up by <code>customer_id</code></li> <li>Foreign key extracted from orders</li> <li>Orders joined with customers to produce enriched records</li> </ul> <p>Key Features:</p> <ul> <li>Foreign key join pattern</li> <li>KSML table join with key extraction</li> <li>Data enrichment from multiple sources</li> <li>Preserves original order keys</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see log messages like:</p> <ul> <li><code>\"Joined order order_001 with customer Alice Johnson\"</code> - Shows successful order-customer joins</li> <li><code>\"Enriched order: order_003 for customer: Bob Smith\"</code> - Displays enriched results with customer names</li> <li>Orders enriched with customer tier, email, and name information</li> </ul> <p>Use Cases:</p> <p>This pattern is commonly used for:</p> <ul> <li>Order enrichment with customer details</li> <li>Transaction enrichment with account information</li> <li>Event enrichment with user profiles</li> <li>Any scenario where records contain foreign key references</li> </ul>"},{"location":"reference/function-reference/#generator","title":"generator","text":"<p>Function used in producers to generate messages. It takes no input parameters and produces key-value pairs.</p> <p>For a comprehensive guide on using generator functions in producer definitions, see the Producer Tutorial.</p>"},{"location":"reference/function-reference/#parameters_12","title":"Parameters","text":"<p>None</p>"},{"location":"reference/function-reference/#return-value_12","title":"Return Value","text":"<ul> <li>A tuple <code>(key, value)</code> for a single message, or</li> <li>A list of tuples <code>[(key1, value1), (key2, value2), ...]</code> for multiple messages (batch generation)</li> </ul>"},{"location":"reference/function-reference/#example-single-message","title":"Example: Single Message","text":"<pre><code>  generate_tutorial_data:\n    type: generator\n    globalCode: |\n      import random\n      sensor_id = 0\n      locations = [\"server_room\", \"warehouse\", \"data_center\"]\n    code: |\n      global sensor_id, locations\n      key = \"sensor\" + str(sensor_id)\n      sensor_id = (sensor_id + 1) % 5\n      location = random.choice(locations)\n      sensors = {\"temperature\": random.randrange(150), \"humidity\": random.randrange(90), \"location\": location}\n      value = {\"sensors\": sensors}\n    expression: (key, value)\n    resultType: (string, json)\n</code></pre> <p>This generator returns a single tuple <code>(key, value)</code> each time it's called.</p> <ul> <li>Full Example: Generating JSON data</li> </ul>"},{"location":"reference/function-reference/#example-batch-generation-list-of-tuples","title":"Example: Batch Generation (List of Tuples)","text":"<pre><code>  generate_sensordata_batch:\n    type: generator\n    globalCode: |\n      import time\n      import random\n    code: |\n      result = []\n\n      # Generate 10 new messages in a single call\n      for sensorCounter in range(10):\n        key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n\n        # Generate some random sensor measurement data\n        types = { 0: { \"type\": \"AREA\", \"unit\": random.choice([ \"m2\", \"ft2\" ]), \"value\": str(random.randrange(1000)) },\n                  1: { \"type\": \"HUMIDITY\", \"unit\": random.choice([ \"g/m3\", \"%\" ]), \"value\": str(random.randrange(100)) },\n                  2: { \"type\": \"LENGTH\", \"unit\": random.choice([ \"m\", \"ft\" ]), \"value\": str(random.randrange(1000)) },\n                  3: { \"type\": \"STATE\", \"unit\": \"state\", \"value\": random.choice([ \"off\", \"on\" ]) },\n                  4: { \"type\": \"TEMPERATURE\", \"unit\": random.choice([ \"C\", \"F\" ]), \"value\": str(random.randrange(-100, 100)) }\n                }\n\n        # Build the result value using any of the above measurement types\n        value = { \"name\": key, \"timestamp\": str(round(time.time()*1000)), **random.choice(types) }\n        value[\"color\"] = random.choice([ \"black\", \"blue\", \"red\", \"yellow\", \"white\" ])\n        value[\"owner\"] = random.choice([ \"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\" ])\n        value[\"city\"] = random.choice([ \"Amsterdam\", \"Xanten\", \"Utrecht\", \"Alkmaar\", \"Leiden\" ])\n\n        if random.randrange(10) == 0:\n          key = None\n        if random.randrange(10) == 0:\n          value = None\n        result = result + [(key,value)]\n    expression: result                            # Return a list of messages\n</code></pre> <p>This generator returns a list of tuples - generating 10 messages in a single call. This is useful for:</p> <ul> <li>High-throughput scenarios - Generate multiple messages efficiently</li> <li>Related data sets - Produce correlated messages together</li> <li>Batch processing - Reduce overhead by generating messages in bulk</li> </ul> Full Batch Generator Example (click to expand) <pre><code># $schema: https://axual.github.io/ksml/latest/ksml-language-spec.json\n\n# This example shows how to generate multiple messages (batch) from a single generator call\n\nfunctions:\n  generate_sensordata_batch:\n    type: generator\n    globalCode: |\n      import time\n      import random\n    code: |\n      result = []\n\n      # Generate 10 new messages in a single call\n      for sensorCounter in range(10):\n        key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n\n        # Generate some random sensor measurement data\n        types = { 0: { \"type\": \"AREA\", \"unit\": random.choice([ \"m2\", \"ft2\" ]), \"value\": str(random.randrange(1000)) },\n                  1: { \"type\": \"HUMIDITY\", \"unit\": random.choice([ \"g/m3\", \"%\" ]), \"value\": str(random.randrange(100)) },\n                  2: { \"type\": \"LENGTH\", \"unit\": random.choice([ \"m\", \"ft\" ]), \"value\": str(random.randrange(1000)) },\n                  3: { \"type\": \"STATE\", \"unit\": \"state\", \"value\": random.choice([ \"off\", \"on\" ]) },\n                  4: { \"type\": \"TEMPERATURE\", \"unit\": random.choice([ \"C\", \"F\" ]), \"value\": str(random.randrange(-100, 100)) }\n                }\n\n        # Build the result value using any of the above measurement types\n        value = { \"name\": key, \"timestamp\": str(round(time.time()*1000)), **random.choice(types) }\n        value[\"color\"] = random.choice([ \"black\", \"blue\", \"red\", \"yellow\", \"white\" ])\n        value[\"owner\"] = random.choice([ \"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\" ])\n        value[\"city\"] = random.choice([ \"Amsterdam\", \"Xanten\", \"Utrecht\", \"Alkmaar\", \"Leiden\" ])\n\n        if random.randrange(10) == 0:\n          key = None\n        if random.randrange(10) == 0:\n          value = None\n        result = result + [(key,value)]\n    expression: result                            # Return a list of messages\n    resultType: list(tuple(string, json))         # The result type is a list of key/value tuples\n\nproducers:\n  # Produce batch of sensor data messages every 3 seconds\n  sensordata_batch_producer:\n    generator: generate_sensordata_batch\n    interval: 3s\n    to:\n      topic: sensor_data\n      keyType: string\n      valueType: json\n</code></pre>"},{"location":"reference/function-reference/#keyvaluemapper","title":"keyValueMapper","text":"<p>Transforms both the key and value of a record.</p>"},{"location":"reference/function-reference/#parameters_13","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_13","title":"Return Value","text":"<p>Tuple of (new_key, new_value)</p>"},{"location":"reference/function-reference/#example_11","title":"Example","text":"<pre><code>  extract_product_id:\n    type: keyValueMapper\n    code: |\n      # Map from order (key, value) to product_id for join\n      product_id = value.get(\"product_id\") if value else None\n    expression: product_id\n    resultType: string\n</code></pre> <p>Full example for <code>keyValueMapper</code>:</p> <ul> <li>Example: Product Catalog Enrichment</li> </ul>"},{"location":"reference/function-reference/#keyvalueprinter","title":"keyValuePrinter","text":"<p>Converts a message to a string for output to a file or stdout.</p>"},{"location":"reference/function-reference/#parameters_14","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed"},{"location":"reference/function-reference/#return-value_14","title":"Return Value","text":"<p>String to be written to file or stdout</p>"},{"location":"reference/function-reference/#example_12","title":"Example","text":"<p>The keyValuePrinter formats records for human-readable output to stdout or files. This example shows converting sales data into formatted reports for monitoring and debugging.</p> Producer - <code>keyValuePrinter</code> example (click to expand) <pre><code>functions:\n  generate_sales_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 1\n      products = [\"laptop\", \"mouse\", \"keyboard\", \"monitor\", \"webcam\"]\n      customers = [\"alice\", \"bob\", \"charlie\", \"diana\"]\n\n    code: |\n      global counter, products, customers\n\n      sale_id = f\"sale_{counter:03d}\"\n      counter += 1\n\n      sale_data = {\n        \"sale_id\": sale_id,\n        \"product\": random.choice(products),\n        \"customer\": random.choice(customers),\n        \"amount\": round(random.uniform(19.99, 299.99), 2),\n        \"quantity\": random.randint(1, 3),\n        \"region\": random.choice([\"north\", \"south\", \"east\", \"west\"])\n      }\n\n    expression: (sale_id, sale_data)\n    resultType: (string, json)\n\nproducers:\n  sales_producer:\n    generator: generate_sales_data\n    interval: 3s\n    to:\n      topic: sales_data\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>keyValuePrinter</code> example (click to expand) <pre><code>streams:\n  sales_input:\n    topic: sales_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  format_sales_report:\n    type: keyValuePrinter\n    code: |\n      if value is None:\n        return f\"ERROR: Sale {key} has no data\"\n\n      # Extract sale information\n      product = value.get(\"product\", \"Unknown\")\n      customer = value.get(\"customer\", \"Unknown\")\n      amount = value.get(\"amount\", 0)\n      quantity = value.get(\"quantity\", 0)\n      region = value.get(\"region\", \"Unknown\")\n\n      # Create formatted sales report\n      return f\"SALE REPORT | ID: {key} | Customer: {customer} | Product: {product} | Qty: {quantity} | Amount: ${amount:.2f} | Region: {region}\"\n    resultType: string\n\npipelines:\n  print_sales:\n    from: sales_input\n    print:\n      mapper: format_sales_report\n</code></pre> <p>What the example does:</p> <p>Demonstrates formatted business reporting with KSML:</p> <ul> <li>Sales Data Processing: Converts raw sales records into reports</li> <li>Custom Formatting: Transforms JSON into readable output</li> <li>Print Operation: Outputs formatted data via <code>print</code></li> <li>Real-time Monitoring: Enables instant visibility of transactions</li> </ul> <p>Key Features:</p> <ul> <li>Python string formatting</li> <li>Null/error handling</li> <li><code>keyValuePrinter</code> function</li> <li>Field extraction from JSON</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see formatted output like:</p> <ul> <li><code>SALE REPORT | ID: sale_001 | Customer: alice | Product: laptop | Qty: 2 | Amount: $1299.99 | Region: north</code></li> <li><code>SALE REPORT | ID: sale_002 | Customer: bob | Product: mouse | Qty: 1 | Amount: $29.99 | Region: south</code></li> </ul>"},{"location":"reference/function-reference/#metadatatransformer","title":"metadataTransformer","text":"<p>Transforms a message's metadata (headers and timestamp).</p>"},{"location":"reference/function-reference/#parameters_15","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed metadata Object Contains the headers and timestamp of the message"},{"location":"reference/function-reference/#return-value_15","title":"Return Value","text":"<p>Modified metadata for the output message</p>"},{"location":"reference/function-reference/#example_13","title":"Example","text":"<pre><code>  enrich_event_metadata:\n    type: metadataTransformer\n    code: |\n      import time\n\n      # Get processing timestamp\n      process_time = int(time.time() * 1000)\n\n      # Determine event severity based on status code\n      status_code = value.get(\"status_code\", 200) if value else 200\n      severity = \"critical\" if status_code &gt;= 500 else \"warning\" if status_code &gt;= 400 else \"info\"\n\n      # Add processing headers\n      new_headers = [\n        {\"key\": \"processed_timestamp\", \"value\": str(process_time)},\n        {\"key\": \"event_severity\", \"value\": severity},\n        {\"key\": \"processor_id\", \"value\": \"ksml-metadata-enricher\"}\n      ]\n\n      # Preserve existing headers and add new ones\n      existing_headers = metadata.get(\"headers\", [])\n      metadata[\"headers\"] = existing_headers + new_headers\n\n      log.info(\"Enriched event {} with {} additional headers\", key, len(new_headers))\n      return metadata\n</code></pre> Producer - <code>metadataTransformer</code> example (click to expand) <pre><code>functions:\n  generate_api_events:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      endpoints = [\"/api/users\", \"/api/orders\", \"/api/products\", \"/api/health\"]\n      methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n\n    code: |\n      global counter, endpoints, methods\n\n      # Generate event ID as key\n      event_id = f\"evt_{counter:04d}\"\n      counter += 1\n\n      # Generate API event data\n      event_data = {\n        \"event_id\": event_id,\n        \"endpoint\": random.choice(endpoints),\n        \"method\": random.choice(methods),\n        \"user_id\": f\"user_{random.randint(1, 50):03d}\",\n        \"status_code\": random.choices([200, 201, 400, 404, 500], weights=[60, 15, 15, 5, 5])[0],\n        \"response_time_ms\": random.randint(50, 1500),\n        \"timestamp\": \"auto\"\n      }\n\n    expression: (event_id, event_data)\n    resultType: (string, json)\n\nproducers:\n  api_event_producer:\n    generator: generate_api_events\n    interval: 2s\n    to:\n      topic: api_events\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>metadataTransformer</code> example (click to expand) <pre><code>streams:\n  api_events_input:\n    topic: api_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  enrich_event_metadata:\n    type: metadataTransformer\n    code: |\n      import time\n\n      # Get processing timestamp\n      process_time = int(time.time() * 1000)\n\n      # Determine event severity based on status code\n      status_code = value.get(\"status_code\", 200) if value else 200\n      severity = \"critical\" if status_code &gt;= 500 else \"warning\" if status_code &gt;= 400 else \"info\"\n\n      # Add processing headers\n      new_headers = [\n        {\"key\": \"processed_timestamp\", \"value\": str(process_time)},\n        {\"key\": \"event_severity\", \"value\": severity},\n        {\"key\": \"processor_id\", \"value\": \"ksml-metadata-enricher\"}\n      ]\n\n      # Preserve existing headers and add new ones\n      existing_headers = metadata.get(\"headers\", [])\n      metadata[\"headers\"] = existing_headers + new_headers\n\n      log.info(\"Enriched event {} with {} additional headers\", key, len(new_headers))\n      return metadata\n\npipelines:\n  enrich_api_events:\n    from: api_events_input\n    via:\n      - type: transformMetadata\n        mapper: enrich_event_metadata\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"API Event: {} {} - Status: {} - Response Time: {}ms\", \n                     value.get(\"method\"), value.get(\"endpoint\"),\n                     value.get(\"status_code\"), value.get(\"response_time_ms\"))\n    print:\n      mapper:\n        code: |\n          method = value.get('method', 'UNKNOWN')\n          endpoint = value.get('endpoint', '/unknown')\n          status = value.get('status_code', 0)\n          return f\"ENRICHED EVENT | {key} | {method} {endpoint} | Status: {status} | Headers processed\"\n        resultType: string\n</code></pre> <p>This example:</p> <ul> <li> <p>Shows metadata enrichment in stream processing.</p> </li> <li> <p>Generates realistic API events (endpoints + status codes)</p> </li> <li>Enriches headers with timestamps, severity, processor ID</li> <li>Classifies events (critical/warning/info) by status code</li> <li>Logs enrichment for monitoring/debugging</li> <li>Uses Python <code>time</code> for timestamps</li> <li>Has conditional + extensible headers</li> <li>Uses <code>transformMetadata</code> with <code>mapper</code> param</li> <li>Output is formatted via <code>print</code></li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see enriched events with additional headers:</p> <ul> <li><code>ENRICHED EVENT | evt_0001 | POST /api/users | Status: 200 | Headers processed</code></li> <li><code>ENRICHED EVENT | evt_0002 | DELETE /api/health | Status: 400 | Headers processed</code></li> <li>Log messages showing: \"Enriched event evt_0001 with 3 additional headers\"</li> </ul>"},{"location":"reference/function-reference/#streampartitioner","title":"streamPartitioner","text":"<p>Determines which partition a record should be sent to when writing to a Kafka topic. This allows custom partitioning logic based on record content, ensuring related records go to the same partition for ordered processing.</p>"},{"location":"reference/function-reference/#parameters_16","title":"Parameters","text":"Parameter Type Description topic String The name of the topic the record is being sent to key Any The key of the record being partitioned value Any The value of the record being partitioned numPartitions Integer The total number of partitions available in the target topic"},{"location":"reference/function-reference/#return-value_16","title":"Return Value","text":"<p>An integer representing the partition number (0 to numPartitions-1) where the record should be sent</p>"},{"location":"reference/function-reference/#example_14","title":"Example","text":"<p>Note:</p> <p>To test this streamPartitioner example, ensure your topics have sufficient partitions. The example requires minimum 9 partitions since it routes to partitions 0-8. Update your docker-compose.yml:</p> <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 9 --replication-factor 1 --topic order_events\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 9 --replication-factor 1 --topic partitioned_orders\n</code></pre> <pre><code>functions:\n  # StreamPartitioner function determines which partition to send each record to\n  # Returns an integer representing the target partition number\n  priority_region_partitioner:\n    type: streamPartitioner\n    code: |\n      # Custom partitioning logic based on priority and region\n      # This ensures orders with same priority+region go to same partition\n      # for ordered processing and improved locality\n\n      if value:\n        priority = value.get(\"priority\", \"standard\")\n        region = value.get(\"region\", \"UNKNOWN\")\n\n        # Map priority to a base partition range\n        priority_map = {\n          \"express\": 0,    # Express gets partitions 0-2\n          \"standard\": 3,   # Standard gets partitions 3-5\n          \"economy\": 6     # Economy gets partitions 6-8\n        }\n\n        # Map region to offset within priority range\n        region_map = {\n          \"NORTH\": 0,\n          \"SOUTH\": 1,\n          \"EAST\": 2,\n          \"WEST\": 0,\n          \"CENTRAL\": 1\n        }\n\n        base_partition = priority_map.get(priority, 3)\n        region_offset = region_map.get(region, 0)\n\n        # Calculate target partition (assuming 9 partitions total)\n        partition = (base_partition + region_offset) % 9\n\n        log.debug(\"Routing order {} to partition {}: priority={}, region={}\", \n                  value.get(\"order_id\"), partition, priority, region)\n\n        return partition\n\n      # Default to partition 0 if no value\n      return 0\n</code></pre> Producer - <code>streamPartitioner</code> example (click to expand) <pre><code>functions:\n  generate_order_event:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      order_id = 0\n      regions = [\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\", \"CENTRAL\"]\n      priorities = [\"express\", \"standard\", \"economy\"]\n    code: |\n      global order_id\n\n      order_id += 1\n\n      # Generate order details with priority and region\n      order = {\n        \"order_id\": f\"ORD-{order_id:04d}\",\n        \"amount\": round(random.uniform(10.0, 1000.0), 2),\n        \"priority\": random.choice(priorities),\n        \"region\": random.choice(regions),\n        \"customer_id\": f\"CUST-{random.randint(1000, 9999)}\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Log the order generation\n      log.info(\"Generated order: {} with priority={}, region={}\", \n               order[\"order_id\"], order[\"priority\"], order[\"region\"])\n\n      # Use order_id as key, order details as value\n      key = order[\"order_id\"]\n      value = order\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  order_events_producer:\n    generator: generate_order_event\n    interval: 2s\n    to:\n      topic: order_events\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>streamPartitioner</code> example (click to expand) <pre><code>streams:\n  order_events:\n    topic: order_events\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  partitioned_orders:\n    topic: partitioned_orders\n    keyType: string\n    valueType: json\n\nfunctions:\n  # StreamPartitioner function determines which partition to send each record to\n  # Returns an integer representing the target partition number\n  priority_region_partitioner:\n    type: streamPartitioner\n    code: |\n      # Custom partitioning logic based on priority and region\n      # This ensures orders with same priority+region go to same partition\n      # for ordered processing and improved locality\n\n      if value:\n        priority = value.get(\"priority\", \"standard\")\n        region = value.get(\"region\", \"UNKNOWN\")\n\n        # Map priority to a base partition range\n        priority_map = {\n          \"express\": 0,    # Express gets partitions 0-2\n          \"standard\": 3,   # Standard gets partitions 3-5\n          \"economy\": 6     # Economy gets partitions 6-8\n        }\n\n        # Map region to offset within priority range\n        region_map = {\n          \"NORTH\": 0,\n          \"SOUTH\": 1,\n          \"EAST\": 2,\n          \"WEST\": 0,\n          \"CENTRAL\": 1\n        }\n\n        base_partition = priority_map.get(priority, 3)\n        region_offset = region_map.get(region, 0)\n\n        # Calculate target partition (assuming 9 partitions total)\n        partition = (base_partition + region_offset) % 9\n\n        log.debug(\"Routing order {} to partition {}: priority={}, region={}\", \n                  value.get(\"order_id\"), partition, priority, region)\n\n        return partition\n\n      # Default to partition 0 if no value\n      return 0\n    resultType: integer\n\npipelines:\n  partition_orders:\n    from: order_events\n    via:\n      # Transform to add routing metadata\n      - type: transformValue\n        mapper:\n          code: |\n            # Add partition routing info to the order\n            if value:\n              # Calculate which partition this will go to\n              priority = value.get(\"priority\", \"standard\")\n              region = value.get(\"region\", \"UNKNOWN\")\n              value[\"routing_info\"] = f\"Priority: {priority}, Region: {region}\"\n            result = value\n          expression: result\n          resultType: json\n\n      # Peek to log the routing decision\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Processing order {}: {} -&gt; will be partitioned by priority/region\", \n                       key, value.get(\"routing_info\", \"unknown\"))\n\n    # Use the streamPartitioner function when writing to output topic\n    to:\n      topic: partitioned_orders\n      keyType: string\n      valueType: json\n      partitioner: priority_region_partitioner\n</code></pre> <p>The streamPartitioner function provides custom control over how records are distributed across topic partitions. This example demonstrates intelligent order routing based on business priorities and geographic regions.</p> <p>What the example does:</p> <p>Implements a sophisticated partitioning strategy for order processing:</p> <ul> <li>Routes orders to specific partitions based on priority (express/standard/economy)</li> <li>Further segments by geographic region within each priority tier</li> <li>Ensures orders with same priority+region go to same partition</li> <li>Producer generates realistic order events with various priorities/regions</li> </ul> <p>Key Features:</p> <ul> <li>Custom partition calculation based on multiple fields</li> <li>Guaranteed ordering for related records (same priority+region)</li> <li>Improved data locality and processing efficiency</li> <li>Explicit partition count handling (9 partitions total)</li> <li>Fallback to partition 0 for edge cases</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see log messages like:</p> <ul> <li><code>\"Generated order: ORD-0001 with priority=economy, region=CENTRAL\"</code> - Producer creating orders</li> <li><code>\"Processing order ORD-0001: Priority: economy, Region: CENTRAL -&gt; will be partitioned by priority/region\"</code> - Routing decision</li> <li>Express orders (priority=express) go to partitions 0-2</li> <li>Standard orders (priority=standard) go to partitions 3-5</li> <li>Economy orders (priority=economy) go to partitions 6-8</li> <li>Within each priority range, regions determine the exact partition</li> </ul>"},{"location":"reference/function-reference/#valuejoiner","title":"valueJoiner","text":"<p>Combines values from two streams or tables during join operations, creating enriched records that contain data from both sources. The function must handle cases where one or both values might be null, depending on the join type (inner, left, outer). This function has access to the join key for context-aware value combination.</p>"},{"location":"reference/function-reference/#parameters_17","title":"Parameters","text":"Parameter Type Description key Any The join key used to match records value1 Any The value from the first stream/table value2 Any The value from the second stream/table"},{"location":"reference/function-reference/#return-value_17","title":"Return Value","text":"<p>Combined value</p>"},{"location":"reference/function-reference/#example_15","title":"Example","text":"<pre><code>  join_order_with_product:\n    type: valueJoiner\n    code: |\n      # Combine order and product information\n      result = {}\n\n      # Add order details\n      if value1 is not None:\n        result.update(value1)\n\n      # Add product details\n      if value2 is not None:\n        result[\"product_details\"] = value2\n        # Calculate total price\n        quantity = value1.get(\"quantity\", 0) if value1 else 0\n        price = value2.get(\"price\", 0) if value2 else 0\n        result[\"total_price\"] = quantity * price\n\n      new_value = result\n    expression: new_value\n    resultType: json\n</code></pre> <p>Full example for <code>valueJoiner</code>:</p> <ul> <li>Tutorial: Joins for learning about stream enrichment</li> </ul>"},{"location":"reference/function-reference/#stream-related-functions","title":"Stream Related Functions","text":""},{"location":"reference/function-reference/#timestampextractor","title":"timestampExtractor","text":"<p>Extracts timestamps from messages for time-based operations.</p>"},{"location":"reference/function-reference/#parameters_18","title":"Parameters","text":"Parameter Type Description record Object The ConsumerRecord containing key, value, timestamp, and metadata previousTimestamp Long The previous timestamp (can be used as fallback)"},{"location":"reference/function-reference/#return-value_18","title":"Return Value","text":"<p>Timestamp in milliseconds (long)</p>"},{"location":"reference/function-reference/#example_16","title":"Example","text":"<pre><code>    timestampExtractor:\n      code: |\n        # Extract custom timestamp from message value\n        try:\n          # Try different ways to get the value data\n          value_data = None\n\n          # Method 1: If record is a ConsumerRecord with .value() method\n          if hasattr(record, 'value') and callable(record.value):\n            value_data = record.value()\n          # Method 2: If record is the value directly (dict)\n          elif isinstance(record, dict):\n            value_data = record\n\n          # Extract custom timestamp from event data\n          if value_data and \"event_timestamp\" in value_data:\n            event_time = value_data.get(\"event_timestamp\")\n            if event_time and event_time &gt; 0:\n              log.info(\"Using event timestamp: {} for {}\", event_time, value_data.get(\"event_id\"))\n              return event_time\n\n        except Exception as e:\n          log.warn(\"Error extracting custom timestamp: {}\", str(e))\n\n        # Fallback to record timestamp or current time\n        import time\n        current_time = int(time.time() * 1000)\n        log.info(\"Using current time as fallback: {}\", current_time)\n        return current_time\n</code></pre> Producer - <code>timestampExtractor</code> example (click to expand) <pre><code>functions:\n  generate_events_with_timestamps:\n    type: generator\n    globalCode: |\n      import random, time\n      counter = 0\n      base_t = int(time.time() * 1000)\n    code: |\n      global counter, base_t\n      counter += 1\n      eid = f\"e{counter:04d}\"\n      offset = random.randint(-300, 60)  # -5min to +1min\n      return eid, {\n        \"event_id\": eid,\n        \"event_timestamp\": base_t + (counter * 1000) + (offset * 1000),\n        \"event_type\": random.choice([\"action\", \"system\", \"data\"]),\n        \"user_id\": f\"u{random.randint(1, 100):03d}\",\n        \"delay\": abs(offset) if offset &lt; 0 else 0\n      }\n    resultType: (string, json)\n\nproducers:\n  event_producer:\n    generator: generate_events_with_timestamps\n    interval: 1s\n    to:\n      topic: timestamped_events\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>timestampExtractor</code> example (click to expand) <pre><code>streams:\n  events_input:\n    topic: timestamped_events\n    keyType: string\n    valueType: json\n    # Configure custom timestamp extraction\n    timestampExtractor:\n      code: |\n        # Extract custom timestamp from message value\n        try:\n          # Try different ways to get the value data\n          value_data = None\n\n          # Method 1: If record is a ConsumerRecord with .value() method\n          if hasattr(record, 'value') and callable(record.value):\n            value_data = record.value()\n          # Method 2: If record is the value directly (dict)\n          elif isinstance(record, dict):\n            value_data = record\n\n          # Extract custom timestamp from event data\n          if value_data and \"event_timestamp\" in value_data:\n            event_time = value_data.get(\"event_timestamp\")\n            if event_time and event_time &gt; 0:\n              log.info(\"Using event timestamp: {} for {}\", event_time, value_data.get(\"event_id\"))\n              return event_time\n\n        except Exception as e:\n          log.warn(\"Error extracting custom timestamp: {}\", str(e))\n\n        # Fallback to record timestamp or current time\n        import time\n        current_time = int(time.time() * 1000)\n        log.info(\"Using current time as fallback: {}\", current_time)\n        return current_time\n\n  ordered_events:\n    topic: time_ordered_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  log_timestamp_info:\n    type: forEach\n    code: |\n      event_time = value.get(\"event_timestamp\") if value else 0\n      delay = value.get(\"processing_delay\", 0) if value else 0\n\n      log.info(\"Event processed in time order: {} (event_time={}, delay={}s)\", \n               key, event_time, delay)\n\npipelines:\n  process_with_event_time:\n    from: events_input\n    via:\n      - type: peek\n        forEach: log_timestamp_info\n    to: ordered_events\n</code></pre> <p>What the example does:</p> <p>Demonstrates custom timestamp extraction for event-time processing:</p> <ul> <li>Creates events with custom timestamps that simulate out-of-order and delayed processing scenarios</li> <li>Extracts event-time timestamps from message content rather than using record timestamps</li> <li>Processes events based on their event time rather than arrival time</li> <li>Provides robust fallback mechanisms for missing or invalid timestamps</li> <li>Uses custom <code>timestampExtractor</code> function in stream definition</li> <li>Uses Python time manipulation and timestamp handling</li> <li>Support for both ConsumerRecord and direct value access patterns</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see events processed in time order:</p> <ul> <li><code>Event processed in time order: event_0001 (event_time=1755974335641, delay=155s)</code></li> <li><code>Event processed in time order: event_0002 (event_time=1755974539885, delay=41s)</code></li> <li>Log messages showing: \"Using event timestamp: 1755974601885 for event_0015\"</li> </ul>"},{"location":"reference/function-reference/#topicnameextractor","title":"topicNameExtractor","text":"<p>Dynamically determines the target topic for message routing based on record content. This enables intelligent message distribution, multi-tenancy support, and content-based routing patterns without requiring separate processing pipelines. The function has access to record context for advanced routing decisions.</p>"},{"location":"reference/function-reference/#parameters_19","title":"Parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed recordContext Object Record metadata and processing context"},{"location":"reference/function-reference/#return-value_19","title":"Return Value","text":"<p>String representing the topic name to send the message to</p>"},{"location":"reference/function-reference/#example_17","title":"Example","text":"<pre><code>  route_by_sensor_type:\n    type: topicNameExtractor\n    code: |\n      if value is None:\n        return \"unknown_sensor_data\"\n\n      sensor_type = value.get(\"sensor_type\", \"unknown\")\n      alert_level = value.get(\"alert_level\", \"normal\")\n\n      # Route critical alerts to dedicated topic regardless of sensor type\n      if alert_level == \"critical\":\n        log.warn(\"Critical alert from sensor {}: {} reading = {}\", \n                 value.get(\"sensor_id\"), sensor_type, value.get(\"reading\"))\n        return \"critical_sensor_alerts\"\n\n      # Route by sensor type for normal and warning levels\n      if sensor_type == \"temperature\":\n        return \"temperature_sensors\"\n      elif sensor_type == \"humidity\":\n        return \"humidity_sensors\" \n      elif sensor_type == \"pressure\":\n        return \"pressure_sensors\"\n      else:\n        return \"unknown_sensor_data\"\n</code></pre> Producer - <code>topicNameExtractor</code> example (click to expand) <pre><code>functions:\n  generate_sensor_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      sensor_types = [\"temperature\", \"humidity\", \"pressure\"]\n      locations = [\"factory_a\", \"factory_b\", \"warehouse\"]\n    code: |\n      global counter, sensor_types, locations\n\n      # Generate sensor ID as key\n      sensor_id = f\"sensor_{counter % 10:02d}\"\n      counter += 1\n\n      # Generate sensor reading\n      sensor_type = random.choice(sensor_types)\n\n      value = {\n        \"sensor_id\": sensor_id,\n        \"sensor_type\": sensor_type,\n        \"location\": random.choice(locations),\n        \"reading\": round(random.uniform(10.0, 100.0), 2),\n        \"timestamp\": counter * 1000,\n        \"alert_level\": random.choices([\"normal\", \"warning\", \"critical\"], [80, 15, 5])[0]\n      }\n\n    expression: (sensor_id, value)\n    resultType: (string, json)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_data\n    interval: 2s\n    to:\n      topic: mixed_sensor_data\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>topicNameExtractor</code> example (click to expand) <pre><code>streams:\n  mixed_sensors:\n    topic: mixed_sensor_data\n    keyType: string\n    valueType: json\n  temperature_data:\n    topic: temperature_sensors\n    keyType: string\n    valueType: json\n  humidity_data:\n    topic: humidity_sensors\n    keyType: string\n    valueType: json\n  pressure_data:\n    topic: pressure_sensors\n    keyType: string\n    valueType: json\n  critical_alerts:\n    topic: critical_sensor_alerts\n    keyType: string\n    valueType: json\n\nfunctions:\n  route_by_sensor_type:\n    type: topicNameExtractor\n    code: |\n      if value is None:\n        return \"unknown_sensor_data\"\n\n      sensor_type = value.get(\"sensor_type\", \"unknown\")\n      alert_level = value.get(\"alert_level\", \"normal\")\n\n      # Route critical alerts to dedicated topic regardless of sensor type\n      if alert_level == \"critical\":\n        log.warn(\"Critical alert from sensor {}: {} reading = {}\", \n                 value.get(\"sensor_id\"), sensor_type, value.get(\"reading\"))\n        return \"critical_sensor_alerts\"\n\n      # Route by sensor type for normal and warning levels\n      if sensor_type == \"temperature\":\n        return \"temperature_sensors\"\n      elif sensor_type == \"humidity\":\n        return \"humidity_sensors\" \n      elif sensor_type == \"pressure\":\n        return \"pressure_sensors\"\n      else:\n        return \"unknown_sensor_data\"\n\n  log_routing:\n    type: forEach\n    code: |\n      log.info(\"Sensor data: {} type={} level={} reading={}\", \n               key, value.get(\"sensor_type\"), \n               value.get(\"alert_level\"), value.get(\"reading\"))\n\npipelines:\n  route_sensor_data:\n    from: mixed_sensors\n    via:\n      - type: peek\n        forEach: log_routing\n    toTopicNameExtractor:\n      topicNameExtractor: route_by_sensor_type\n</code></pre> <p>What the example does:</p> <p>Demonstrates dynamic topic routing based on message content:</p> <ul> <li>Creates mixed sensor data (temperature, humidity, pressure) with varying alert levels</li> <li>Routes messages to different topics based on sensor type and priority</li> <li>Prioritizes critical alerts to a dedicated topic regardless of sensor type</li> <li>Distributes normal/warning messages to type-specific topics</li> </ul> <p>Key Technical Features:</p> <ul> <li><code>topicNameExtractor</code> function for dynamic topic selection</li> <li>Priority-based routing with alert level evaluation</li> <li>Fallback topic handling for unknown sensor types</li> <li><code>toTopicNameExtractor</code> operation instead of static <code>to</code> operation</li> <li>Integration with logging for routing visibility</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see messages routed to different topics:</p> <ul> <li>Critical alerts: <code>\"Critical alert from sensor sensor_05: pressure reading = 78.26\"</code> \u2192 <code>critical_sensor_alerts</code> topic</li> <li>Normal temperature readings \u2192 <code>temperature_sensors</code> topic</li> <li>Normal humidity readings \u2192 <code>humidity_sensors</code> topic</li> <li>Normal pressure readings \u2192 <code>pressure_sensors</code> topic</li> <li>Unknown sensor types \u2192 <code>unknown_sensor_data</code> topic</li> </ul>"},{"location":"reference/function-reference/#other-functions","title":"Other Functions","text":""},{"location":"reference/function-reference/#generic","title":"generic","text":"<p>Generic custom function that can be used for any purpose. It can accept custom parameters and return any type of value.</p>"},{"location":"reference/function-reference/#parameters_20","title":"Parameters","text":"<p>User-defined parameters</p>"},{"location":"reference/function-reference/#return-value_20","title":"Return Value","text":"<p>Any value, depending on the function's purpose</p>"},{"location":"reference/function-reference/#example_18","title":"Example","text":"<pre><code>  calculate_price:\n    type: generic\n    parameters:\n      - name: base_price\n        type: double\n      - name: discount_rate\n        type: double\n      - name: tax_rate\n        type: double\n    code: |\n      # Calculate discounted price\n      discount_amount = base_price * (discount_rate / 100)\n      discounted_price = base_price - discount_amount\n\n      # Calculate tax on discounted price  \n      tax_amount = discounted_price * (tax_rate / 100)\n      final_price = discounted_price + tax_amount\n\n      return {\n        \"original_price\": base_price,\n        \"discount_amount\": discount_amount,\n        \"discounted_price\": discounted_price,\n        \"tax_amount\": tax_amount,\n        \"final_price\": final_price,\n        \"total_savings\": discount_amount\n      }\n    resultType: json\n</code></pre> Producer - <code>generic</code> example (click to expand) <pre><code>functions:\n  generate_product_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      products = [\"laptop\", \"phone\", \"tablet\", \"headphones\"]\n      categories = [\"electronics\", \"accessories\", \"computing\"]\n    code: |\n      global counter, products, categories\n      product_id = f\"prod_{counter:03d}\"\n      counter += 1\n\n      product_data = {\n        \"product_id\": product_id,\n        \"name\": random.choice(products),\n        \"category\": random.choice(categories),\n        \"base_price\": round(random.uniform(50.0, 1000.0), 2),\n        \"discount_rate\": random.choice([0, 5, 10, 15, 20]),\n        \"quantity\": random.randint(1, 100)\n      }\n    expression: (product_id, product_data)\n    resultType: (string, json)\n\nproducers:\n  product_producer:\n    generator: generate_product_data\n    interval: 2s\n    to:\n      topic: raw_products\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>generic</code> example (click to expand) <pre><code>streams:\n  raw_products:\n    topic: raw_products\n    keyType: string\n    valueType: json\n\nfunctions:\n  calculate_price:\n    type: generic\n    parameters:\n      - name: base_price\n        type: double\n      - name: discount_rate\n        type: double\n      - name: tax_rate\n        type: double\n    code: |\n      # Calculate discounted price\n      discount_amount = base_price * (discount_rate / 100)\n      discounted_price = base_price - discount_amount\n\n      # Calculate tax on discounted price  \n      tax_amount = discounted_price * (tax_rate / 100)\n      final_price = discounted_price + tax_amount\n\n      return {\n        \"original_price\": base_price,\n        \"discount_amount\": discount_amount,\n        \"discounted_price\": discounted_price,\n        \"tax_amount\": tax_amount,\n        \"final_price\": final_price,\n        \"total_savings\": discount_amount\n      }\n    resultType: json\n\n  enrich_product:\n    type: valueTransformer\n    code: |\n      if value and \"base_price\" in value and \"discount_rate\" in value:\n        # Call our generic function with custom tax rate\n        price_info = calculate_price(\n          base_price=value[\"base_price\"],\n          discount_rate=value[\"discount_rate\"], \n          tax_rate=8.5\n        )\n\n        # Add calculated pricing to the product data\n        value[\"pricing\"] = price_info\n        value[\"processed_at\"] = \"2024-01-01T00:00:00Z\"\n\n      return value\n    resultType: json\n\npipelines:\n  process_products:\n    from: raw_products\n    via:\n      - type: transformValue\n        mapper: enrich_product\n      - type: peek\n        forEach:\n          code: |\n            original = value[\"pricing\"][\"original_price\"]\n            final = value[\"pricing\"][\"final_price\"]\n            savings = value[\"pricing\"][\"total_savings\"]\n            log.info(\"Processed product: {} - Original: ${:.2f}, Final: ${:.2f}, Saved: ${:.2f}\".format(\n                     key, original, final, savings))\n    to:\n      topic: enriched_products\n      keyType: string\n      valueType: json\n</code></pre> <p>What the example does:</p> <p>Demonstrates how to create reusable business logic with generic functions:</p> <ul> <li>Creates sample product data with base prices and discount rates</li> <li><code>calculate_price</code> accepts parameters to compute final pricing with tax</li> <li>The generic function is called from within a valueTransformer</li> <li>Adds calculated pricing information to product records</li> </ul> <p>Key Technical Features:</p> <ul> <li><code>type: generic</code> for reusable custom functions</li> <li>Custom parameter definitions with types (double, string, etc.)</li> <li>Return any data structure (JSON objects, arrays, primitives)</li> <li>Call generic functions from other functions using standard Python syntax</li> <li>Mix generic functions with standard KSML function types</li> </ul> <p>Expected Results:</p> <p>When running these examples, you will see:</p> <ul> <li>Product data being generated with random base prices and discount rates</li> <li>Log messages showing: \"Processed product: prod_001 - Original: $856.58, Final: $922.50, Saved: $128.49\"</li> <li>Each product enriched with detailed pricing calculations including tax</li> <li>Generic function providing consistent pricing logic across all products</li> </ul>"},{"location":"reference/function-reference/#how-ksml-functions-relate-to-kafka-streams","title":"How KSML Functions Relate to Kafka Streams","text":"<p>KSML functions are Python implementations that map directly to Kafka Streams Java interfaces. Understanding this relationship helps you leverage Kafka Streams documentation and concepts:</p>"},{"location":"reference/function-reference/#direct-mappings","title":"Direct Mappings","text":""},{"location":"reference/function-reference/#functions-for-stateless-operations_1","title":"Functions for stateless operations","text":"KSML Function Type Kafka Streams Interface Purpose forEach <code>ForeachAction&lt;K,V&gt;</code> Process records for side effects keyTransformer <code>KeyValueMapper&lt;K,V,KR&gt;</code> Transform keys keyValueTransformer <code>KeyValueMapper&lt;K,V,KeyValue&lt;KR,VR&gt;&gt;</code> Transform both key and value predicate <code>Predicate&lt;K,V&gt;</code> Filter records based on conditions valueTransformer <code>ValueTransformer&lt;V,VR&gt;</code> / <code>ValueMapper&lt;V,VR&gt;</code> Transform values"},{"location":"reference/function-reference/#functions-for-stateful-operations_1","title":"Functions for stateful operations","text":"KSML Function Type Kafka Streams Interface Purpose aggregator <code>Aggregator&lt;K,V,VA&gt;</code> Aggregate records incrementally initializer <code>Initializer&lt;VA&gt;</code> Provide initial aggregation values merger <code>Merger&lt;K,V&gt;</code> Merge aggregation results reducer <code>Reducer&lt;V&gt;</code> Combine values of same type"},{"location":"reference/function-reference/#special-purpose-functions_1","title":"Special Purpose Functions","text":"KSML Function Type Kafka Streams Interface Purpose foreignKeyExtractor <code>Function&lt;V,FK&gt;</code> Extract foreign key for joins keyValueMapper <code>KeyValueMapper&lt;K,V,VR&gt;</code> Convert key-value to single output valueJoiner <code>ValueJoiner&lt;V1,V2,VR&gt;</code> Join values from two streams"},{"location":"reference/function-reference/#stream-related-functions_1","title":"Stream Related Functions","text":"KSML Function Type Kafka Streams Interface Purpose streamPartitioner <code>StreamPartitioner&lt;K,V&gt;</code> Custom partition selection timestampExtractor <code>TimestampExtractor</code> Extract event time from records topicNameExtractor <code>TopicNameExtractor&lt;K,V&gt;</code> Dynamic topic routing"},{"location":"reference/function-reference/#function-execution-context","title":"Function Execution Context","text":"<p>When your Python functions execute, they have access to:</p> <ul> <li> <p>Logger: For outputting information to application logs</p> <ul> <li><code>log.&lt;log-level&gt;(\"Debug message\")</code> -  can be debug, info, warn, error, trace <li> <p>Metrics: For monitoring function performance and behavior</p> <ul> <li><code>metrics.counter(\"name\").increment()</code> - Count occurrences</li> <li><code>metrics.gauge(\"name\").record(value)</code> - Record values</li> <li><code>with metrics.timer(\"name\"):</code> - Measure execution time</li> </ul> </li> <li> <p>State Stores: For maintaining state between function invocations (when configured)</p> <ul> <li><code>store.get(key)</code> - Retrieve value from store</li> <li><code>store.put(key, value)</code> - Store a value</li> <li><code>store.delete(key)</code> - Remove a value</li> <li>Must be declared in the function's <code>stores</code> parameter</li> </ul> </li> <p>This execution context provides the tools needed for debugging, monitoring, and implementing stateful processing.</p>"},{"location":"reference/operation-reference/","title":"Operation Reference","text":"<p>This document provides a comprehensive reference for all operations available in KSML. Each operation is described with its parameters, behavior, and examples.</p>"},{"location":"reference/operation-reference/#introduction","title":"Introduction","text":"<p>Operations are the building blocks of stream processing in KSML. They define how data is transformed, filtered, aggregated, and otherwise processed as it flows through your application. Operations form the middle part of pipelines, taking input from the previous operation and producing output for the next operation.</p> <p>Understanding the different types of operations and when to use them is crucial for building effective stream processing applications.</p>"},{"location":"reference/operation-reference/#operations-overview","title":"Operations Overview","text":"<p>KSML supports 28 operations for stream processing. Each operation serves a specific purpose in transforming, filtering, aggregating, or routing data:</p> Operation Purpose Common Use Cases Stateless Transformation Operations flatMap Transform one record into multiple records Split batch messages, expand arrays map Transform both key and value Change message format, enrich data mapKey Transform only the key Change partitioning key mapValues Transform only the value (preserves key) Modify payload without affecting partitioning selectKey Select a new key from the value Extract key from message content transformKey Transform key using custom function Complex key transformations transformValue Transform value using custom function Complex value transformations Filtering Operations filter Keep records that match a condition Remove unwanted messages filterNot Remove records that match a condition Exclude specific messages Format Conversion Operations convertKey Convert key format (e.g., JSON to Avro) Change serialization format convertValue Convert value format (e.g., JSON to Avro) Change serialization format Grouping &amp; Partitioning Operations groupBy Group by a new key Prepare for aggregation with new key groupByKey Group by existing key Prepare for aggregation repartition Redistribute records across partitions Custom partitioning logic Stateful Aggregation Operations aggregate Build custom aggregations Complex calculations, custom state count Count records per key Track occurrences reduce Combine records with same key Accumulate values Join Operations join Inner join two streams Correlate related events leftJoin Left outer join two streams Include all left records merge Combine multiple streams into one Stream unification outerJoin Full outer join two streams Include all records from both sides Windowing Operations windowBySession Group into session windows User session analysis windowByTime Group into fixed time windows Time-based aggregations Output Operations forEach Process without producing output Side effects, external calls print Print to console Debugging, monitoring to Send to a specific topic Write results to Kafka toTopicNameExtractor Send to dynamically determined topic Route to different topics Control Flow Operations branch Split stream into multiple branches Conditional routing peek Observe records without modification Logging, debugging"},{"location":"reference/operation-reference/#choosing-the-right-operation","title":"Choosing the Right Operation","text":"<p>When designing your KSML application, consider these factors:</p> <ul> <li>State Requirements: Stateful operations (aggregations, joins) require state stores and more resources</li> <li>Partitioning: Operations like <code>groupBy</code> may trigger data redistribution</li> <li>Performance: Some operations are more computationally expensive than others</li> <li>Error Handling: Use <code>try</code> operations to handle potential failures gracefully</li> </ul>"},{"location":"reference/operation-reference/#stateless-transformation-operations","title":"Stateless Transformation Operations","text":"<p>Stateless transformation operations modify records (key, value, or both) without maintaining state between records. These operations are the most common building blocks for data processing pipelines.</p>"},{"location":"reference/operation-reference/#map","title":"<code>map</code>","text":"<p>Transforms both the key and value of each record.</p>"},{"location":"reference/operation-reference/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> Object Yes Specifies how to transform the key and value <p>The <code>mapper</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression returning a tuple (key, value)</li> <li><code>code</code>: A Python code block returning a tuple (key, value)</li> </ul>"},{"location":"reference/operation-reference/#example","title":"Example","text":"<pre><code>      - type: map\n        mapper: extract_fields\n</code></pre> <p>Full example for <code>map</code>:</p> <ul> <li>Tutorial: Filtering and Transforming</li> </ul>"},{"location":"reference/operation-reference/#mapvalues","title":"<code>mapValues</code>","text":"<p>Transforms the value of each record without changing the key.</p>"},{"location":"reference/operation-reference/#parameters_1","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> Object Yes Specifies how to transform the value <p>The <code>mapper</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression</li> <li><code>code</code>: A Python code block</li> </ul>"},{"location":"reference/operation-reference/#example_1","title":"Example","text":"<pre><code>          - type: mapValues\n            mapper: add_priority_processing\n</code></pre> <p>Full example for <code>mapValues</code>:</p> <ul> <li>Tutorial: Branching</li> </ul>"},{"location":"reference/operation-reference/#mapkey","title":"<code>mapKey</code>","text":"<p>Transforms the key of each record without modifying the value.</p>"},{"location":"reference/operation-reference/#parameters_2","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> Object Yes Specifies how to transform the key <p>The <code>mapper</code> can be defined using: - <code>expression</code>: A simple expression returning the new key - <code>code</code>: A Python code block returning the new key</p>"},{"location":"reference/operation-reference/#example_2","title":"Example","text":"<pre><code>      - type: mapKey\n        mapper: extract_region_key\n</code></pre> <p>Full example for <code>mapKey</code>:</p> <ul> <li>Function Reference: keyTransformer - Shows mapKey used with keyTransformer function</li> </ul>"},{"location":"reference/operation-reference/#flatmap","title":"<code>flatMap</code>","text":"<p>Transforms each record into zero or more records, useful for splitting batch messages into individual records.</p>"},{"location":"reference/operation-reference/#parameters_3","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> Object Yes Specifies how to transform each record into multiple records <p>The <code>mapper</code> must specify:</p> <ul> <li><code>resultType</code>: Format <code>\"[(keyType,valueType)]\"</code> indicating list of tuples</li> <li><code>code</code>: Python code returning a list of tuples <code>[(key, value), ...]</code></li> </ul>"},{"location":"reference/operation-reference/#example_3","title":"Example","text":"<pre><code>      - type: flatMap\n        mapper:\n          resultType: list(tuple(string, json))\n          code: |\n            return [(f\"{value['order_id']}_{i['item_id']}\", {\n              \"order_id\": value['order_id'],\n              \"customer_id\": value['customer_id'],\n              \"item_id\": i['item_id'],\n              \"quantity\": i['quantity'],\n              \"total\": i['quantity'] * i['price']\n            }) for i in value['items']]\n</code></pre> <p>This example splits order batches containing multiple items into individual item records:</p> Producer - <code>flatMap</code> example (click to expand) <pre><code>functions:\n  generate_order_batch:\n    type: generator\n    globalCode: |\n      import json, random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n      cust = random.choice([\"alice\", \"bob\", \"charlie\"])\n      batch = {\n        \"order_id\": f\"ord-{counter}\",\n        \"customer_id\": cust,\n        \"items\": [\n          {\"item_id\": \"laptop\", \"quantity\": 1, \"price\": 999.99},\n          {\"item_id\": \"mouse\", \"quantity\": 2, \"price\": 29.99},\n          {\"item_id\": \"keyboard\", \"quantity\": 1, \"price\": 79.99}\n        ]\n      }\n      return cust, json.dumps(batch)\n    resultType: (string, json)\n\nproducers:\n  order_batch_producer:\n    generator: generate_order_batch\n    interval: 3s\n    to:\n      topic: order_batches\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>flatMap</code> example (click to expand) <pre><code>streams:\n  order_batches_input:\n    topic: order_batches\n    keyType: string\n    valueType: json\n\npipelines:\n  main:\n    from: order_batches_input\n    via:\n      - type: flatMap\n        mapper:\n          resultType: list(tuple(string, json))\n          code: |\n            return [(f\"{value['order_id']}_{i['item_id']}\", {\n              \"order_id\": value['order_id'],\n              \"customer_id\": value['customer_id'],\n              \"item_id\": i['item_id'],\n              \"quantity\": i['quantity'],\n              \"total\": i['quantity'] * i['price']\n            }) for i in value['items']]\n    to:\n      topic: individual_items\n      keyType: string\n      valueType: json\n</code></pre> <p>What this example does: </p> <ul> <li>The producer generates order batches containing multiple items.</li> <li>The processor uses <code>flatMap</code> to split each order batch into individual item records - transforming 1 input record into 3 output records (one per item).</li> <li>Each output record has a unique key combining order ID and item ID, with calculated total prices per item.</li> </ul>"},{"location":"reference/operation-reference/#selectkey","title":"<code>selectKey</code>","text":"<p>Changes the key of each record without modifying the value. This operation extracts a new key from the existing key and/or value, enabling data repartitioning and preparation for joins based on different key attributes.</p>"},{"location":"reference/operation-reference/#parameters_4","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> Object Yes Specifies how to derive the new key from the key/value <p>The <code>mapper</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression returning the new key (can use both <code>key</code> and <code>value</code>)</li> <li><code>code</code>: A Python code block returning the new key (can use both <code>key</code> and <code>value</code>)</li> </ul>"},{"location":"reference/operation-reference/#example_4","title":"Example","text":"<pre><code>      - type: selectKey\n        mapper:\n          expression: value.get(\"user_id\")\n</code></pre> <p>This example demonstrates changing the key from session_id to user_id for better data organization:</p> Producer - <code>selectKey</code> example (click to expand) <pre><code>streams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_user_event:\n    type: generator\n    resultType: (string,json)\n    code: |\n      import random, time\n      uid = random.choice([\"u01\", \"u02\", \"u03\"])\n      evt = random.choice([\"login\", \"purchase\", \"view\"])\n      sid = f\"s{random.randint(100, 999)}\"\n      data = {\n        \"user_id\": uid,\n        \"event_type\": evt,\n        \"session_id\": sid,\n        \"timestamp\": int(time.time())\n      }\n      if evt == \"purchase\":\n        data[\"amount\"] = round(random.uniform(10, 500), 2)\n      return (sid, data)\n\nproducers:\n  # Produce user events every 3 seconds\n  user_event_producer:\n    generator: generate_user_event\n    interval: 3s\n    to: user_events\n</code></pre> Processor - <code>selectKey</code> example (click to expand) <pre><code>streams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json\n\n  user_keyed_events:\n    topic: user_keyed_events\n    keyType: string\n    valueType: json\n\npipelines:\n  rekey_user_events:\n    from: user_events\n    via:\n      - type: selectKey\n        mapper:\n          expression: value.get(\"user_id\")\n    to: user_keyed_events\n</code></pre> <p>What this example does:</p> <ul> <li>The producer generates user events (login, purchase, view, logout, search) with different session IDs as keys</li> <li>The processor uses <code>selectKey</code> to change the key from <code>session_id</code> to <code>user_id</code>, enabling better data partitioning for user-centric analytics</li> <li>This rekeying allows subsequent operations to group and aggregate data by user rather than by session</li> </ul>"},{"location":"reference/operation-reference/#transformkey","title":"<code>transformKey</code>","text":"<p>Transforms the key using a custom transformer function.</p>"},{"location":"reference/operation-reference/#parameters_5","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> String Yes Name of the key transformer function"},{"location":"reference/operation-reference/#example_5","title":"Example","text":"<pre><code>      - type: transformKey\n        mapper: extract_customer_id\n</code></pre> <p>Full example for <code>transformKey</code>:</p> <ul> <li>Tutorial: Joins</li> </ul>"},{"location":"reference/operation-reference/#transformvalue","title":"<code>transformValue</code>","text":"<p>Transforms the value using a custom transformer function.</p>"},{"location":"reference/operation-reference/#parameters_6","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> String Yes Name of the value transformer function"},{"location":"reference/operation-reference/#example_6","title":"Example","text":"<pre><code>      - type: transformValue\n        mapper: convert_temperature\n</code></pre> <p>Full example for <code>transformValue</code>:</p> <ul> <li>Tutorial: Filtering and Transforming</li> </ul>"},{"location":"reference/operation-reference/#filtering-operations","title":"Filtering Operations","text":"<p>Filtering operations selectively pass or remove records based on conditions, allowing you to control which data continues through your processing pipeline.</p>"},{"location":"reference/operation-reference/#filter","title":"<code>filter</code>","text":"<p>Keeps only records that satisfy a condition.</p>"},{"location":"reference/operation-reference/#parameters_7","title":"Parameters","text":"Parameter Type Required Description <code>if</code> Object Yes Specifies the condition <p>The <code>if</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple boolean expression</li> <li><code>code</code>: A Python code block returning a boolean</li> </ul>"},{"location":"reference/operation-reference/#example_7","title":"Example","text":"<pre><code>      - type: filter\n        if: is_critical_sensor\n</code></pre> <p>Full example for <code>filter</code>:</p> <ul> <li>Tutorial: Filtering and Transforming</li> </ul>"},{"location":"reference/operation-reference/#filternot","title":"<code>filterNot</code>","text":"<p>Excludes records that satisfy a condition (opposite of filter). Records are kept when the condition returns false.</p>"},{"location":"reference/operation-reference/#parameters_8","title":"Parameters","text":"Parameter Type Required Description <code>if</code> Object Yes Specifies the condition <p>The <code>if</code> parameter must reference a predicate function that returns a boolean.</p>"},{"location":"reference/operation-reference/#example_8","title":"Example","text":"<pre><code>      - type: filterNot\n        if: is_inactive_product\n</code></pre> <p>This example filters out products with \"inactive\" status, keeping all other products:</p> Producer - <code>filterNot</code> example (click to expand) <pre><code>functions:\n  generate_product:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n      pid = f\"p{counter:03d}\"\n      return pid, {\n        \"product_id\": pid,\n        \"name\": random.choice([\"laptop\", \"phone\", \"tablet\"]),\n        \"status\": random.choice([\"active\", \"inactive\", \"pending\"]),\n        \"price\": round(random.uniform(50, 1500), 2)\n      }\n    resultType: (string, json)\n\nproducers:\n  product_producer:\n    generator: generate_product\n    interval: 2s\n    to:\n      topic: product_stream\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>filterNot</code> example (click to expand) <pre><code>streams:\n  product_input:\n    topic: product_stream\n    keyType: string\n    valueType: json\n\nfunctions:\n  is_inactive_product:\n    type: predicate\n    code: |\n      if value is None:\n        return False\n\n      status = value.get(\"status\", \"\")\n      if status == \"inactive\":\n        print(f\"Filtering out inactive product: {value.get('product_id')} - {value.get('name')}\")\n        return True\n\n      return False\n\npipelines:\n  main:\n    from: product_input\n    via:\n      - type: filterNot\n        if: is_inactive_product\n      - type: peek\n        forEach:\n          code: |\n            print(f\"Active product kept: {key} - {value['name']} (status: {value['status']}, price: ${value['price']})\")\n    to:\n      topic: active_products\n      keyType: string\n      valueType: json\n</code></pre> <p>What this example does:</p> <ul> <li>The producer generates products with different statuses: active, inactive, pending, discontinued</li> <li>The processor uses <code>filterNot</code> with a predicate function to exclude products with \"inactive\" status</li> <li>Products with other statuses (active, pending, discontinued) are kept and passed through to the output topic</li> </ul>"},{"location":"reference/operation-reference/#format-conversion-operations","title":"Format Conversion Operations","text":"<p>Format conversion operations change the serialization format of keys or values without altering the actual data content.</p>"},{"location":"reference/operation-reference/#convertkey","title":"<code>convertKey</code>","text":"<p>Converts the key to a different data format.</p>"},{"location":"reference/operation-reference/#parameters_9","title":"Parameters","text":"Parameter Type Required Description <code>into</code> String Yes Target format for the key"},{"location":"reference/operation-reference/#example_9","title":"Example","text":"<pre><code>      - type: peek\n        forEach:\n</code></pre> <p>Full example for <code>convertKey</code>:</p> <ul> <li>Tutorial: Windowing</li> </ul>"},{"location":"reference/operation-reference/#convertvalue","title":"<code>convertValue</code>","text":"<p>Converts the value to a different data format.</p>"},{"location":"reference/operation-reference/#parameters_10","title":"Parameters","text":"Parameter Type Required Description <code>into</code> String Yes Target format for the value"},{"location":"reference/operation-reference/#example_10","title":"Example","text":"<pre><code>      - type: convertValue\n        into: json\n</code></pre> <p>Full example for <code>convertValue</code>:</p> <ul> <li>Tutorial: Data Formats</li> </ul>"},{"location":"reference/operation-reference/#grouping-partitioning-operations","title":"Grouping &amp; Partitioning Operations","text":"<p>Grouping and partitioning operations organize data by keys and control how records are distributed across partitions, preparing data for aggregation or improving processing parallelism.</p>"},{"location":"reference/operation-reference/#groupby","title":"<code>groupBy</code>","text":"<p>Groups records by a new key derived from the record.</p>"},{"location":"reference/operation-reference/#parameters_11","title":"Parameters","text":"Parameter Type Required Description <code>keySelector</code> Object Yes Specifies how to select the new key <p>The <code>keySelector</code> can be defined using: - <code>expression</code>: A simple expression returning the grouping key - <code>code</code>: A Python code block returning the grouping key</p>"},{"location":"reference/operation-reference/#example_11","title":"Example","text":"<pre><code>      - type: groupBy\n        name: group_by_sensor_type\n        mapper:\n          code: |\n            if value is None:\n              return \"unknown\"  \n            if not \"sensor_type\" in value:\n              return \"unknown\"\n          expression: value[\"sensor_type\"]\n          resultType: string\n</code></pre> <p>Full example for <code>groupBy</code>:</p> <ul> <li>Tutorial: State Stores</li> </ul>"},{"location":"reference/operation-reference/#groupbykey","title":"<code>groupByKey</code>","text":"<p>Groups records by their existing key for subsequent aggregation operations.</p>"},{"location":"reference/operation-reference/#parameters_12","title":"Parameters","text":"<p>None. This operation is typically followed by an aggregation operation.</p>"},{"location":"reference/operation-reference/#example_12","title":"Example","text":"<pre><code>      - type: groupByKey\n</code></pre> <p>Full example for <code>groupByKey</code>:</p> <ul> <li>Tutorial: Aggregations</li> </ul>"},{"location":"reference/operation-reference/#repartition","title":"<code>repartition</code>","text":"<p>Redistributes records across partitions, optionally using custom partitioning logic. This operation allows you to control data distribution for performance optimization or to meet specific processing requirements.</p>"},{"location":"reference/operation-reference/#parameters_13","title":"Parameters","text":"Parameter Type Required Description <code>numberOfPartitions</code> Integer No Number of partitions for redistribution <code>partitioner</code> String No Function name for custom partitioning logic"},{"location":"reference/operation-reference/#example_13","title":"Example","text":"<p>Note:</p> <p>To test this repartition example, ensure your topics have sufficient partitions. The example requires minimum 4 partitions since it redistributes to 4 partitions (0-3). Update your docker-compose.yml:</p> <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 4 --replication-factor 1 --topic user_activities\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 4 --replication-factor 1 --topic repartitioned_activities\n</code></pre> <pre><code>      - type: repartition\n        numberOfPartitions: 4\n        partitioner: activity_partitioner\n</code></pre> Producer - <code>repartition</code> example (click to expand) <pre><code>functions:\n  generate_user_activity:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      activity_id = 0\n      user_ids = [\"user_001\", \"user_002\", \"user_003\", \"user_004\", \"user_005\"]\n      activity_types = [\"login\", \"purchase\", \"browse\", \"logout\", \"search\"]\n      regions = [\"north\", \"south\", \"east\", \"west\"]\n    code: |\n      global activity_id\n\n      activity_id += 1\n\n      # Generate user activity with region as initial key\n      region = random.choice(regions)\n      activity = {\n        \"activity_id\": f\"activity_{activity_id:04d}\",\n        \"user_id\": random.choice(user_ids),\n        \"activity_type\": random.choice(activity_types),\n        \"timestamp\": int(time.time() * 1000),\n        \"region\": region\n      }\n\n      # Log the activity generation\n      log.info(\"Generated activity: {} for user {} in region {} - type: {}\", \n               activity[\"activity_id\"], activity[\"user_id\"], region, activity[\"activity_type\"])\n\n      # Use region as key initially (will be repartitioned by user_id later)\n      key = region\n      value = activity\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  user_activity_producer:\n    generator: generate_user_activity\n    interval: 3s\n    to:\n      topic: user_activities\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>repartition</code> example (click to expand) <pre><code>streams:\n  user_activities:\n    topic: user_activities\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  repartitioned_activities:\n    topic: repartitioned_activities\n    keyType: string\n    valueType: json\n\nfunctions:\n  # Custom partitioner that distributes activities by user type\n  # Users ending in even numbers get partition 0-1, odd numbers get partition 2-3\n  activity_partitioner:\n    type: streamPartitioner\n    code: |\n      # Custom partitioning logic based on user_id pattern\n      # This demonstrates intelligent partitioning for user-based processing\n\n      if value and \"user_id\" in value:\n        user_id = value[\"user_id\"]\n        # Extract user number from user_001, user_002, etc.\n        user_num = int(user_id.split(\"_\")[-1])\n\n        # Even user numbers (002, 004, 006, ...) -&gt; partitions 0-1\n        # Odd user numbers (001, 003, 005, ...) -&gt; partitions 2-3\n        if user_num % 2 == 0:\n          # Distribute even users across partitions 0 and 1\n          partition = (user_num // 2) % 2  # 002-&gt;1, 004-&gt;0, 006-&gt;1, 008-&gt;0\n        else:\n          # Distribute odd users across partitions 2 and 3\n          partition = 2 + ((user_num // 2) % 2)  # 001-&gt;2, 003-&gt;3, 005-&gt;2, 007-&gt;3\n\n        log.debug(\"Routing user {} (num:{}) to partition {}\", \n                  user_id, user_num, partition)\n\n        return partition\n\n      # Default to partition 0\n      return 0\n    resultType: integer\n\npipelines:\n  repartition_activities:\n    from: user_activities\n    via:\n      # First, change key from region to user_id for user-based processing\n      - type: mapKey\n        mapper:\n          code: |\n            # Extract user_id from the activity to become the new key\n            if value and \"user_id\" in value:\n              user_key = value[\"user_id\"]\n              log.info(\"Changing key from region '{}' to user_id '{}'\", key, user_key)\n              result = user_key\n            else:\n              result = key\n          expression: result\n          resultType: string\n\n      # Add processing metadata\n      - type: transformValue\n        mapper:\n          code: |\n            # Add repartitioning info to track the transformation\n            if value:\n              value[\"processing_info\"] = f\"Repartitioned by user: {value.get('user_id', 'unknown')}\"\n              value[\"original_region\"] = key  # Keep track of original region\n            result = value\n          expression: result\n          resultType: json\n\n      # Repartition with custom partitioner for user-based distribution\n      - type: repartition\n        numberOfPartitions: 4\n        partitioner: activity_partitioner\n\n      # Log the repartitioned activity\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Repartitioned activity {}: {} -&gt; user-based partitioning applied\", \n                       key, value.get(\"processing_info\", \"unknown\"))\n\n    # Send to output topic to observe the repartitioning results\n    to:\n      topic: repartitioned_activities\n      keyType: string\n      valueType: json\n      partitioner: activity_partitioner\n</code></pre> <p>The repartition operation demonstrates data redistribution by changing keys from regions to user IDs, then using custom partitioning logic to distribute activities based on user patterns. This ensures related user activities are processed together while optimizing partition utilization.</p> <p>What the example does:</p> <p>Demonstrates intelligent data redistribution for user-centric processing:</p> <ul> <li>Changes partitioning strategy from region-based to user-based keys</li> <li>Applies custom partitioning logic based on user ID patterns</li> <li>Routes even user numbers (002, 004, 006, ...) to partitions 0-1 (alternating: 002\u21921, 004\u21920, 006\u21921)</li> <li>Routes odd user numbers (001, 003, 005, ...) to partitions 2-3 (alternating: 001\u21922, 003\u21923, 005\u21922)</li> <li>Producer generates activities initially keyed by region for realistic repartitioning scenario</li> </ul> <p>Key Features:</p> <ul> <li>Dynamic key transformation from region to user_id</li> <li>Custom partition calculation based on user patterns</li> <li>Guaranteed co-location of activities for the same user</li> <li>Processing metadata tracking for observability</li> <li>Explicit partition count handling (4 partitions total)</li> <li>Partitioner applied both in repartition operation and output topic for consistency</li> <li>Fallback to partition 0 for edge cases (missing user_id)</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see log messages like:</p> <ul> <li><code>\"Generated activity: activity_0001 for user user_001 in region south - type: purchase\"</code> - Producer creating activities</li> <li><code>\"Changing key from region 'south' to user_id 'user_001'\"</code> - Key transformation</li> <li><code>\"Repartitioned activity user_001: Repartitioned by user: user_001 -&gt; user-based partitioning applied\"</code> - Successful repartitioning</li> </ul> <p>Partition assignments follow the custom logic:</p> <ul> <li>Odd users: user_001 \u2192 partition 2, user_003 \u2192 partition 3, user_005 \u2192 partition 2</li> <li>Even users: user_002 \u2192 partition 1, user_004 \u2192 partition 0, user_006 \u2192 partition 1</li> <li>Activities for the same user are guaranteed to go to the same partition (ensuring order)</li> </ul>"},{"location":"reference/operation-reference/#stateful-aggregation-operations","title":"Stateful Aggregation Operations","text":"<p>Stateful aggregation operations maintain state between records to perform calculations like counting, summing, or building custom aggregates based on record keys.</p>"},{"location":"reference/operation-reference/#aggregate","title":"<code>aggregate</code>","text":"<p>Aggregates records by key using a custom aggregation function.</p>"},{"location":"reference/operation-reference/#parameters_14","title":"Parameters","text":"Parameter Type Required Description <code>initializer</code> Object Yes Specifies the initial value for the aggregation <code>aggregator</code> Object Yes Specifies how to combine the current record with the aggregate <p>Both <code>initializer</code> and <code>aggregator</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression</li> <li><code>code</code>: A Python code block</li> </ul>"},{"location":"reference/operation-reference/#example_14","title":"Example","text":"<pre><code>          windowSize: 1m\n          retention: 10m\n        initializer: initialize_sales_stats\n        aggregator: aggregate_sales\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n</code></pre> <p>Full example for <code>aggregate</code>:</p> <ul> <li>Tutorial: Aggregations</li> </ul>"},{"location":"reference/operation-reference/#count","title":"<code>count</code>","text":"<p>Counts the number of records for each key.</p>"},{"location":"reference/operation-reference/#parameters_15","title":"Parameters","text":"<p>None.</p>"},{"location":"reference/operation-reference/#example_15","title":"Example","text":"<pre><code>      - type: count\n</code></pre> <p>Full example for <code>count</code>:</p> <ul> <li>Tutorial: Aggregations</li> </ul>"},{"location":"reference/operation-reference/#reduce","title":"<code>reduce</code>","text":"<p>Combines records with the same key using a reducer function.</p>"},{"location":"reference/operation-reference/#parameters_16","title":"Parameters","text":"Parameter Type Required Description <code>reducer</code> Object Yes Specifies how to combine two values <p>The <code>reducer</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression</li> <li><code>code</code>: A Python code block</li> </ul>"},{"location":"reference/operation-reference/#example_16","title":"Example","text":"<pre><code>      - type: reduce\n        reducer: sum_amounts\n</code></pre> <p>Full example for <code>reduce</code>:</p> <ul> <li>Tutorial: Aggregations</li> </ul>"},{"location":"reference/operation-reference/#join-operations","title":"Join Operations","text":"<p>Join operations combine data from multiple streams or tables based on matching keys, enabling you to correlate related events from different data sources.</p>"},{"location":"reference/operation-reference/#join","title":"<code>join</code>","text":"<p>Performs an inner join between two streams.</p>"},{"location":"reference/operation-reference/#parameters_17","title":"Parameters","text":"Parameter Type Required Description <code>stream</code> String Yes The name of the stream to join with <code>table</code> String Yes The name of the table to join with (for stream-table joins) <code>valueJoiner</code> Object Yes Function that defines how to combine values from both sides <code>timeDifference</code> Duration No The time difference for the join window (for stream-stream joins) <code>grace</code> Duration No Grace period for late-arriving data (for stream-stream joins) <code>foreignKeyExtractor</code> Object No Function to extract foreign key (for stream-table joins) <code>partitioner</code> String No Function name for custom partitioning of current stream <code>otherPartitioner</code> String No Function name for custom partitioning of join stream/table"},{"location":"reference/operation-reference/#example_17","title":"Example","text":"<pre><code>      - type: join\n        stream: product_purchases\n        valueJoiner: correlate_click_and_purchase\n        timeDifference: 30m  # Look for purchases within 30 minutes of a click\n        grace: 5m  # Grace period for late events\n        thisStore:\n          name: clicks_join_store\n          type: window\n          windowSize: 60m  # Must be 2*timeDifference\n          retention: 65m   # Must be 2*timeDifference + grace = 60m + 5m\n          retainDuplicates: true\n        otherStore:\n          name: purchases_join_store\n          type: window\n          windowSize: 60m  # Must be 2*timeDifference\n          retention: 65m   # Must be 2*timeDifference + grace = 60m + 5m\n          retainDuplicates: true\n</code></pre> <p>Full example for <code>join</code>:</p> <ul> <li>Tutorial: Joins</li> </ul>"},{"location":"reference/operation-reference/#leftjoin","title":"<code>leftJoin</code>","text":"<p>Performs a left join between two streams.</p>"},{"location":"reference/operation-reference/#parameters_18","title":"Parameters","text":"Parameter Type Required Description <code>stream</code> String Yes The name of the stream to join with <code>table</code> String Yes The name of the table to join with (for stream-table joins) <code>valueJoiner</code> Object Yes Function that defines how to combine values from both sides <code>timeDifference</code> Duration No The time difference for the join window (for stream-stream joins) <code>grace</code> Duration No Grace period for late-arriving data (for stream-stream joins) <code>foreignKeyExtractor</code> Object No Function to extract foreign key (for stream-table joins) <code>partitioner</code> String No Function name for custom partitioning of current stream <code>otherPartitioner</code> String No Function name for custom partitioning of join stream/table"},{"location":"reference/operation-reference/#example_18","title":"Example","text":"<pre><code>      - type: leftJoin\n        table: user_locations\n        valueJoiner: join_activity_with_location\n</code></pre> <p>Full example for <code>leftJoin</code>:</p> <ul> <li>Tutorial: Joins</li> </ul>"},{"location":"reference/operation-reference/#merge","title":"<code>merge</code>","text":"<p>Merges multiple streams with identical key and value types into a single unified stream. The merge operation combines streams without any joining logic - it simply forwards all records from all input streams to the output stream in the order they arrive.</p>"},{"location":"reference/operation-reference/#parameters_19","title":"Parameters","text":"Parameter Type Required Description <code>stream</code> String Yes The name of the stream to merge with the main stream"},{"location":"reference/operation-reference/#example_19","title":"Example","text":"<pre><code>    to:\n      topic: merged_stream\n</code></pre> Producer - <code>merge</code> example (click to expand) <pre><code>functions:\n  generate_stream_a:\n    type: generator\n    globalCode: |\n      import random\n      message_id = 0\n      colors = [\"red\", \"blue\", \"green\"]\n    code: |\n      global message_id\n      message_id += 1\n\n      message = {\n        \"id\": f\"stream_a_{message_id}\",\n        \"color\": random.choice(colors),\n        \"source\": \"stream_a\"\n      }\n\n      log.info(\"Generated message from stream A: {} - color: {}\", message[\"id\"], message[\"color\"])\n\n      key = message[\"color\"]\n      value = message\n    expression: (key, value)\n    resultType: (string, json)\n\n  generate_stream_b:\n    type: generator\n    globalCode: |\n      import random\n      message_id = 0\n      colors = [\"red\", \"blue\", \"green\"]\n    code: |\n      global message_id\n      message_id += 1\n\n      message = {\n        \"id\": f\"stream_b_{message_id}\",\n        \"color\": random.choice(colors),\n        \"source\": \"stream_b\"\n      }\n\n      log.info(\"Generated message from stream B: {} - color: {}\", message[\"id\"], message[\"color\"])\n\n      key = message[\"color\"]\n      value = message\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  stream_a_producer:\n    generator: generate_stream_a\n    interval: 3s\n    to:\n      topic: stream_a\n      keyType: string\n      valueType: json\n\n  stream_b_producer:\n    generator: generate_stream_b\n    interval: 2s\n    to:\n      topic: stream_b\n      keyType: string\n      valueType: json\n</code></pre> Processor - <code>merge</code> example (click to expand) <pre><code>streams:\n  stream_a:\n    topic: stream_a\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  stream_b:\n    topic: stream_b\n    keyType: string\n    valueType: json\n    offsetResetPolicy: earliest\n\n  merged_stream:\n    topic: merged_stream\n    keyType: string\n    valueType: json\n\npipelines:\n  merge_streams:\n    from: stream_a\n    via:\n      # Merge with stream B\n      - type: merge\n        stream: stream_b\n\n      # Log merged messages\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Merged message: {} from {} - color: {}\", \n                       value.get(\"id\", \"unknown\"), \n                       value.get(\"source\", \"unknown\"), \n                       value.get(\"color\", \"unknown\"))\n    to:\n      topic: merged_stream\n      keyType: string\n      valueType: json\n</code></pre> <p>What This Example Does:</p> <p>The example demonstrates merging two independent streams (<code>stream_a</code> and <code>stream_b</code>) into a single processing pipeline. Both producers generate messages with a color key and JSON values containing an id, color, and source field. The merge operation combines both streams so that messages from either stream flow through the same downstream processing.</p> <p>How the Merge Operation Works:</p> <ul> <li>Stream Union: The merge operation creates a simple union of multiple streams - all records from all input streams are forwarded to the output</li> <li>No Transformation: Records pass through unchanged, maintaining their original keys and values</li> <li>Interleaved Processing: Messages from different streams are processed as they arrive, interleaved based on timing</li> <li>Shared Pipeline: After merging, both streams share the same downstream operations (in this example, the peek operation logs all messages)</li> </ul> <p>Important Notes:</p> <ul> <li>All streams being merged must have identical key and value types</li> <li>Records maintain their original timestamps and ordering per stream</li> <li>No complex joining logic - this is a simple stream union operation</li> <li>Can merge any number of streams by chaining multiple merge operations</li> </ul> <p>Expected Results:</p> <p>When running this example, you'll see interleaved log messages like:</p> <ul> <li><code>\"Generated message from stream A: stream_a_1 - color: green\"</code> - Producer A creating messages every 3 seconds</li> <li><code>\"Generated message from stream B: stream_b_2 - color: blue\"</code> - Producer B creating messages every 2 seconds  </li> <li><code>\"Merged message: stream_a_1 from stream_a - color: green\"</code> - Merged pipeline processing stream A messages</li> <li><code>\"Merged message: stream_b_2 from stream_b - color: blue\"</code> - Same pipeline processing stream B messages</li> </ul> <p>Both streams flow through the unified pipeline after merging, demonstrating how merge combines multiple data sources for shared processing.</p>"},{"location":"reference/operation-reference/#outerjoin","title":"<code>outerJoin</code>","text":"<p>Performs an outer join between two streams.</p>"},{"location":"reference/operation-reference/#parameters_20","title":"Parameters","text":"Parameter Type Required Description <code>stream</code> String Yes The name of the stream to join with <code>table</code> String Yes The name of the table to join with (for stream-table joins) <code>valueJoiner</code> Object Yes Function that defines how to combine values from both sides <code>timeDifference</code> Duration No The time difference for the join window (for stream-stream joins) <code>grace</code> Duration No Grace period for late-arriving data (for stream-stream joins) <code>foreignKeyExtractor</code> Object No Function to extract foreign key (for stream-table joins) <code>partitioner</code> String No Function name for custom partitioning of current stream <code>otherPartitioner</code> String No Function name for custom partitioning of join stream/table"},{"location":"reference/operation-reference/#example_20","title":"Example","text":"<pre><code>      - type: outerJoin\n        stream: user_logouts\n        valueJoiner: analyze_user_session\n        timeDifference: 10m  # Look for logouts within 10 minutes of login\n        grace: 2m  # Grace period for late events\n        thisStore:\n          name: login_session_store\n          type: window\n          windowSize: 20m  # Must be 2*timeDifference\n          retention: 22m   # Must be 2*timeDifference + grace\n          retainDuplicates: true\n        otherStore:\n          name: logout_session_store\n          type: window\n          windowSize: 20m  # Must be 2*timeDifference\n          retention: 22m   # Must be 2*timeDifference + grace\n          retainDuplicates: true\n</code></pre> <p>Full example for <code>outerJoin</code>:</p> <ul> <li>Tutorial: Joins</li> </ul>"},{"location":"reference/operation-reference/#windowing-operations","title":"Windowing Operations","text":"<p>Windowing operations group records into time-based windows, enabling temporal aggregations and time-bounded processing.</p>"},{"location":"reference/operation-reference/#windowbytime","title":"<code>windowByTime</code>","text":"<p>Groups records into time windows.</p>"},{"location":"reference/operation-reference/#parameters_21","title":"Parameters","text":"Parameter Type Required Description <code>windowType</code> String No The type of window (<code>tumbling</code>, <code>hopping</code>, or <code>sliding</code>) <code>timeDifference</code> Duration Yes The duration of the window <code>advanceBy</code> Long No Only required for <code>hopping</code> windows, how often to advance the window <code>grace</code> Long No Grace period for late-arriving data"},{"location":"reference/operation-reference/#example_21","title":"Example","text":"<pre><code>        duration: 5m\n        grace: 30s\n      - type: count\n        store:\n</code></pre> <p>Full example for <code>windowByTime</code>:</p> <ul> <li>Tutorial: Windowing</li> </ul>"},{"location":"reference/operation-reference/#windowbysession","title":"<code>windowBySession</code>","text":"<p>Groups records into session windows, where events with timestamps within <code>inactivityGap</code> durations are seen as belonging to the same session.</p>"},{"location":"reference/operation-reference/#parameters_22","title":"Parameters","text":"Parameter Type Required Description <code>inactivityGap</code> Duration Yes The maximum duration between events before they are seen as belonging to a different session <code>grace</code> Long No Grace period for late-arriving data"},{"location":"reference/operation-reference/#example_22","title":"Example","text":"<pre><code>        grace: 30s\n      - type: count\n        store:\n</code></pre> <p>Full example for <code>windowBySession</code>:</p> <ul> <li>Tutorial: Windowing</li> </ul>"},{"location":"reference/operation-reference/#output-operations","title":"Output Operations","text":"<p>Output operations represent the end of a processing pipeline, sending records to topics or performing terminal actions like logging.</p>"},{"location":"reference/operation-reference/#to","title":"<code>to</code>","text":"<p>Sends records to a specific Kafka topic.</p>"},{"location":"reference/operation-reference/#parameters_23","title":"Parameters","text":"Parameter Type Required Description <code>topic</code> String Yes The name of the target topic <code>keyType</code> String No The data type of the key <code>valueType</code> String No The data type of the value <code>partitioner</code> String No Function name for custom partitioning logic"},{"location":"reference/operation-reference/#example_23","title":"Example","text":"<pre><code>    to: output_stream\n</code></pre>"},{"location":"reference/operation-reference/#example-with-custom-partitioner","title":"Example with Custom Partitioner","text":"<pre><code>    to:\n      topic: partitioned_orders\n      keyType: string\n      valueType: json\n      partitioner: priority_region_partitioner\n</code></pre> <p>Full example for <code>to</code>:</p> <ul> <li>Tutorial: Filtering and Transforming</li> </ul> <p>Full example for <code>to</code> with partitioner:</p> <ul> <li>streamPartitioner Function Reference</li> </ul>"},{"location":"reference/operation-reference/#totopicnameextractor","title":"<code>toTopicNameExtractor</code>","text":"<p>Sends records to topics determined dynamically based on the record content. This operation enables content-based routing, allowing you to distribute messages to different topics based on their attributes, priorities, or business logic.</p>"},{"location":"reference/operation-reference/#parameters_24","title":"Parameters","text":"Parameter Type Required Description <code>topicNameExtractor</code> String Yes Name of the function that determines the topic name <code>partitioner</code> String No Function name for custom partitioning logic"},{"location":"reference/operation-reference/#example_24","title":"Example","text":"<pre><code>    toTopicNameExtractor:\n      topicNameExtractor: route_by_severity\n</code></pre> <p>This example demonstrates routing system events to different topics based on severity level:</p> Producer - <code>toTopicNameExtractor</code> example (click to expand) <pre><code>streams:\n  system_events:\n    topic: system_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_system_event:\n    type: generator\n    resultType: (string,json)\n    code: |\n      import random, time\n      sev = random.choice([\"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"])\n      msgs = {\n        \"CRITICAL\": [\"System failure\", \"Service down\"],\n        \"ERROR\": [\"Request timeout\", \"DB failed\"],\n        \"WARNING\": [\"High CPU\", \"Disk low\"],\n        \"INFO\": [\"Service started\", \"Health OK\"]\n      }\n      eid = f\"e{random.randint(1000, 9999)}\"\n      return eid, {\n        \"event_id\": eid,\n        \"severity\": sev,\n        \"component\": random.choice([\"api\", \"db\", \"cache\"]),\n        \"message\": random.choice(msgs[sev]),\n        \"timestamp\": int(time.time()),\n        \"cpu\": random.uniform(10 if sev==\"INFO\" else 70, 100)\n      }\n\nproducers:\n  # Produce system events every 2 seconds\n  system_event_producer:\n    generator: generate_system_event\n    interval: 2s\n    to: system_events\n</code></pre> Processor - <code>toTopicNameExtractor</code> example (click to expand) <pre><code>streams:\n  system_events:\n    topic: system_events\n    keyType: string\n    valueType: json\n\n  critical_alerts:\n    topic: critical_alerts\n    keyType: string\n    valueType: json\n\n  error_logs:\n    topic: error_logs\n    keyType: string\n    valueType: json\n\n  warning_logs:\n    topic: warning_logs\n    keyType: string\n    valueType: json\n\n  info_logs:\n    topic: info_logs\n    keyType: string\n    valueType: json\n\nfunctions:\n  route_by_severity:\n    type: topicNameExtractor\n    code: |\n      if value is None: return \"info_logs\"\n      sev = value.get(\"severity\", \"INFO\")\n      if sev == \"CRITICAL\":\n        log.error(\"CRITICAL: {}\", value.get(\"message\"))\n      return {\"CRITICAL\": \"critical_alerts\", \"ERROR\": \"error_logs\", \n              \"WARNING\": \"warning_logs\"}.get(sev, \"info_logs\")\n    resultType: string\n\npipelines:\n  route_system_events:\n    from: system_events\n    toTopicNameExtractor:\n      topicNameExtractor: route_by_severity\n</code></pre> <p>What this example does:</p> <ul> <li>The producer generates system events with different severity levels (INFO, WARNING, ERROR, CRITICAL) from various system components</li> <li>The processor uses <code>toTopicNameExtractor</code> with a custom function to route events to different topics based on severity</li> <li>Critical events are logged and sent to <code>critical_alerts</code> topic, while other severities go to their respective log topics</li> <li>This pattern enables priority-based processing and separate handling of critical system issues</li> </ul>"},{"location":"reference/operation-reference/#foreach","title":"<code>forEach</code>","text":"<p>Processes each record with a side effect, typically used for logging or external actions. This is a terminal operation that does not forward records.</p>"},{"location":"reference/operation-reference/#parameters_25","title":"Parameters","text":"Parameter Type Required Description <code>forEach</code> Object Yes Specifies the action to perform on each record <p>The <code>forEach</code> can be defined using: - <code>code</code>: A Python code block performing the side effect</p>"},{"location":"reference/operation-reference/#example_25","title":"Example","text":"<pre><code>        forEach:\n          code: |\n            log_message(key, value)\n</code></pre> <p>Full example for <code>forEach</code>:</p> <ul> <li>Tutorial: Filtering and Transforming</li> </ul>"},{"location":"reference/operation-reference/#print","title":"<code>print</code>","text":"<p>Prints each record to stdout for debugging purposes. This operation can use a custom mapper function to format the output, providing colored indicators and structured logging.</p>"},{"location":"reference/operation-reference/#parameters_26","title":"Parameters","text":"Parameter Type Required Description <code>mapper</code> String No Name of keyValuePrinter function to format output <code>prefix</code> String No Optional prefix for the printed output"},{"location":"reference/operation-reference/#example_26","title":"Example","text":"<pre><code>    from: debug_messages\n    print:\n      mapper: format_debug_output\n</code></pre> <p>This example demonstrates printing debug messages with color-coded log levels and custom formatting:</p> Producer - <code>print</code> example (click to expand) <pre><code>streams:\n  debug_messages:\n    topic: debug_messages\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_debug_message:\n    type: generator\n    resultType: (string,json)\n    code: |\n      import random, time\n      lvl = random.choice([\"INFO\", \"WARNING\", \"ERROR\", \"DEBUG\"])\n      msgs = {\n        \"ERROR\": [\"DB timeout\", \"Payment failed\"],\n        \"WARNING\": [\"High CPU\", \"Rate limit near\"],\n        \"DEBUG\": [\"Auth check\", \"Loading config\"],\n        \"INFO\": [\"Service started\", \"Request done\"]\n      }\n      rid = f\"r{random.randint(100, 999)}\"\n      return rid, {\n        \"timestamp\": int(time.time()),\n        \"level\": lvl,\n        \"component\": random.choice([\"UserSvc\", \"PaySvc\"]),\n        \"message\": random.choice(msgs[lvl]),\n        \"request_id\": rid,\n        \"thread_id\": f\"t{random.randint(1, 5)}\"\n      }\n\nproducers:\n  # Produce debug messages every 2 seconds\n  debug_message_producer:\n    generator: generate_debug_message\n    interval: 2s\n    to: debug_messages\n</code></pre> Processor - <code>print</code> example (click to expand) <pre><code>streams:\n  debug_messages:\n    topic: debug_messages  \n    keyType: string\n    valueType: json\n\nfunctions:\n  format_debug_output:\n    type: keyValuePrinter\n    code: |\n      if value is None: return f\"[NULL] {key}\"\n      lvl = value.get(\"level\", \"?\")\n      colors = {\"ERROR\": \"red\", \"WARNING\": \"yellow\", \"INFO\": \"green\", \"DEBUG\": \"blue\"}\n      # Include prefix in the output string\n      return f\"DEBUG_CONSOLE: {colors.get(lvl, 'white')} [{lvl}] {value.get('component')} | {value.get('message')} | {value.get('request_id')}\"\n    resultType: string\n\npipelines:\n  print_debug_messages:\n    from: debug_messages\n    print:\n      mapper: format_debug_output\n</code></pre> <p>What this example does:</p> <ul> <li>The producer generates different types of debug messages (INFO, WARNING, ERROR, DEBUG) from various system components</li> <li>The processor uses <code>print</code> with a custom <code>keyValuePrinter</code> function to format each message with color indicators (\"red\" for ERROR, \"yellow\" for WARNING, \"green\" for INFO, \"blue\" for DEBUG)</li> <li>The formatted output includes component name, message text, request ID, and thread information for comprehensive debugging</li> </ul>"},{"location":"reference/operation-reference/#control-flow-operations","title":"Control Flow Operations","text":"<p>Control flow operations manage the flow of data through your processing pipeline, allowing for branching logic and record observation.</p>"},{"location":"reference/operation-reference/#branch","title":"<code>branch</code>","text":"<p>Splits a stream into multiple substreams based on conditions.</p>"},{"location":"reference/operation-reference/#parameters_27","title":"Parameters","text":"Parameter Type Required Description <code>branches</code> Array Yes List of conditions and handling pipeline for each branch <p>The tag <code>branches</code> does not exist in the KSML language, but is meant to represent a composite object here that consists of two elements:</p> Parameter Type Required Description <code>if</code> Predicate Yes A condition which can evaluate to True or False. When True, the message is sent down the branch's pipeline <code>pipeline</code> Pipeline Yes A pipeline that contains a list of processing steps to send the message through"},{"location":"reference/operation-reference/#example_27","title":"Example","text":"<pre><code>    branch:\n      # Branch 1: Priority orders (premium customers, high value)\n      - if: is_priority\n        via:\n          - type: mapValues\n            mapper: add_priority_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"PRIORITY: Order {} - ${} premium order\", \n                       value.get(\"order_id\"), value.get(\"total_amount\"))\n        to: priority_orders\n\n      # Branch 2: Regional orders (US/EU, not priority)\n      - if: is_regional\n        via:\n          - type: mapValues\n            mapper: add_regional_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"REGIONAL: Order {} from {}\", \n                       value.get(\"order_id\"), value.get(\"region\"))\n        to: regional_orders\n\n      # Branch 3: International orders  \n      - if: is_international\n        via:\n          - type: mapValues\n            mapper: add_international_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"INTERNATIONAL: Order {} from {} (customs required)\", \n                       value.get(\"order_id\"), value.get(\"region\"))\n        to: international_orders\n</code></pre> <p>Full example for <code>branch</code>:</p> <ul> <li>Tutorial: Branching</li> </ul>"},{"location":"reference/operation-reference/#peek","title":"<code>peek</code>","text":"<p>Performs a side effect on each record without changing it.</p>"},{"location":"reference/operation-reference/#parameters_28","title":"Parameters","text":"Parameter Type Required Description <code>forEach</code> Object Yes Specifies the action to perform on each record <p>The <code>forEach</code> can be defined using:</p> <ul> <li><code>expression</code>: A simple expression (rarely used for peek)</li> <li><code>code</code>: A Python code block performing the side effect</li> </ul>"},{"location":"reference/operation-reference/#example_28","title":"Example","text":"<pre><code>          - type: peek\n            forEach:\n              code: |\n                log.info(\"PRIORITY: Order {} - ${} premium order\", \n                       value.get(\"order_id\"), value.get(\"total_amount\"))\n</code></pre> <p>Full example for <code>peek</code>:</p> <ul> <li>Tutorial: Branching</li> </ul>"},{"location":"reference/operation-reference/#combining-operations","title":"Combining Operations","text":"<p>Operations can be combined in various ways to create complex processing pipelines.</p>"},{"location":"reference/operation-reference/#sequential-operations","title":"Sequential Operations","text":"<p>Operations are executed in sequence, with each operation processing the output of the previous operation.</p> <pre><code>pipelines:\n  my_pipeline:\n    from: input_stream\n    via:\n      - type: filter\n        if:\n          expression: value.get(\"amount\") &gt; 0\n      - type: transformValue\n        mapper:\n          code: enrich_transaction(value)\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processed transaction: {}\", value)\n    to: output_stream\n</code></pre>"},{"location":"reference/operation-reference/#branching-and-merging","title":"Branching and Merging","text":"<p>You can create complex topologies by branching streams and merging them back together.</p> <pre><code>pipelines:\n  branch_pipeline:\n    from: input_stream\n    branch:\n      - if:\n          expression: value.get(\"type\") == \"A\"\n        as: type_a_stream\n      - if:\n          expression: value.get(\"type\") == \"B\"\n        as: type_b_stream\n\n  process_a_pipeline:\n    from: type_a_stream\n    via:\n      - type: mapValues\n        mapper:\n          code: process_type_a(value)\n    to: merged_stream\n\n  process_b_pipeline:\n    from: type_b_stream\n    via:\n      - type: mapValues\n        mapper:\n          code: process_type_b(value)\n    to: merged_stream\n</code></pre>"},{"location":"reference/operation-reference/#best-practices","title":"Best Practices","text":"<ul> <li>Chain operations thoughtfully: Consider the performance implications of chaining multiple operations.</li> <li>Use stateless operations when possible: Stateless operations are generally more efficient than stateful ones.</li> <li>Be careful with window sizes: Large windows can consume significant memory.</li> <li>Handle errors gracefully: Use error handling operations to prevent pipeline failures.</li> <li>Monitor performance: Keep an eye on throughput and latency, especially for stateful operations.</li> </ul>"},{"location":"reference/operation-reference/#how-ksml-operations-relate-to-kafka-streams","title":"How KSML Operations Relate to Kafka Streams","text":"<p>KSML operations are YAML-based wrappers around Kafka Streams topology operations. Understanding this relationship helps you leverage Kafka Streams documentation and concepts:</p>"},{"location":"reference/operation-reference/#direct-mappings","title":"Direct Mappings","text":""},{"location":"reference/operation-reference/#stateless-transformation-operations_1","title":"Stateless Transformation Operations","text":"KSML Operation Kafka Streams Method Purpose filter <code>KStream.processValues()</code> / <code>KTable.filter()</code> Filter records based on conditions filterNot <code>KStream.processValues()</code> / <code>KTable.filterNot()</code> Filter out matching records flatMap <code>KStream.process()</code> Transform one record to multiple records map <code>KStream.process()</code> Transform both key and value mapKey <code>KStream.process()</code> Transform only the key mapValues <code>KStream.processValues()</code> / <code>KTable.transformValues()</code> Transform only the value selectKey <code>KStream.process()</code> Select new key from record content transformKey <code>KStream.process()</code> Transform key using custom function transformValue <code>KStream.processValues()</code> Transform value using custom function"},{"location":"reference/operation-reference/#format-conversion-operations_1","title":"Format Conversion Operations","text":"KSML Operation Kafka Streams Method Purpose convertKey <code>KStream.processValues()</code> Convert key data format convertValue <code>KStream.processValues()</code> Convert value data format"},{"location":"reference/operation-reference/#grouping-partitioning-operations_1","title":"Grouping &amp; Partitioning Operations","text":"KSML Operation Kafka Streams Method Purpose groupBy <code>KStream.groupBy()</code> / <code>KTable.groupBy()</code> Group by new key groupByKey <code>KStream.groupByKey()</code> Group by existing key repartition <code>KStream.repartition()</code> Redistribute across partitions"},{"location":"reference/operation-reference/#stateful-aggregation-operations_1","title":"Stateful Aggregation Operations","text":"KSML Operation Kafka Streams Method Purpose aggregate <code>KGroupedStream.aggregate()</code> / <code>KGroupedTable.aggregate()</code> Custom aggregation logic count <code>KGroupedStream.count()</code> / <code>KGroupedTable.count()</code> Count records per key reduce <code>KGroupedStream.reduce()</code> / <code>KGroupedTable.reduce()</code> Reduce to single value per key"},{"location":"reference/operation-reference/#join-operations_1","title":"Join Operations","text":"KSML Operation Kafka Streams Method Purpose join <code>KStream.join()</code> / <code>KTable.join()</code> Inner join streams/tables leftJoin <code>KStream.leftJoin()</code> / <code>KTable.leftJoin()</code> Left outer join streams/tables merge <code>KStream.merge()</code> Merge multiple streams into one outerJoin <code>KStream.outerJoin()</code> / <code>KTable.outerJoin()</code> Full outer join streams/tables"},{"location":"reference/operation-reference/#windowing-operations_1","title":"Windowing Operations","text":"KSML Operation Kafka Streams Method Purpose windowBySession <code>KGroupedStream.windowedBy(SessionWindows)</code> Session-based windowing windowByTime <code>KGroupedStream.windowedBy(TimeWindows)</code> Time-based windowing"},{"location":"reference/operation-reference/#output-operations_1","title":"Output Operations","text":"KSML Operation Kafka Streams Method Purpose forEach <code>KStream.processValues()</code> Side effects without output print <code>KStream.processValues()</code> Print to stdout/file to <code>KStream.to()</code> Send to Kafka topic toTopicNameExtractor <code>KStream.to(TopicNameExtractor)</code> Dynamic topic routing"},{"location":"reference/operation-reference/#control-flow-operations_1","title":"Control Flow Operations","text":"KSML Operation Kafka Streams Method Purpose branch <code>KStream.split()</code> Split into multiple branches peek <code>KStream.processValues()</code> Observe records without changes"},{"location":"reference/operation-reference/#key-implementation-details","title":"Key Implementation Details","text":"<ul> <li>Most KSML operations use <code>KStream.process()</code> or <code>KStream.processValues()</code> with custom processor suppliers rather than direct DSL methods. This enables seamless integration with KSML's Python function execution system.</li> <li>Operations automatically adapt to work with KStream, KTable, and windowed streams, mapping to the appropriate Kafka Streams method based on context.</li> <li>Stateful operations support configurable state stores through KSML's unified state management system.</li> <li>Each operation integrates with Python functions through specialized user function wrappers (<code>UserPredicate</code>, <code>UserKeyTransformer</code>, etc.).</li> </ul>"},{"location":"reference/pipeline-reference/","title":"Pipeline Reference","text":"<p>This page provides comprehensive reference documentation for pipelines in KSML - the heart of stream processing logic.</p>"},{"location":"reference/pipeline-reference/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>In KSML, a pipeline defines how data flows from one or more input streams through a series of operations and finally to one or more output destinations.</p> <p>Think of a pipeline as a recipe that describes exactly how data should be processed from start to finish.</p> <p>Pipelines connect:</p> <ul> <li>Sources: Where data comes from (Kafka topics via streams, tables, or global tables)</li> <li>Operations: What happens to the data (transformations, filters, aggregations, etc.)</li> <li>Sinks: Where the processed data goes (output topics, other pipelines, functions, etc.)</li> </ul>"},{"location":"reference/pipeline-reference/#pipeline-structure","title":"Pipeline Structure","text":"<p>Every KSML pipeline has a consistent structure with three main components:</p> <pre><code>pipelines:\n  my_pipeline_name:\n    from: input_stream_name    # Source\n    via:                       # Operations (optional)\n      - type: operation1\n        # operation parameters\n      - type: operation2\n        # operation parameters\n    to: output_stream_name     # Sink\n</code></pre> <p>This structure makes pipelines easy to read and understand, with a clear flow from source to operations to sink.</p>"},{"location":"reference/pipeline-reference/#pipeline-properties","title":"Pipeline Properties","text":"<p>Each pipeline in the <code>pipelines</code> section has these main properties:</p> Property Type Required Description <code>name</code> String No Pipeline name. If not defined, derived from context <code>from</code> String/Object Yes The source stream(s), table(s), or pipeline(s). Can be a reference or inline definition <code>via</code> Array No List of operations to apply to the data <code>to</code> String/Object No* The destination stream or topic. Can be a reference or inline definition <code>as</code> String No* Name to save the result for later pipelines <code>branch</code> Array No* Split the pipeline based on conditions <code>forEach</code> Object No* Process each message with a function <code>print</code> Boolean/Object No* Output messages for debugging (simple boolean or complex object) <code>toTopicNameExtractor</code> Object No* Dynamically determine the output topic <p>*At least one sink type (<code>to</code>, <code>as</code>, <code>branch</code>, <code>forEach</code>, <code>print</code>, or <code>toTopicNameExtractor</code>) is required.</p>"},{"location":"reference/pipeline-reference/#pipeline-components","title":"Pipeline Components","text":""},{"location":"reference/pipeline-reference/#sources-where-data-comes-from","title":"Sources: Where Data Comes From","text":"<p>The source of a pipeline is specified with the <code>from</code> keyword. It defines where the pipeline gets its data from:</p> Parameter Type Required Description <code>from</code> String/Object Yes The source stream, table, or pipeline name. Can be a reference or inline definition <pre><code>from: user_clicks_stream\n</code></pre> <p>A source can be:</p> <ul> <li>A stream (KStream): For event-based processing</li> <li>A table (KTable): For state-based processing</li> <li>A globalTable (GlobalKTable): For reference data</li> <li>Another pipeline: For chaining pipelines together</li> </ul> <p>Sources must be defined elsewhere in your KSML file (in the <code>streams</code>, <code>tables</code>, or <code>globalTables</code> sections) or be the result of a previous pipeline.</p> <p>Full example with <code>from</code></p> <ul> <li>Example with <code>from</code></li> </ul> <p>Note: KSML does not support multiple sources in the <code>from</code> field. For joins, use a single source and specify the join target using the <code>table</code> parameter in join operations:</p> <pre><code># Correct syntax for joins\nfrom: orders_stream\nvia:\n  - type: join\n    table: customers_table  # Join target specified here\n    valueJoiner: join_function\n</code></pre> <p>Learn more about joins</p> <ul> <li>Joins` tutorial</li> </ul>"},{"location":"reference/pipeline-reference/#operations-transforming-the-data","title":"Operations: Transforming the Data","text":"<p>Operations are defined in the <code>via</code> section as a list of transformations to apply to the data:</p> Parameter Type Required Description <code>via</code> Array No List of operations to apply to the data <pre><code>via:\n  - type: filter\n    if:\n      expression: value.get('amount') &gt; 100\n  - type: mapValues\n    mapper:\n      expression: {\"user\": key, \"amount\": value.get('amount'), \"currency\": \"USD\"}\n</code></pre> <p>Each operation:</p> <ul> <li>Has a <code>type</code> that defines what it does</li> <li>Takes parameters specific to that operation type</li> <li>Receives data from the previous operation (or the source)</li> <li>Passes its output to the next operation (or the sink)</li> </ul> <p>Operations are applied in sequence, creating a processing pipeline where data flows from one operation to the next.</p> <p>For a complete list of available operations, see the Operation Reference.</p> <p>Full example with <code>via</code></p> <ul> <li>Example with <code>via</code></li> </ul>"},{"location":"reference/pipeline-reference/#sinks-where-data-goes","title":"Sinks: Where Data Goes","text":"<p>The sink defines where the processed data should be sent. KSML supports several sink types:</p> Parameter Type Required Description <code>to</code> String/Object No* The destination stream or topic name. Can be a reference or inline definition <code>as</code> String No* Name to save the result for use in later pipelines <code>branch</code> Array No* Split the pipeline based on conditions <code>forEach</code> Object No* Process each message with a function (terminal operation) <code>print</code> Boolean/Object No* Output messages for debugging (simple boolean or complex object with filename, label, mapper) <code>toTopicNameExtractor</code> Object No* Dynamically determine the output topic <p>*At least one sink type is required.</p> <pre><code>to: processed_orders_stream  # Send to a predefined stream\n</code></pre> <p>Or more complex sinks:</p> <pre><code>branch:  # Split the pipeline based on conditions\n  - if:\n      expression: value.get('type') == 'purchase'\n    to: purchases_stream\n  - if:\n      expression: value.get('type') == 'refund'\n    to: refunds_stream\n  - to: other_events_stream  # Default branch\n</code></pre> <pre><code>toTopicNameExtractor:  # Dynamic topic routing\n  expression: \"events-\" + value.get('category').lower()\n</code></pre> <p>Common sink types include:</p> <ul> <li><code>to</code>: Send to a predefined stream, table, or topic</li> <li><code>as</code>: Save the result under a name for use in later pipelines</li> <li><code>branch</code>: Split the pipeline based on conditions</li> <li><code>forEach</code>: Process each message with a function (terminal operation)</li> <li><code>print</code>: Output messages for debugging</li> <li><code>toTopicNameExtractor</code>: Dynamically determine the output topic</li> </ul> <p>Examples:</p> <ul> <li><code>to</code>, <code>as</code>: Example with <code>to</code>, <code>as</code></li> <li><code>branch</code>: Example with <code>branch</code> </li> <li><code>forEach</code>, <code>print</code>, <code>toTopicNameExtractor</code>: Operation examples</li> </ul>"},{"location":"reference/pipeline-reference/#pipeline-patterns-and-techniques","title":"Pipeline Patterns and Techniques","text":""},{"location":"reference/pipeline-reference/#when-to-use-via-vs-multiple-pipelines","title":"When to Use <code>via</code> vs Multiple Pipelines","text":"<p>One of the decisions in KSML is whether to use a single pipeline with multiple <code>via</code> operations or break processing into multiple connected pipelines. Here's when to use each approach:</p>"},{"location":"reference/pipeline-reference/#use-via-single-pipeline-when","title":"Use <code>via</code> (Single Pipeline) When:","text":"<p>\u2705 Operations are tightly coupled and belong together logically <pre><code># Good: One logical process - \"validate and enrich user data\"\npipelines:\n  validate_and_enrich_user:\n    from: raw_user_data\n    via:\n      - type: filter           # Remove invalid data\n        if: \n          expression: value.get('email') is not None\n      - type: transformValue   # Add computed field based on validation\n        mapper: add_user_status\n      - type: peek            # Log the final result\n        forEach:\n          code: |\n            log.info(\"Processed user: {}\", key)\n    to: valid_users\n</code></pre></p> <p>\u2705 Simple sequential processing with no reuse needs <pre><code># Good: Straightforward data transformation\npipelines:\n  format_sensor_data:\n    from: raw_sensor_readings\n    via:\n      - type: transformValue\n        mapper: convert_temperature_units\n      - type: transformValue  \n        mapper: add_timestamp_formatting\n      - type: filter\n        if:\n          expression: value.get('temperature') &gt; -50\n    to: formatted_sensor_data\n</code></pre></p>"},{"location":"reference/pipeline-reference/#use-multiple-pipelines-when","title":"Use Multiple Pipelines When:","text":"<p>\u2705 You need to reuse intermediate results <pre><code># Good: Filtered data used by multiple downstream processes\npipelines:\n  filter_high_value_orders:\n    from: all_orders\n    via:\n      - type: filter\n        if:\n          expression: value.get('total') &gt; 1000\n    as: high_value_orders  # Save for reuse\n\n  send_vip_notifications:\n    from: high_value_orders  # Reuse filtered data\n    via:\n      - type: transformValue\n        mapper: create_vip_notification\n    to: vip_notifications\n\n  update_customer_tier:\n    from: high_value_orders  # Reuse same filtered data\n    via:\n      - type: transformValue\n        mapper: calculate_customer_tier\n    to: customer_tier_updates\n</code></pre></p> <p>\u2705 Different processing responsibilities should be separated <pre><code># Good: Separate data cleaning from business logic\npipelines:\n  # Responsibility: Data standardization and validation\n  data_cleaning:\n    from: raw_transactions\n    via:\n      - type: filter\n        if:\n          expression: value.get('amount') &gt; 0\n      - type: transformValue\n        mapper: standardize_format\n    as: clean_transactions\n\n  # Responsibility: Business rule application\n  fraud_detection:\n    from: clean_transactions\n    via:\n      - type: transformValue\n        mapper: calculate_fraud_score\n      - type: filter\n        if:\n          expression: value.get('fraud_score') &lt; 0.8\n    to: verified_transactions\n</code></pre></p>"},{"location":"reference/pipeline-reference/#connecting-pipelines","title":"Connecting Pipelines","text":"<p>KSML makes it easy to connect pipelines together. The key is using the <code>as:</code> parameter to save intermediate results:</p> <pre><code>pipelines:\n  # Pipeline 1: Save intermediate result with 'as:'\n  filter_high_value_orders:\n    from: orders_stream\n    via:\n      - type: filter\n        if:\n          expression: value.get('total') &gt; 1000\n    as: high_value_orders  # \u2190 Save result for other pipelines\n\n  # Pipeline 2: Use the saved result as input\n  process_high_value_orders:\n    from: high_value_orders  # \u2190 Reference the saved result\n    via:\n      - type: transformValue\n        mapper:\n          expression: {\"orderId\": value.get('id'), \"amount\": value.get('total'), \"priority\": \"high\"}\n    to: priority_orders_stream\n</code></pre> <p>When to connect pipelines:</p> <ul> <li>Multiple consumers need the same filtered/processed data</li> <li>Different processing responsibilities should be separated  </li> <li>Complex logic benefits from being broken into stages</li> </ul> <p>When to use <code>via</code> instead:</p> <ul> <li>Operations are part of one business process</li> <li>No need to reuse intermediate results</li> <li>You want lower latency (no intermediate topics)</li> </ul>"},{"location":"reference/pipeline-reference/#branching-pipelines","title":"Branching Pipelines","text":"<p>The <code>branch</code> sink allows you to split a pipeline based on conditions:</p> <pre><code>pipelines:\n  route_messages_by_type:\n    from: events_stream\n    branch:\n      - if:\n          expression: value.get('type') == 'click'\n        to: clicks_stream\n      - if:\n          expression: value.get('type') == 'purchase'\n        to: purchases_stream\n      - if:\n          expression: value.get('type') == 'login'\n        to: logins_stream\n      - to: other_events_stream  # Default branch for anything else\n</code></pre> <p>This is useful for:</p> <ul> <li>Routing different types of events to different destinations</li> <li>Implementing content-based routing patterns</li> <li>Creating specialized processing flows for different data types</li> </ul>"},{"location":"reference/pipeline-reference/#dynamic-output-topics","title":"Dynamic Output Topics","text":"<p>The <code>toTopicNameExtractor</code> sink allows you to dynamically determine the output topic:</p> <pre><code>pipelines:\n  route_to_dynamic_topics:\n    from: events_stream\n    toTopicNameExtractor:\n      expression: \"events-\" + value.get('category').lower()\n</code></pre> <p>This is useful for:</p> <ul> <li>Partitioning data across multiple topics</li> <li>Creating topic hierarchies</li> <li>Implementing multi-tenant architectures</li> </ul> <p>Full example with <code>toTopicNameExtractor</code></p> <ul> <li>Example with <code>toTopicNameExtractor</code></li> </ul>"},{"location":"reference/pipeline-reference/#stream-and-table-configuration","title":"Stream and Table Configuration","text":"<p>Pipelines reference streams, tables, and globalTables that can be defined in two ways:</p>"},{"location":"reference/pipeline-reference/#1-pre-defined-referenced-by-name","title":"1. Pre-defined (Referenced by Name)","text":"<p>Define once, use multiple times:</p> <pre><code>streams:\n  sensor_source:\n    topic: ksml_sensordata\n    keyType: string\n    valueType: avro:SensorData\n    offsetResetPolicy: latest\n\npipelines:\n  my_pipeline:\n    from: sensor_source  # Reference by name\n    to: processed_output\n</code></pre>"},{"location":"reference/pipeline-reference/#2-inline-definition","title":"2. Inline Definition","text":"<p>Define directly where needed:</p> <pre><code>pipelines:\n  my_pipeline:\n    from:\n      topic: ksml_sensordata  # Inline definition\n      keyType: string\n      valueType: avro:SensorData\n      offsetResetPolicy: latest\n    to:\n      topic: processed_output\n      keyType: string\n      valueType: json\n</code></pre> <p>For comprehensive documentation on configuring streams, tables, and their properties (offsetResetPolicy, timestampExtractor, partitioner, etc.), see:</p> <p>\u2192 Data Sources and Targets - Complete Reference</p>"},{"location":"reference/pipeline-reference/#duration-specification-in-pipeline","title":"Duration Specification in pipeline","text":"<p>Some pipeline operations require specifying time durations. In KSML, durations are expressed using a simple format:</p> <pre><code>###x\n</code></pre> <p>Where:</p> <ul> <li><code>###</code> is a positive number</li> <li><code>x</code> is an optional time unit:<ul> <li><code>ms</code>: milliseconds (this is also the default if no unit is specified)</li> <li><code>s</code>: seconds</li> <li><code>m</code>: minutes</li> <li><code>h</code>: hours</li> <li><code>d</code>: days</li> <li><code>w</code>: weeks</li> </ul> </li> </ul> <p>Examples: <pre><code>100   # 100 milliseconds\n500ms # 500 milliseconds\n30s   # 30 seconds\n5m    # 5 minutes\n2h    # 2 hours\n1d    # 1 day\n4w    # 4 weeks\n</code></pre></p> <p>Durations are commonly used in windowing operations:</p> <pre><code>- type: windowByTime\n  windowType: hopping\n  duration: 5m        # 5-minute windows\n  advanceBy: 1m       # Advancing every minute (sliding window)\n</code></pre> <p>Full examples for <code>duration</code></p> <ul> <li>Example with <code>duration</code></li> </ul>"},{"location":"reference/pipeline-reference/#state-store-specification-in-pipelines","title":"State Store Specification in Pipelines","text":"<p>State stores maintain data across multiple messages, enabling stateful operations like aggregations, counts, and joins. They can be defined in two ways:</p>"},{"location":"reference/pipeline-reference/#1-inline-state-store-within-operations","title":"1. Inline State Store (Within Operations)","text":"<p>Define directly in stateful operations when you need custom store configuration:</p> <pre><code>pipelines:\n  count_clicks_5min:\n    from: user_clicks\n    via:\n      - type: groupByKey\n      - type: windowByTime\n        windowType: tumbling\n        duration: 5m\n        grace: 30s\n      - type: count\n        store:                    # Inline store definition\n          name: click_counts_5min\n          type: window            # Window store for windowed aggregations\n          windowSize: 5m          # Must match window duration\n          retention: 30m          # How long to keep window data\n          caching: false          # Disable caching for real-time updates\n</code></pre>"},{"location":"reference/pipeline-reference/#see-it-in-action","title":"See it in action:","text":"<ul> <li>Tutorial: Windowing</li> </ul>"},{"location":"reference/pipeline-reference/#2-pre-defined-state-store-in-stores-section","title":"2. Pre-defined State Store (In <code>stores</code> Section)","text":"<p>Define once for reuse across multiple operations or direct access in functions:</p> <pre><code>stores:\n  stats_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n\nfunctions:\n  detect_anomalies:\n    type: valueTransformer\n    stores:\n      - stats_store\n</code></pre>"},{"location":"reference/pipeline-reference/#see-pre-defined-state-store-in-action","title":"See pre-defined state store in action:","text":"<ul> <li>Example for pre-defined state store: Anomaly Detection</li> </ul>"},{"location":"reference/pipeline-reference/#examples","title":"Examples","text":"<p>Full example with windowed state store:</p> <ul> <li>Windowed Aggregations with State Stores</li> </ul> <p>Comprehensive state store documentation:</p> <ul> <li>State Stores Reference</li> </ul>"},{"location":"reference/state-store-reference/","title":"State Store Reference","text":"<p>State stores enable stateful stream processing in KSML by maintaining data across multiple messages. This reference covers state store configuration, usage patterns, and best practices.</p> <p>For hands-on tutorials, please check out:</p> <ul> <li>State Stores Intermediate Tutorial</li> <li>Custom State Stores Advanced Tutorial </li> </ul>"},{"location":"reference/state-store-reference/#what-are-state-stores","title":"What are State Stores?","text":"<p>State stores allow KSML applications to:</p> <ul> <li>Maintain state between messages for aggregations, counts, and joins</li> <li>Track historical information for context-aware processing  </li> <li>Build complex business logic that depends on previous events</li> </ul>"},{"location":"reference/state-store-reference/#state-store-types","title":"State Store Types","text":"<p>KSML supports three types of state stores, each optimized for specific use cases:</p> Type Description Use Cases Examples <code>keyValue</code> Simple key-value storage General lookups, non-windowed aggregations, manual state management <code>keyValue</code> type state store <code>window</code> Time-windowed storage with automatic expiry Time-based aggregations, windowed joins, temporal analytics <code>window</code> type state store <code>session</code> Session-based storage with activity gaps User session tracking, activity-based grouping <code>session</code> type state store"},{"location":"reference/state-store-reference/#configuration-methods","title":"Configuration Methods","text":""},{"location":"reference/state-store-reference/#1-pre-defined-state-stores","title":"1. Pre-defined State Stores","text":"<p>Define once in the <code>stores</code> section, reuse across operations:</p> <pre><code>stores:\n  owner_count_store:\n    type: keyValue\n    keyType: string\n    valueType: long\n    caching: false\n    persistent: true\n    logging: true\n\npipelines:\n</code></pre> <pre><code>stores:\n  owner_count_store:\n    type: keyValue\n    keyType: string\n    valueType: long\n    caching: false\n    persistent: true\n    logging: true\n\npipelines:\n</code></pre> <p>Full example:</p> <ul> <li>Pre-defined State Stores Tutorial</li> </ul>"},{"location":"reference/state-store-reference/#2-inline-state-stores","title":"2. Inline State Stores","text":"<p>Define directly within operations for custom configurations:</p> <pre><code>      - type: aggregate\n        store:\n          name: sensor_type_aggregates\n          type: keyValue\n          caching: true\n          persistent: false\n          logging: false\n        initializer: initialize_sum\n        aggregator: update_sum\n      - type: toStream\n</code></pre> <p>Full example:</p> <ul> <li>Inline State Stores Tutorial</li> </ul>"},{"location":"reference/state-store-reference/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"reference/state-store-reference/#common-parameters-all-store-types","title":"Common Parameters (All Store Types)","text":"Parameter Type Required Default Description <code>name</code> String No Operation name Unique identifier for the state store <code>type</code> String Yes - Store type: <code>keyValue</code>, <code>window</code>, or <code>session</code> <code>keyType</code> String Yes - Data type for keys (see Data Types Reference) <code>valueType</code> String Yes - Data type for values <code>persistent</code> Boolean No <code>false</code> If <code>true</code>, uses RocksDB (disk); if <code>false</code>, uses in-memory storage <code>caching</code> Boolean No <code>false</code> If <code>true</code>, improves read performance but delays updates <code>logging</code> Boolean No <code>false</code> If <code>true</code>, creates changelog topic for fault tolerance (in addition to local storage) <code>timestamped</code> Boolean No <code>false</code> If <code>true</code>, stores timestamp with each entry"},{"location":"reference/state-store-reference/#window-store-specific-parameters","title":"Window Store Specific Parameters","text":"Parameter Type Required Default Description <code>windowSize</code> Duration Yes - Size of the time window (must match operation's window duration) <code>retention</code> Duration No - How long to retain window data (should be &gt; windowSize + grace period) <code>retainDuplicates</code> Boolean No <code>false</code> Whether to keep duplicate entries in windows"},{"location":"reference/state-store-reference/#keyvalue-store-specific-parameters","title":"KeyValue Store Specific Parameters","text":"Parameter Type Required Default Description <code>versioned</code> Boolean No <code>false</code> If <code>true</code>, maintains version history of values <code>historyRetention</code> Duration No (Yes if versioned) - How long to keep old versions <code>segmentInterval</code> Duration No - Segment size for versioned stores <p>Important: Versioned stores (<code>versioned: true</code>) cannot have caching enabled (<code>caching: false</code> is required).</p>"},{"location":"reference/state-store-reference/#session-store-specific-parameters","title":"Session Store Specific Parameters","text":"Parameter Type Required Default Description <code>retention</code> Duration No - How long to retain session data"},{"location":"reference/state-store-reference/#usage-in-operations","title":"Usage in Operations","text":""},{"location":"reference/state-store-reference/#with-aggregations","title":"With Aggregations","text":"<p>State stores are automatically used by aggregation operations:</p> <pre><code>pipelines:\n  calculate_statistics:\n    from: payment_stream\n    via:\n      - type: groupByKey\n      - type: aggregate\n        store:\n          type: keyValue\n          retention: 1h\n          caching: false\n</code></pre> <p>Full example:</p> <ul> <li>Aggregations with State Stores</li> </ul>"},{"location":"reference/state-store-reference/#with-manual-state-access","title":"With Manual State Access","text":"<p>Access state stores directly in functions for custom caching, enrichment, and state management:</p> <pre><code>stores:\n  user_profile_cache:\n    name: user_profile_cache\n    type: keyValue\n    keyType: string\n    valueType: json\n    persistent: true\n    logging: true\n    retention: 1h\n</code></pre> <pre><code>functions:\n  enrich_and_cache:\n    type: valueTransformer\n    code: |\n      # Get cached data for this user\n      cached_data = user_profile_cache.get(key)\n</code></pre> <p>Full example</p> State Store with Manual State Access (click to expand) <pre><code># User events producer for manual state store access example\n# Generates user activity events to demonstrate cache enrichment\n\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  generate_user_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n\n      users = [\"user_001\", \"user_002\", \"user_003\", \"user_004\", \"user_005\"]\n      actions = [\"login\", \"view_product\", \"add_to_cart\", \"checkout\", \"logout\"]\n      products = [\"laptop\", \"phone\", \"tablet\", \"monitor\", \"keyboard\"]\n      event_counter = 0\n    code: |\n      global event_counter\n      event_counter += 1\n\n      user_id = random.choice(users)\n      action = random.choice(actions)\n\n      event = {\n        \"event_id\": f\"evt_{event_counter}\",\n        \"user_id\": user_id,\n        \"action\": action,\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Add product info for relevant actions\n      if action in [\"view_product\", \"add_to_cart\"]:\n        event[\"product\"] = random.choice(products)\n        event[\"price\"] = round(random.uniform(50, 2000), 2)\n\n      return (user_id, event)\n    resultType: (string, json)\n\nproducers:\n  user_event_generator:\n    generator: generate_user_events\n    interval: 2s\n    to: user_events\n</code></pre> State Store with Manual State Access (click to expand) <pre><code># Manual state store access example\n# Demonstrates how to directly access state stores in functions for enrichment and caching\n\nstreams:\n  user_events_input:\n    topic: user_events\n    keyType: string\n    valueType: json\n    offsetResetPolicy: latest\n\n  enriched_events:\n    topic: enriched_user_events\n    keyType: string\n    valueType: json\n\nstores:\n  user_profile_cache:\n    type: keyValue\n    keyType: string\n    valueType: json\n    persistent: true\n    logging: true\n    historyRetention: 1h\n\nfunctions:\n  enrich_and_cache:\n    type: valueTransformer\n    code: |\n      # Get cached data for this user\n      cached_data = user_profile_cache.get(key)\n\n      # Log the state access\n      if cached_data is not None:\n        log.info(\"Found cached data for user: {}\", key)\n      else:\n        log.info(\"No cached data found for user: {}\", key)\n\n      # Store the current event data in cache\n      user_profile_cache.put(key, value)\n\n      # Create enriched result\n      result = {\n        \"event_id\": value.get(\"event_id\"),\n        \"user_id\": value.get(\"user_id\"),\n        \"action\": value.get(\"action\"),\n        \"timestamp\": value.get(\"timestamp\"),\n        \"has_cached_data\": cached_data is not None\n      }\n\n      # Copy product fields if present\n      if value.get(\"product\") is not None:\n        result[\"product\"] = value.get(\"product\")\n        result[\"price\"] = value.get(\"price\")\n    expression: result\n    resultType: json\n    stores:\n      - user_profile_cache\n\npipelines:\n  enrich_user_events:\n    from: user_events_input\n    via:\n      - type: transformValue\n        mapper: enrich_and_cache\n\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"ENRICHED EVENT - User: {}, Action: {}, Has Cached Data: {}\", \n                     key, \n                     value.get(\"action\"),\n                     value.get(\"has_cached_data\"))\n\n    to: enriched_events\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Store declaration: List store names in the function's <code>stores</code> parameter</li> <li>State retrieval: Use <code>store_name.get(key)</code> to read cached data</li> <li>State storage: Use <code>store_name.put(key, value)</code> to update cache</li> <li>Null handling: Check if <code>cached_data is not None</code> for first-time detection</li> </ul> <p>Key functions:</p> <ul> <li><code>store.get(key)</code> - Retrieve value from state store (returns None if not found)</li> <li><code>store.put(key, value)</code> - Store key-value pair in state store</li> </ul>"},{"location":"reference/state-store-reference/#with-session-operations","title":"With Session Operations","text":"<p>Session stores track user activity with inactivity gaps and automatic timeout: <pre><code>pipelines:\n  user_session_analysis:\n    from: user_clicks\n    via:\n      - type: groupByKey\n      - type: windowBySession\n        inactivityGap: 2m  # Close session after 2 minutes of inactivity\n        grace: 30s\n      - type: count\n        store:\n          name: user_sessions\n          type: session\n          retention: 1h\n          caching: false\n</code></pre></p> <p>Full example:</p> <ul> <li>Session store type example</li> </ul> <p>Session store patterns:</p> <ul> <li>Activity tracking: Monitor user engagement and session duration</li> <li>Timeout detection: Identify inactive sessions for cleanup</li> <li>State aggregation: Accumulate metrics within session boundaries</li> </ul>"},{"location":"reference/state-store-reference/#with-windowed-operations","title":"With Windowed Operations","text":"<p>Window stores require specific configuration that varies by operation type:</p>"},{"location":"reference/state-store-reference/#for-aggregations-count-aggregate-etc","title":"For Aggregations (Count, Aggregate, etc.)","text":"<pre><code>pipelines:\n  count_clicks_5min:\n    from: user_clicks\n    via:\n      - type: groupByKey\n      - type: windowByTime\n        windowType: tumbling\n        duration: 5m\n        grace: 30s\n      - type: count\n        store:\n          name: click_counts_5min\n          type: window\n          windowSize: 5m          # Must match window duration\n          retention: 35m          # windowSize + grace + buffer\n          caching: false\n</code></pre>"},{"location":"reference/state-store-reference/#for-stream-stream-joins","title":"For Stream-Stream Joins","text":"<pre><code>pipelines:\n  join_streams:\n    from: stream1\n    via:\n      - type: join\n        stream: stream2\n        timeDifference: 15m\n        grace: 5m\n        thisStore:\n          type: window\n          windowSize: 30m         # Must be 2 \u00d7 timeDifference\n          retention: 35m          # 2 \u00d7 timeDifference + grace\n          retainDuplicates: true\n</code></pre>"},{"location":"reference/state-store-reference/#retention-guidelines","title":"Retention Guidelines","text":"<p>For Window Aggregations: <pre><code>retention = windowSize + gracePeriod + processingBuffer\n</code></pre></p> <p>For Stream-Stream Joins: <pre><code>windowSize = 2 \u00d7 timeDifference\nretention = 2 \u00d7 timeDifference + gracePeriod\n</code></pre></p> <p>Full examples:</p> <ul> <li>Windowed Aggregations</li> <li>Stream-Stream Joins</li> </ul>"},{"location":"reference/state-store-reference/#storage-architecture","title":"Storage Architecture","text":""},{"location":"reference/state-store-reference/#local-storage-vs-changelog-topics","title":"Local Storage vs Changelog Topics","text":"<p>It's important to understand that <code>persistent</code> and <code>logging</code> control different aspects of state storage:</p> <ol> <li><code>persistent: true</code> \u2192 Uses RocksDB for local storage (survives process restarts)</li> <li><code>persistent: false</code> \u2192 Uses in-memory storage (lost on process restart)</li> <li><code>logging: true</code> \u2192 ADDITIONALLY replicates state to a Kafka changelog topic</li> <li><code>logging: false</code> \u2192 No changelog topic (only local storage)</li> </ol> <p>Key Point: When <code>logging: true</code>, state is stored BOTH locally (RocksDB or memory) AND in the changelog topic. The changelog is for fault tolerance and recovery, not primary storage.</p>"},{"location":"reference/state-store-reference/#common-configuration-patterns","title":"Common Configuration Patterns","text":"Configuration Description Use Case <code>persistent: true, logging: false</code> RocksDB only (local durability, no fault tolerance) Development, non-critical state <code>persistent: true, logging: true</code> RocksDB + changelog (local durability + fault tolerance) Production (Recommended) \u2705 <code>persistent: false, logging: true</code> In-memory + changelog (fast access, fault tolerant) High-performance with recovery <code>persistent: false, logging: false</code> In-memory only (fastest, data lost on restart) Temporary caches, testing"},{"location":"reference/state-store-reference/#changelog-topics","title":"Changelog Topics","text":"<p>When <code>logging: true</code> is configured:</p> <ul> <li>Topic Name: <code>&lt;application.id&gt;-&lt;store-name&gt;-changelog</code></li> <li>Compaction: Topics are log-compacted to keep only the latest value per key</li> <li>Recovery: On restart/rebalance, missing local state is rebuilt from the changelog</li> <li>Performance: Normal reads use local storage (fast), changelog is only for recovery</li> <li>Requirements: Required for exactly-once semantics</li> </ul>"},{"location":"reference/state-store-reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/state-store-reference/#caching-impact","title":"Caching Impact","text":"Setting Behavior Use When <code>caching: true</code> Batches updates, reduces downstream load High-throughput with acceptable latency <code>caching: false</code> Immediate emission of all updates Real-time requirements, debugging"},{"location":"reference/state-store-reference/#persistence-trade-offs","title":"Persistence Trade-offs","text":"Setting Pros Cons Use When <code>persistent: true</code> Survives restarts, enables recovery Slower, uses disk space Production, critical state <code>persistent: false</code> Fast, memory-only Lost on restart Temporary state, caching"},{"location":"reference/state-store-reference/#logging-impact","title":"Logging Impact","text":"Setting Pros Cons Use When <code>logging: true</code> Fault tolerance, fast recovery, exactly-once support Additional Kafka topics, network/storage overhead Production, fault tolerance needed <code>logging: false</code> Lower overhead, simpler setup No recovery on failure Development, non-critical state <p>Important: Changelog logging supplements local storage - it does NOT replace it. State is always stored locally (RocksDB or memory) for fast access, with the changelog used only for recovery purposes.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the KSML tutorials and guides section. This section provides a progressive learning path from beginner to advanced topics, as well as practical use case guides.</p>"},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":""},{"location":"tutorials/#quick-start","title":"Quick Start","text":"<p>This tutorial will help you understand what KSML is, how to set it up, and how to create your first KSML application.</p>"},{"location":"tutorials/#beginner-tutorials","title":"Beginner Tutorials","text":"<p>When you've installed KSML and are ready to develop, these tutorials cover the basics of building simple data pipelines, filtering and transforming data, and implementing logging and monitoring.</p>"},{"location":"tutorials/#intermediate-tutorials","title":"Intermediate Tutorials","text":"<p>Once you're comfortable with the basics, move on to these tutorials to learn about aggregations, joins, windowed operations, and error handling.</p>"},{"location":"tutorials/#advanced-tutorials","title":"Advanced Tutorials","text":"<p>These tutorials cover complex topics such as complex event processing, custom state stores, performance optimization, and external integration.</p>"},{"location":"tutorials/#how-to-use-these-tutorials","title":"How to Use These Tutorials","text":"<p>Each tutorial builds on concepts from previous tutorials, so we recommend following them in order within each learning path. Each tutorial includes:</p> <ol> <li>Objectives: What you'll learn</li> <li>Prerequisites: What you need to know before starting</li> <li>Step-by-step instructions: Detailed walkthrough with explanations</li> <li>Complete examples: Working code you can run and modify</li> <li>Next steps: Suggestions for what to learn next</li> </ol>"},{"location":"tutorials/advanced/","title":"Advanced Tutorials","text":"<p>Welcome to the KSML advanced tutorials! These tutorials are designed for users who have mastered the beginner and intermediate concepts and are ready to explore the most powerful and complex features of KSML.</p> <p>These tutorials will help you build sophisticated, high-performance data processing applications and integrate KSML with other systems and technologies.</p>"},{"location":"tutorials/advanced/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/advanced/#complex-event-processing","title":"Complex Event Processing","text":"<p>Learn how to implement complex event processing patterns with KSML:</p> <ul> <li>Pattern detection across multiple streams</li> <li>Temporal pattern matching</li> <li>Event correlation and enrichment</li> <li>Building event-driven applications</li> </ul>"},{"location":"tutorials/advanced/#custom-state-stores","title":"Custom State Stores","text":"<p>This tutorial covers advanced state management techniques:</p> <ul> <li>Implementing custom state stores</li> <li>Optimizing state access patterns</li> <li>Managing state store size and performance</li> <li>Implementing custom serialization/deserialization</li> </ul>"},{"location":"tutorials/advanced/#performance-optimization","title":"Performance Optimization","text":"<p>Learn how to optimize your KSML applications for maximum performance:</p> <ul> <li>Identifying bottlenecks</li> <li>Optimizing serialization/deserialization</li> <li>Tuning Kafka Streams configurations</li> <li>Scaling strategies</li> <li>Monitoring performance metrics</li> </ul>"},{"location":"tutorials/advanced/#integration-with-external-systems","title":"Integration with External Systems","text":"<p>This tutorial focuses on integrating KSML with other systems:</p> <ul> <li>Connecting to databases</li> <li>Integrating with REST APIs</li> <li>Working with message queues</li> <li>Implementing the Kafka Connect pattern in KSML</li> </ul>"},{"location":"tutorials/advanced/#learning-path","title":"Learning Path","text":"<p>These advanced tutorials can be approached based on your specific needs and interests. Each tutorial is self-contained but may reference concepts from other advanced or intermediate tutorials.</p> <p>After completing these tutorials, you'll have a comprehensive understanding of KSML's capabilities and be able to implement complex, production-grade stream processing applications.</p>"},{"location":"tutorials/advanced/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered KSML, it's time to apply your knowledge to real-world scenarios:</p> <p>\ud83d\udc49 Explore Use Case Guides to see how KSML solves common business problems and get inspiration for your own projects.</p>"},{"location":"tutorials/advanced/complex-event-processing/","title":"Complex Event Processing in KSML","text":"<p>This tutorial explores how to implement complex event processing (CEP) patterns in KSML, allowing you to detect meaningful patterns across multiple events and streams in real-time.</p>"},{"location":"tutorials/advanced/complex-event-processing/#introduction-to-complex-event-processing","title":"Introduction to Complex Event Processing","text":"<p>Complex Event Processing (CEP) is a method of tracking and analyzing streams of data to identify patterns, correlate events, and derive higher-level insights. CEP enables real-time decision making by processing events as they occur rather than in batch.</p> <p>Key capabilities of CEP in KSML:</p> <ul> <li>Pattern detection: Identify sequences of events that form meaningful patterns</li> <li>Temporal analysis: Detect time-based patterns and relationships</li> <li>Event correlation: Connect related events from different sources</li> <li>Anomaly detection: Identify unusual patterns or deviations</li> <li>Stateful processing: Maintain context across multiple events</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code># Pattern Detection\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic pattern_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic detected_patterns &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic temporal_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic temporal_patterns &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic system_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic correlated_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_metrics &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic anomalies_detected &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic credit_card_transactions &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic fraud_alerts &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/advanced/complex-event-processing/#pattern-detection","title":"Pattern Detection","text":"<p>Pattern detection identifies specific sequences of events within a stream. This example detects an A\u2192B\u2192C pattern across events.</p> <p>What it does:</p> <ul> <li>Produces events: Creates events with types A, B, C, D, E - deliberately generates A\u2192B\u2192C sequences for session_001 to demonstrate pattern completion</li> <li>Tracks sequences: Uses a state store to remember where each session is in the A\u2192B\u2192C pattern (stores \"A\", \"AB\", or deletes when complete)</li> <li>Detects completion: When event C arrives and the state shows \"AB\", it recognizes the full A\u2192B\u2192C pattern is complete</li> <li>Outputs results: Only emits a detection message when the complete pattern A\u2192B\u2192C is found, otherwise filters out partial matches</li> <li>Resets state: Clears the pattern tracking after successful detection or if the sequence breaks (e.g., gets A\u2192D instead of A\u2192B)</li> </ul> Pattern Events Producer - click to expand <pre><code># Producer for pattern detection example - generates event sequences\n\nfunctions:\n  generate_pattern_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      patterns = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n      sessions = [\"session_001\", \"session_002\", \"session_003\"]\n    code: |\n      global event_counter, patterns, sessions\n\n      event_counter += 1\n      session_id = random.choice(sessions)\n\n      # Create pattern events - generate A-&gt;B-&gt;C sequence for specific sessions\n      if event_counter % 9 == 1:\n        event_type = \"A\"\n        session_id = \"session_001\"  # Force same session for pattern\n      elif event_counter % 9 == 2:\n        event_type = \"B\"\n        session_id = \"session_001\"  # Same session\n      elif event_counter % 9 == 3:\n        event_type = \"C\"\n        session_id = \"session_001\"  # Complete the pattern\n      elif event_counter % 9 in [4, 5]:  # Partial pattern\n        event_type = [\"A\", \"B\"][event_counter % 9 - 4]\n      else:\n        event_type = random.choice([\"D\", \"E\"])  # Other events\n\n      # Create structured JSON event for better readability in Kowl UI\n      event = {\n        \"event_id\": f\"evt_{event_counter:04d}\",\n        \"session_id\": session_id,\n        \"event_type\": event_type,\n        \"timestamp\": int(time.time() * 1000),\n        \"data\": f\"event_data_{event_type}\",\n        \"source\": \"pattern_generator\",\n        \"sequence_number\": event_counter,\n        \"metadata\": {\n          \"simulation\": True,\n          \"pattern_type\": \"abc_sequence\"\n        }\n      }\n\n      key = session_id\n      value = event\n\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  pattern_event_producer:\n    generator: generate_pattern_events\n    interval: 1s\n    to:\n      topic: pattern_events\n      keyType: string\n      valueType: json\n</code></pre> Pattern Detection Processor - click to expand <pre><code># Processor for pattern detection - detects A-&gt;B-&gt;C sequences\n\nstreams:\n  pattern_events:\n    topic: pattern_events\n    keyType: string\n    valueType: json\n\nstores:\n  pattern_tracker:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  detect_abc_pattern:\n    type: valueTransformer\n    globalCode: |\n      import time\n    stores:\n      - pattern_tracker\n    code: |\n      # Extract fields from JSON event\n      event_type = value.get(\"event_type\")\n      event_id = value.get(\"event_id\")\n      session_id = value.get(\"session_id\")\n      timestamp = value.get(\"timestamp\")\n      sequence_number = value.get(\"sequence_number\")\n      current_time = int(time.time() * 1000)\n\n      if not event_type or not event_id:\n        return None\n\n      # Get current pattern state\n      current_state = pattern_tracker.get(key) or \"\"\n\n      # Check for pattern completion\n      if event_type == \"A\":\n        # Start new pattern\n        pattern_tracker.put(key, \"A\")\n        log.info(\"Pattern started for {}: A\", key)\n        return None\n      elif event_type == \"B\" and current_state == \"A\":\n        # Continue pattern\n        pattern_tracker.put(key, \"AB\")\n        log.info(\"Pattern progressing for {}: A-&gt;B\", key)\n        return None\n      elif event_type == \"C\" and current_state == \"AB\":\n        # Pattern complete!\n        pattern_tracker.delete(key)\n        log.info(\"PATTERN DETECTED for {}: A-&gt;B-&gt;C\", key)\n\n        # Create structured JSON result\n        return {\n          \"pattern_type\": \"ABC_SEQUENCE\",\n          \"status\": \"DETECTED\",\n          \"session_id\": session_id,\n          \"completing_event\": {\n            \"event_id\": event_id,\n            \"event_type\": event_type,\n            \"timestamp\": timestamp,\n            \"sequence_number\": sequence_number\n          },\n          \"detection_timestamp\": current_time,\n          \"processing_time\": current_time - timestamp,\n          \"metadata\": {\n            \"pattern_duration\": current_time - timestamp,\n            \"detector\": \"abc_pattern_detector\"\n          }\n        }\n      else:\n        # Reset on invalid sequence\n        if current_state:\n          log.info(\"Pattern broken for {}, was: {}, got: {}\", key, current_state, event_type)\n          pattern_tracker.delete(key)\n        return None\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  pattern_detection_pipeline:\n    from: pattern_events\n    via:\n      - type: mapValues\n        mapper: detect_abc_pattern\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: detected_patterns\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>State store usage for tracking partial patterns</li> <li>Sequential event processing</li> <li>Pattern completion and reset logic</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#temporal-pattern-matching","title":"Temporal Pattern Matching","text":"<p>Temporal patterns add time constraints to event sequences. This example detects quick checkout behavior (cart\u2192checkout within 5 minutes).</p> <p>What it does:</p> <ul> <li>Produces shopping events: Creates events like \"add_to_cart\" and \"checkout\" with realistic timestamps and shopping details</li> <li>Stores cart events: When \"add_to_cart\" happens, saves the cart timestamp and details in state store as JSON</li> <li>Measures time gaps: When \"checkout\" arrives, calculates milliseconds between cart and checkout events  </li> <li>Classifies by speed: If checkout happens within 5 minutes (300,000ms) = \"QUICK_CHECKOUT\", otherwise \"SLOW_CHECKOUT\"</li> <li>Outputs results: Only emits a message when both cart and checkout events are found, showing the time difference and classification</li> </ul> Temporal Events Producer - click to expand <pre><code># Producer for temporal pattern matching - generates time-sensitive events\n\nfunctions:\n  generate_temporal_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      users = [\"user_001\", \"user_002\", \"user_003\"]\n      actions = [\"login\", \"view_product\", \"add_to_cart\", \"checkout\", \"logout\"]\n    code: |\n      global event_counter, users, actions\n\n      event_counter += 1\n      user_id = random.choice(users)\n\n      # Create temporal sequences\n      if event_counter % 5 == 1:\n        action = \"login\"\n      elif event_counter % 5 == 2:\n        action = \"view_product\"\n      elif event_counter % 5 == 3:\n        action = \"add_to_cart\"\n      elif event_counter % 5 == 4:\n        action = \"checkout\"  # Should happen within 5 minutes of add_to_cart\n      else:\n        action = random.choice(actions)\n\n      current_timestamp = int(time.time() * 1000)\n\n      # Create structured JSON event for better readability in Kowl UI\n      event = {\n        \"user_id\": user_id,\n        \"action\": action,\n        \"timestamp\": current_timestamp,\n        \"event_id\": f\"evt_{event_counter:04d}\",\n        \"session_id\": f\"session_{user_id}_{event_counter // 10}\",\n        \"sequence_number\": event_counter,\n        \"source\": \"temporal_generator\",\n        \"metadata\": {\n          \"simulation\": True,\n          \"pattern_type\": \"temporal_checkout\",\n          \"time_window_minutes\": 5\n        }\n      }\n\n      # Add action-specific data\n      if action == \"view_product\":\n        event[\"product_id\"] = f\"prod_{random.randint(100, 999)}\"\n      elif action == \"add_to_cart\":\n        event[\"product_id\"] = f\"prod_{random.randint(100, 999)}\"\n        event[\"quantity\"] = random.randint(1, 5)\n        event[\"price\"] = round(random.uniform(10, 100), 2)\n      elif action == \"checkout\":\n        event[\"total_amount\"] = round(random.uniform(50, 500), 2)\n        event[\"payment_method\"] = random.choice([\"credit_card\", \"paypal\", \"bank_transfer\"])\n\n    expression: (user_id, event)\n    resultType: (string, json)\n\nproducers:\n  temporal_event_producer:\n    generator: generate_temporal_events\n    interval: 2s\n    to:\n      topic: temporal_events\n      keyType: string\n      valueType: json\n</code></pre> Temporal Pattern Processor - click to expand <pre><code># Processor for temporal pattern matching - detects actions within time windows\n\nstreams:\n  temporal_events:\n    topic: temporal_events\n    keyType: string\n    valueType: json\n\nstores:\n  temporal_state:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n\nfunctions:\n  detect_temporal_pattern:\n    type: valueTransformer\n    globalCode: |\n      import time\n      import json\n    stores:\n      - temporal_state\n    code: |\n      # Extract fields from JSON event\n      action = value.get(\"action\")\n      event_time = value.get(\"timestamp\")\n      user_id = value.get(\"user_id\")\n      event_id = value.get(\"event_id\")\n      session_id = value.get(\"session_id\")\n      current_time = int(time.time() * 1000)\n\n      if not action or not event_time:\n        return None\n\n      # Get stored state\n      state = temporal_state.get(key)\n\n      if action == \"add_to_cart\":\n        # Store cart event with detailed JSON state\n        cart_state = {\n          \"event\": \"add_to_cart\",\n          \"timestamp\": event_time,\n          \"event_id\": event_id,\n          \"session_id\": session_id,\n          \"product_id\": value.get(\"product_id\"),\n          \"quantity\": value.get(\"quantity\"),\n          \"price\": value.get(\"price\")\n        }\n        temporal_state.put(key, json.dumps(cart_state))\n        log.info(\"Cart event for {}: {}\", key, event_time)\n        return None\n\n      elif action == \"checkout\" and state:\n        # Parse stored cart state\n        try:\n          cart_data = json.loads(state)\n          if cart_data.get(\"event\") == \"add_to_cart\":\n            cart_time = cart_data.get(\"timestamp\")\n            time_diff = event_time - cart_time\n\n            # Create structured result\n            result = {\n              \"pattern_type\": \"TEMPORAL_CHECKOUT\",\n              \"user_id\": user_id,\n              \"session_id\": session_id,\n              \"cart_event\": {\n                \"event_id\": cart_data.get(\"event_id\"),\n                \"timestamp\": cart_time,\n                \"product_id\": cart_data.get(\"product_id\"),\n                \"quantity\": cart_data.get(\"quantity\"),\n                \"price\": cart_data.get(\"price\")\n              },\n              \"checkout_event\": {\n                \"event_id\": event_id,\n                \"timestamp\": event_time,\n                \"total_amount\": value.get(\"total_amount\"),\n                \"payment_method\": value.get(\"payment_method\")\n              },\n              \"time_difference_ms\": time_diff,\n              \"time_difference_minutes\": round(time_diff / 60000, 2),\n              \"detection_timestamp\": current_time,\n              \"processing_time\": current_time - event_time\n            }\n\n            if time_diff &lt;= 300000:  # Within 5 minutes\n              temporal_state.delete(key)\n              result[\"status\"] = \"QUICK_CHECKOUT\"\n              result[\"is_quick\"] = True\n              log.info(\"QUICK CHECKOUT detected for {} in {}ms\", key, time_diff)\n              return result\n            else:\n              temporal_state.delete(key)\n              result[\"status\"] = \"SLOW_CHECKOUT\"\n              result[\"is_quick\"] = False\n              log.info(\"Slow checkout for {} ({}ms)\", key, time_diff)\n              return result\n        except (json.JSONDecodeError, KeyError) as e:\n          log.warn(\"Error parsing cart state for {}: {}\", key, str(e))\n          temporal_state.delete(key)\n\n      return None\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  temporal_pattern_pipeline:\n    from: temporal_events\n    via:\n      - type: mapValues\n        mapper: detect_temporal_pattern\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: temporal_patterns\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Time-based pattern constraints</li> <li>Timestamp extraction and comparison</li> <li>Temporal window analysis</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#event-correlation","title":"Event Correlation","text":"<p>Event correlation combines related events from different streams to provide enriched context.</p> <p>What it does:</p> <ul> <li>Produces two event streams: Creates user events (page_view, click, form_submit) and system events (api_call, db_query, error) with the same user IDs</li> <li>Joins streams by user: Uses leftJoin to connect system events with the latest user activity for each user ID</li> <li>Detects specific patterns: Looks for meaningful combinations like \"form_submit + error\", \"page_view + db_query\", or \"click + api_call\"</li> <li>Measures timing: Calculates milliseconds between user action and system response to determine correlation strength (HIGH/MEDIUM)</li> <li>Outputs correlations: Only emits results when it finds the specific patterns, showing both events with timing analysis and relationship details</li> </ul> Correlation Events Producer (generates both user and system events) - click to expand <pre><code># Producer for event correlation - generates events from multiple sources\n\nfunctions:\n  generate_user_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\"]\n    code: |\n      global event_counter, users\n\n      event_counter += 1\n      user_id = random.choice(users)\n\n      # Generate user activity with structured JSON\n      activity = random.choice([\"page_view\", \"click\", \"scroll\", \"form_submit\"])\n      current_timestamp = int(time.time() * 1000)\n\n      event = {\n        \"event_type\": \"user_activity\",\n        \"user_id\": user_id,\n        \"activity\": activity,\n        \"timestamp\": current_timestamp,\n        \"event_id\": f\"user_evt_{event_counter:04d}\",\n        \"session_id\": f\"session_{user_id}_{event_counter // 5}\",\n        \"source\": \"user_interface\",\n        \"metadata\": {\n          \"simulation\": True,\n          \"correlation_type\": \"user_system\"\n        }\n      }\n\n      # Add activity-specific data\n      if activity == \"page_view\":\n        event[\"page_url\"] = f\"/page_{random.randint(1, 10)}\"\n        event[\"referrer\"] = random.choice([\"direct\", \"search\", \"social\"])\n      elif activity == \"click\":\n        event[\"element_id\"] = f\"btn_{random.randint(1, 5)}\"\n        event[\"element_type\"] = random.choice([\"button\", \"link\", \"image\"])\n      elif activity == \"form_submit\":\n        event[\"form_id\"] = f\"form_{random.randint(1, 3)}\"\n        event[\"form_data_length\"] = random.randint(10, 100)\n\n    expression: (user_id, event)\n    resultType: (string, json)\n\n  generate_system_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\"]\n    code: |\n      global event_counter, users\n\n      event_counter += 1\n      user_id = random.choice(users)\n\n      # Generate system events with structured JSON\n      event_type = random.choice([\"api_call\", \"db_query\", \"cache_hit\", \"error\"])\n      current_timestamp = int(time.time() * 1000)\n\n      event = {\n        \"event_type\": \"system_event\",\n        \"system_event\": event_type,\n        \"user_id\": user_id,\n        \"timestamp\": current_timestamp,\n        \"event_id\": f\"sys_evt_{event_counter:04d}\",\n        \"source\": \"backend_system\",\n        \"server_id\": f\"server_{random.randint(1, 3)}\",\n        \"metadata\": {\n          \"simulation\": True,\n          \"correlation_type\": \"user_system\"\n        }\n      }\n\n      # Add event-specific data\n      if event_type == \"api_call\":\n        event[\"endpoint\"] = f\"/api/v1/resource_{random.randint(1, 5)}\"\n        event[\"response_time_ms\"] = random.randint(50, 500)\n        event[\"status_code\"] = random.choice([200, 201, 400, 500])\n      elif event_type == \"db_query\":\n        event[\"table_name\"] = random.choice([\"users\", \"orders\", \"products\"])\n        event[\"query_time_ms\"] = random.randint(10, 200)\n        event[\"rows_affected\"] = random.randint(1, 100)\n      elif event_type == \"error\":\n        event[\"error_code\"] = random.choice([\"E001\", \"E002\", \"E003\"])\n        event[\"error_message\"] = f\"Error in operation {random.randint(1, 10)}\"\n        event[\"severity\"] = random.choice([\"warning\", \"error\", \"critical\"])\n      elif event_type == \"cache_hit\":\n        event[\"cache_key\"] = f\"cache_{random.randint(1, 20)}\"\n        event[\"cache_type\"] = random.choice([\"redis\", \"memcached\"])\n\n    expression: (user_id, event)\n    resultType: (string, json)\n\nproducers:\n  user_event_producer:\n    generator: generate_user_events\n    interval: 2s\n    to:\n      topic: user_events\n      keyType: string\n      valueType: json\n\n  system_event_producer:\n    generator: generate_system_events\n    interval: 3s\n    to:\n      topic: system_events\n      keyType: string\n      valueType: json\n</code></pre> Event Correlation Processor - click to expand <pre><code># Processor for event correlation - correlates user and system events\n\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json\n\n  system_events:\n    topic: system_events\n    keyType: string\n    valueType: json\n\ntables:\n  user_context:\n    topic: user_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  join_events:\n    type: valueJoiner\n    globalCode: |\n      import time\n    code: |\n      # Join system event with user context - create enriched JSON\n      current_time = int(time.time() * 1000)\n\n      # Extract system event details\n      system_event = value1.get(\"system_event\") if value1 else \"unknown\"\n      system_timestamp = value1.get(\"timestamp\") if value1 else current_time\n      system_event_id = value1.get(\"event_id\") if value1 else \"unknown\"\n\n      # Create enriched event with correlation data\n      enriched_event = {\n        \"correlation_id\": f\"corr_{system_event_id}\",\n        \"system_event\": value1 if value1 else {},\n        \"user_context\": value2 if value2 else {},\n        \"correlation_timestamp\": current_time,\n        \"has_user_context\": bool(value2),\n        \"time_since_user_activity\": current_time - (value2.get(\"timestamp\", current_time) if value2 else current_time)\n      }\n\n    expression: enriched_event\n    resultType: json\n\n  correlate_events:\n    type: valueTransformer\n    globalCode: |\n      import time\n    code: |\n      # Extract correlation data from enriched event\n      if not value:\n        return None\n\n      system_event = value.get(\"system_event\", {})\n      user_context = value.get(\"user_context\", {})\n      correlation_id = value.get(\"correlation_id\", \"unknown\")\n      has_user_context = value.get(\"has_user_context\", False)\n      time_since_user_activity = value.get(\"time_since_user_activity\", 0)\n      current_time = int(time.time() * 1000)\n\n      # Extract system and user event details\n      system_event_type = system_event.get(\"system_event\", \"unknown\")\n      user_activity = user_context.get(\"activity\", \"none\") if has_user_context else \"none\"\n\n      # Define correlation patterns and create structured results\n      correlation_result = None\n\n      if system_event_type == \"error\" and user_activity == \"form_submit\":\n        log.warn(\"ERROR after form submit for user {}\", key)\n        correlation_result = {\n          \"pattern_type\": \"FORM_ERROR_CORRELATION\",\n          \"status\": \"CRITICAL\",\n          \"description\": \"System error occurred after user form submission\",\n          \"correlation_id\": correlation_id,\n          \"user_id\": key,\n          \"system_event\": {\n            \"type\": system_event_type,\n            \"event_id\": system_event.get(\"event_id\"),\n            \"timestamp\": system_event.get(\"timestamp\"),\n            \"error_details\": {\n              \"error_code\": system_event.get(\"error_code\"),\n              \"error_message\": system_event.get(\"error_message\"),\n              \"severity\": system_event.get(\"severity\")\n            }\n          },\n          \"user_activity\": {\n            \"type\": user_activity,\n            \"event_id\": user_context.get(\"event_id\"),\n            \"timestamp\": user_context.get(\"timestamp\"),\n            \"form_details\": {\n              \"form_id\": user_context.get(\"form_id\"),\n              \"form_data_length\": user_context.get(\"form_data_length\")\n            }\n          },\n          \"time_correlation\": {\n            \"time_since_user_activity_ms\": time_since_user_activity,\n            \"correlation_strength\": \"HIGH\" if time_since_user_activity &lt; 5000 else \"MEDIUM\"\n          }\n        }\n\n      elif system_event_type == \"db_query\" and user_activity == \"page_view\":\n        log.info(\"DB query triggered by page view for {}\", key)\n        correlation_result = {\n          \"pattern_type\": \"PAGE_LOAD_CORRELATION\",\n          \"status\": \"NORMAL\",\n          \"description\": \"Database query triggered by user page view\",\n          \"correlation_id\": correlation_id,\n          \"user_id\": key,\n          \"system_event\": {\n            \"type\": system_event_type,\n            \"event_id\": system_event.get(\"event_id\"),\n            \"timestamp\": system_event.get(\"timestamp\"),\n            \"db_details\": {\n              \"table_name\": system_event.get(\"table_name\"),\n              \"query_time_ms\": system_event.get(\"query_time_ms\"),\n              \"rows_affected\": system_event.get(\"rows_affected\")\n            }\n          },\n          \"user_activity\": {\n            \"type\": user_activity,\n            \"event_id\": user_context.get(\"event_id\"),\n            \"timestamp\": user_context.get(\"timestamp\"),\n            \"page_details\": {\n              \"page_url\": user_context.get(\"page_url\"),\n              \"referrer\": user_context.get(\"referrer\")\n            }\n          },\n          \"time_correlation\": {\n            \"time_since_user_activity_ms\": time_since_user_activity,\n            \"correlation_strength\": \"HIGH\" if time_since_user_activity &lt; 2000 else \"MEDIUM\"\n          }\n        }\n\n      elif system_event_type == \"api_call\" and user_activity == \"click\":\n        log.info(\"API call from user interaction for {}\", key)\n        correlation_result = {\n          \"pattern_type\": \"USER_API_CORRELATION\",\n          \"status\": \"NORMAL\",\n          \"description\": \"API call triggered by user click interaction\",\n          \"correlation_id\": correlation_id,\n          \"user_id\": key,\n          \"system_event\": {\n            \"type\": system_event_type,\n            \"event_id\": system_event.get(\"event_id\"),\n            \"timestamp\": system_event.get(\"timestamp\"),\n            \"api_details\": {\n              \"endpoint\": system_event.get(\"endpoint\"),\n              \"response_time_ms\": system_event.get(\"response_time_ms\"),\n              \"status_code\": system_event.get(\"status_code\")\n            }\n          },\n          \"user_activity\": {\n            \"type\": user_activity,\n            \"event_id\": user_context.get(\"event_id\"),\n            \"timestamp\": user_context.get(\"timestamp\"),\n            \"click_details\": {\n              \"element_id\": user_context.get(\"element_id\"),\n              \"element_type\": user_context.get(\"element_type\")\n            }\n          },\n          \"time_correlation\": {\n            \"time_since_user_activity_ms\": time_since_user_activity,\n            \"correlation_strength\": \"HIGH\" if time_since_user_activity &lt; 1000 else \"MEDIUM\"\n          }\n        }\n\n      if correlation_result:\n        correlation_result[\"detection_timestamp\"] = current_time\n        correlation_result[\"processing_time\"] = current_time - system_event.get(\"timestamp\", current_time)\n        correlation_result[\"metadata\"] = {\n          \"detector\": \"event_correlation_processor\",\n          \"version\": \"1.0\"\n        }\n\n      return correlation_result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  correlation_pipeline:\n    from: system_events\n    via:\n      - type: leftJoin\n        table: user_context\n        valueJoiner: join_events\n      - type: mapValues\n        mapper: correlate_events\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: correlated_events\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Stream-table joins for context enrichment</li> <li>Cross-stream event correlation</li> <li>Cause-effect relationship detection</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#anomaly-detection","title":"Anomaly Detection","text":"<p>Anomaly detection identifies unusual patterns using statistical analysis.</p> <p>What it does:</p> <ul> <li>Produces sensor readings: Creates temperature values that are normally 40-60\u00b0C, but every 20th reading is a spike (90-100\u00b0C) and every 25th is a drop (0-10\u00b0C)</li> <li>Tracks statistics per sensor: Stores running count, sum, sum-of-squares, min, max in state store to calculate mean and standard deviation</li> <li>Calculates z-scores: After 10+ readings, computes how many standard deviations each new reading is from the mean</li> <li>Detects outliers: When z-score &gt; 3.0, flags as anomaly (spike if above mean, drop if below mean)</li> <li>Outputs anomalies: Only emits detection messages when statistical threshold is exceeded, showing z-score, mean, and severity level</li> </ul> Metrics Producer (with occasional anomalies) - click to expand <pre><code># Producer for anomaly detection - generates metrics with occasional anomalies\n\nfunctions:\n  generate_metrics:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      import math\n      metric_counter = 0\n      sensors = [\"sensor_001\", \"sensor_002\", \"sensor_003\"]\n    code: |\n      global metric_counter, sensors\n\n      metric_counter += 1\n      sensor_id = random.choice(sensors)\n\n      # Generate normal values with occasional anomalies\n      if metric_counter % 20 == 0:\n        # Anomaly - spike\n        value = random.uniform(90, 100)\n      elif metric_counter % 25 == 0:\n        # Anomaly - drop\n        value = random.uniform(0, 10)\n      else:\n        # Normal range with some variation\n        base = 50 + 10 * math.sin(metric_counter / 10)\n        value = base + random.uniform(-5, 5)\n\n      current_timestamp = int(time.time() * 1000)\n\n      # Create structured JSON metric for better readability in Kowl UI\n      metric = {\n        \"sensor_id\": sensor_id,\n        \"value\": round(value, 2),\n        \"timestamp\": current_timestamp,\n        \"metric_id\": f\"metric_{metric_counter:04d}\",\n        \"sensor_type\": \"temperature\",\n        \"unit\": \"celsius\",\n        \"location\": f\"zone_{random.randint(1, 5)}\",\n        \"source\": \"sensor_network\",\n        \"metadata\": {\n          \"simulation\": True,\n          \"anomaly_detection\": True,\n          \"normal_range\": [40, 60]\n        }\n      }\n\n      # Add anomaly indicators for testing\n      if metric_counter % 20 == 0:\n        metric[\"anomaly_type\"] = \"spike\"\n        metric[\"expected_anomaly\"] = True\n      elif metric_counter % 25 == 0:\n        metric[\"anomaly_type\"] = \"drop\" \n        metric[\"expected_anomaly\"] = True\n      else:\n        metric[\"expected_anomaly\"] = False\n\n    expression: (sensor_id, metric)\n    resultType: (string, json)\n\nproducers:\n  metrics_producer:\n    generator: generate_metrics\n    interval: 1s\n    to:\n      topic: sensor_metrics\n      keyType: string\n      valueType: json\n</code></pre> Anomaly Detection Processor - click to expand <pre><code># Processor for anomaly detection using statistical analysis\n\nstreams:\n  sensor_metrics:\n    topic: sensor_metrics\n    keyType: string\n    valueType: json\n\nstores:\n  stats_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n\nfunctions:\n  detect_anomalies:\n    type: valueTransformer\n    stores:\n      - stats_store\n    code: |\n      import json\n\n      # Extract fields from JSON metric\n      current_value = value.get(\"value\")\n      timestamp = value.get(\"timestamp\")\n      sensor_id = value.get(\"sensor_id\")\n      metric_id = value.get(\"metric_id\")\n\n      if current_value is None or timestamp is None:\n        return None\n\n      # Get or initialize statistics\n      stats_json = stats_store.get(key)\n      if stats_json:\n        stats = json.loads(stats_json)\n      else:\n        stats = {\"count\": 0, \"sum\": 0, \"sum_sq\": 0, \"min\": current_value, \"max\": current_value}\n\n      # Calculate running statistics\n      n = stats[\"count\"]\n      if n &gt; 10:  # Need enough samples\n        mean = stats[\"sum\"] / n\n        variance = (stats[\"sum_sq\"] / n) - (mean * mean)\n        std_dev = variance ** 0.5 if variance &gt; 0 else 1\n\n        # Detect anomalies (values outside 3 standard deviations)\n        z_score = abs(current_value - mean) / std_dev if std_dev &gt; 0 else 0\n\n        if z_score &gt; 3:\n          log.warn(\"ANOMALY detected for {}: value={}, mean={:.2f}, z_score={:.2f}\", \n                   key, current_value, mean, z_score)\n          anomaly_type = \"spike\" if current_value &gt; mean else \"drop\"\n          result = {\n            \"anomaly_type\": \"STATISTICAL_ANOMALY\",\n            \"status\": \"DETECTED\",\n            \"pattern\": anomaly_type,\n            \"sensor_id\": sensor_id,\n            \"metric_id\": metric_id,\n            \"anomaly_value\": current_value,\n            \"statistical_analysis\": {\n              \"mean\": round(mean, 2),\n              \"std_dev\": round(std_dev, 2),\n              \"z_score\": round(z_score, 2),\n              \"threshold\": 3.0,\n              \"sample_count\": n + 1\n            },\n            \"detection_timestamp\": timestamp,\n            \"sensor_metadata\": {\n              \"sensor_type\": value.get(\"sensor_type\"),\n              \"location\": value.get(\"location\"),\n              \"unit\": value.get(\"unit\")\n            },\n            \"severity\": \"HIGH\" if z_score &gt; 5 else \"MEDIUM\"\n          }\n        else:\n          result = None\n      else:\n        result = None\n\n      # Update statistics\n      stats[\"count\"] = n + 1\n      stats[\"sum\"] += current_value\n      stats[\"sum_sq\"] += current_value * current_value\n      stats[\"min\"] = min(stats[\"min\"], current_value)\n      stats[\"max\"] = max(stats[\"max\"], current_value)\n\n      # Store updated stats\n      stats_store.put(key, json.dumps(stats))\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  anomaly_detection_pipeline:\n    from: sensor_metrics\n    via:\n      - type: mapValues\n        mapper: detect_anomalies\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: anomalies_detected\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Statistical anomaly detection (z-score)</li> <li>Running statistics calculation</li> <li>Threshold-based alerting</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#fraud-detection-system","title":"Fraud Detection System","text":"<p>A practical example combining multiple CEP techniques for real-time fraud detection.</p> <p>What it does:</p> <ul> <li>Produces transactions: Creates credit card purchases with amounts, merchants, locations - deliberately generates suspicious patterns (high amounts every 15th, rapid transactions every 20th)</li> <li>Stores transaction history: Keeps last location, timestamp, and totals for each card number in state store</li> <li>Scores fraud risk: Adds points for patterns: +40 for amounts &gt;$5000, +30 for transactions &lt;60s apart, +20 for location changes &lt;1hr, +20 for suspicious merchants</li> <li>Classifies threats: If score \u226570 = \"FRAUD_ALERT\", if 30-69 = \"SUSPICIOUS_TRANSACTION\", otherwise no output</li> <li>Outputs alerts: Only emits results when fraud score thresholds are met, showing detected patterns, risk factors, and recommended actions</li> </ul> Credit Card Transactions Producer - click to expand <pre><code># Producer for fraud detection - generates credit card transactions\n\nfunctions:\n  generate_transactions:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      txn_counter = 0\n      cards = [\"4111-1111-1111-1111\", \"4222-2222-2222-2222\", \"4333-3333-3333-3333\"]\n      merchants = [\"grocery_store\", \"gas_station\", \"online_shop\", \"restaurant\", \"atm_withdrawal\"]\n      locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]\n    code: |\n      global txn_counter, cards, merchants, locations\n\n      txn_counter += 1\n      card_number = random.choice(cards)\n      current_timestamp = int(time.time() * 1000)\n\n      # Generate transaction with occasional suspicious patterns\n      if txn_counter % 15 == 0:\n        # Suspicious: high amount\n        amount = random.uniform(5000, 10000)\n        merchant = \"luxury_goods\"\n        is_suspicious = True\n        risk_category = \"high_amount\"\n      elif txn_counter % 20 == 0:\n        # Suspicious: rapid transactions\n        amount = random.uniform(100, 500)\n        merchant = \"atm_withdrawal\"\n        is_suspicious = True\n        risk_category = \"rapid_transaction\"\n      else:\n        # Normal transaction\n        amount = random.uniform(10, 500)\n        merchant = random.choice(merchants)\n        is_suspicious = False\n        risk_category = \"normal\"\n\n      location = random.choice(locations)\n\n      # Create structured JSON transaction for better readability in Kowl UI\n      transaction = {\n        \"transaction_id\": f\"txn_{txn_counter:06d}\",\n        \"card_number\": card_number,\n        \"amount\": round(amount, 2),\n        \"merchant\": merchant,\n        \"location\": location,\n        \"timestamp\": current_timestamp,\n        \"merchant_category\": {\n          \"grocery_store\": \"retail\",\n          \"gas_station\": \"fuel\",\n          \"online_shop\": \"ecommerce\",\n          \"restaurant\": \"food_service\",\n          \"atm_withdrawal\": \"cash\",\n          \"luxury_goods\": \"luxury\"\n        }.get(merchant, \"other\"),\n        \"transaction_type\": \"purchase\" if merchant != \"atm_withdrawal\" else \"withdrawal\",\n        \"currency\": \"USD\",\n        \"channel\": \"pos\" if merchant in [\"grocery_store\", \"gas_station\", \"restaurant\"] else \"online\",\n        \"metadata\": {\n          \"simulation\": True,\n          \"fraud_detection\": True,\n          \"is_suspicious\": is_suspicious,\n          \"risk_category\": risk_category,\n          \"sequence_number\": txn_counter\n        }\n      }\n\n    expression: (card_number, transaction)\n    resultType: (string, json)\n\nproducers:\n  transaction_producer:\n    generator: generate_transactions\n    interval: 1s\n    to:\n      topic: credit_card_transactions\n      keyType: string\n      valueType: json\n</code></pre> Fraud Detection Processor - click to expand <pre><code># Processor for fraud detection - analyzes transaction patterns\n\nstreams:\n  credit_card_transactions:\n    topic: credit_card_transactions\n    keyType: string\n    valueType: json\n\nstores:\n  transaction_history:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  detect_fraud:\n    type: valueTransformer\n    stores:\n      - transaction_history\n    code: |\n      import json\n      import time\n\n      # Extract fields from JSON transaction\n      if not value:\n        return None\n\n      amount = value.get(\"amount\")\n      merchant = value.get(\"merchant\")\n      location = value.get(\"location\")\n      timestamp = value.get(\"timestamp\")\n      transaction_id = value.get(\"transaction_id\")\n      merchant_category = value.get(\"merchant_category\")\n      transaction_type = value.get(\"transaction_type\")\n      channel = value.get(\"channel\")\n\n      if amount is None or merchant is None or location is None or timestamp is None:\n        return None\n\n      # Get transaction history\n      history_json = transaction_history.get(key)\n      if history_json:\n        history = json.loads(history_json)\n      else:\n        history = {\"last_location\": location, \"last_time\": timestamp, \"txn_count\": 0, \"total_amount\": 0}\n\n      # Check for fraud patterns\n      fraud_score = 0\n      fraud_reasons = []\n      fraud_patterns = []\n\n      # Pattern 1: High amount transaction\n      if amount &gt; 5000:\n        fraud_score += 40\n        fraud_reasons.append(f\"high_amount:{amount:.2f}\")\n        fraud_patterns.append({\n          \"type\": \"HIGH_AMOUNT\",\n          \"details\": {\"amount\": amount, \"threshold\": 5000},\n          \"risk_weight\": 40\n        })\n\n      # Pattern 2: Rapid transactions (within 60 seconds)\n      time_diff = timestamp - history[\"last_time\"]\n      if time_diff &lt; 60000 and history[\"txn_count\"] &gt; 0:\n        fraud_score += 30\n        fraud_reasons.append(f\"rapid_txn:{time_diff}ms\")\n        fraud_patterns.append({\n          \"type\": \"RAPID_TRANSACTION\",\n          \"details\": {\"time_difference_ms\": time_diff, \"threshold_ms\": 60000},\n          \"risk_weight\": 30\n        })\n\n      # Pattern 3: Location change\n      if history[\"last_location\"] != location and time_diff &lt; 3600000:  # Within 1 hour\n        fraud_score += 20\n        fraud_reasons.append(f\"location_change:{history['last_location']}-&gt;{location}\")\n        fraud_patterns.append({\n          \"type\": \"LOCATION_CHANGE\",\n          \"details\": {\n            \"previous_location\": history[\"last_location\"],\n            \"current_location\": location,\n            \"time_difference_ms\": time_diff,\n            \"threshold_ms\": 3600000\n          },\n          \"risk_weight\": 20\n        })\n\n      # Pattern 4: Suspicious merchant\n      if merchant in [\"luxury_goods\", \"online_gambling\", \"crypto_exchange\"]:\n        fraud_score += 20\n        fraud_reasons.append(f\"suspicious_merchant:{merchant}\")\n        fraud_patterns.append({\n          \"type\": \"SUSPICIOUS_MERCHANT\",\n          \"details\": {\"merchant\": merchant, \"merchant_category\": merchant_category},\n          \"risk_weight\": 20\n        })\n\n      # Update history\n      history[\"last_location\"] = location\n      history[\"last_time\"] = timestamp\n      history[\"txn_count\"] += 1\n      history[\"total_amount\"] += amount\n      transaction_history.put(key, json.dumps(history))\n\n      # Generate structured alert if fraud score is high\n      result = None\n      if fraud_score &gt;= 50:\n        log.warn(\"FRAUD ALERT for card {}: score={}, reasons={}\", key, fraud_score, fraud_reasons)\n        result = {\n          \"alert_type\": \"FRAUD_ALERT\",\n          \"status\": \"HIGH_RISK\",\n          \"card_number\": key,\n          \"fraud_score\": fraud_score,\n          \"severity\": \"CRITICAL\",\n          \"transaction\": {\n            \"transaction_id\": transaction_id,\n            \"amount\": amount,\n            \"merchant\": merchant,\n            \"merchant_category\": merchant_category,\n            \"location\": location,\n            \"timestamp\": timestamp,\n            \"transaction_type\": transaction_type,\n            \"channel\": channel\n          },\n          \"fraud_patterns\": fraud_patterns,\n          \"risk_analysis\": {\n            \"total_score\": fraud_score,\n            \"threshold_exceeded\": \"HIGH_RISK\",\n            \"patterns_detected\": len(fraud_patterns),\n            \"recommendation\": \"BLOCK_TRANSACTION\"\n          },\n          \"detection_timestamp\": int(time.time() * 1000),\n          \"cardholder_history\": {\n            \"previous_location\": history.get(\"last_location\"),\n            \"transaction_count\": history.get(\"txn_count\", 0) + 1,\n            \"total_spending\": history.get(\"total_amount\", 0) + amount\n          }\n        }\n      elif fraud_score &gt;= 30:\n        log.info(\"Suspicious transaction for {}: score={}\", key, fraud_score)\n        result = {\n          \"alert_type\": \"SUSPICIOUS_TRANSACTION\",\n          \"status\": \"MEDIUM_RISK\",\n          \"card_number\": key,\n          \"fraud_score\": fraud_score,\n          \"severity\": \"WARNING\",\n          \"transaction\": {\n            \"transaction_id\": transaction_id,\n            \"amount\": amount,\n            \"merchant\": merchant,\n            \"merchant_category\": merchant_category,\n            \"location\": location,\n            \"timestamp\": timestamp,\n            \"transaction_type\": transaction_type,\n            \"channel\": channel\n          },\n          \"fraud_patterns\": fraud_patterns,\n          \"risk_analysis\": {\n            \"total_score\": fraud_score,\n            \"threshold_exceeded\": \"MEDIUM_RISK\",\n            \"patterns_detected\": len(fraud_patterns),\n            \"recommendation\": \"REVIEW_REQUIRED\"\n          },\n          \"detection_timestamp\": int(time.time() * 1000),\n          \"cardholder_history\": {\n            \"previous_location\": history.get(\"last_location\"),\n            \"transaction_count\": history.get(\"txn_count\", 0) + 1,\n            \"total_spending\": history.get(\"total_amount\", 0) + amount\n          }\n        }\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  fraud_detection_pipeline:\n    from: credit_card_transactions\n    via:\n      - type: mapValues\n        mapper: detect_fraud\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: fraud_alerts\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Multi-factor pattern analysis</li> <li>Risk scoring algorithms</li> <li>Transaction velocity checks</li> <li>Geographic anomaly detection</li> </ul>"},{"location":"tutorials/advanced/complex-event-processing/#conclusion","title":"Conclusion","text":"<p>Complex Event Processing in KSML provides capabilities for real-time pattern detection and analysis. By combining state management, temporal operations, and correlation techniques, you can build sophisticated event processing applications that derive actionable insights from streaming data.</p>"},{"location":"tutorials/advanced/custom-state-stores/","title":"Custom State Stores in KSML","text":"<p>This tutorial explores how to implement and optimize custom state stores in KSML, allowing you to maintain and manage state in your stream processing applications with greater flexibility and control.</p>"},{"location":"tutorials/advanced/custom-state-stores/#introduction-to-state-stores","title":"Introduction to State Stores","text":"<p>State stores are a critical component of stateful stream processing applications. They allow your application to:</p> <ul> <li>Maintain data across multiple messages and events</li> <li>Track historical information for context-aware processing  </li> <li>Implement stateful operations like aggregations and joins</li> <li>Build sophisticated business logic that depends on previous events</li> <li>Persist state for fault tolerance and recovery</li> </ul> <p>KSML provides built-in state store capabilities that integrate seamlessly with Kafka Streams, offering exactly-once processing guarantees and automatic state management.</p>"},{"location":"tutorials/advanced/custom-state-stores/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Complete the State Stores Tutorial - This tutorial builds on fundamental state store concepts, configuration methods, and basic patterns covered in the intermediate tutorial</li> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_activity &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_session_stats &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic server_metrics &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic windowed_metrics &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_clicks &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic session_analytics &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic device_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic device_alerts &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic order_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic order_processing_results &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/advanced/custom-state-stores/#advanced-key-value-store-patterns","title":"Advanced Key-Value Store Patterns","text":"<p>Building on the basic key-value concepts from the State Stores Tutorial, this example demonstrates advanced user session tracking with complex business logic.</p> <p>What it does:</p> <ul> <li>Produces user activities: Creates events like login, page_view, click with user IDs, session IDs (changes every 10 events), browser/device info</li> <li>Stores user profiles: Keeps running JSON data per user including total sessions, action counts, time spent, devices/browsers used </li> <li>Detects session changes: When session_id changes from previous event, increments session counter and logs the transition</li> <li>Tracks comprehensive stats: Updates action counters, adds new pages/devices/countries to lists, calculates total time across all sessions</li> <li>Outputs session updates: Returns enriched user profile showing current session, lifetime statistics, and behavioral patterns whenever activity occurs</li> </ul> User Activity Producer - click to expand <pre><code># Producer for user session tracking - generates user activity events\n\nfunctions:\n  generate_user_activity:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"]\n      actions = [\"login\", \"page_view\", \"click\", \"search\", \"logout\"]\n    code: |\n      global event_counter, users, actions\n\n      event_counter += 1\n      user_id = random.choice(users)\n      action = random.choice(actions)\n\n      # Generate structured JSON activity for better readability in Kowl UI\n      activity = {\n        \"user_id\": user_id,\n        \"action\": action,\n        \"timestamp\": int(time.time() * 1000),\n        \"session_id\": f\"session_{user_id}_{event_counter // 10}\",  # Change session every 10 events\n        \"page\": f\"/page/{random.randint(1, 5)}\",\n        \"duration_ms\": random.randint(100, 5000),\n        \"event_id\": f\"evt_{event_counter:06d}\",\n        \"browser\": random.choice([\"chrome\", \"firefox\", \"safari\", \"edge\"]),\n        \"device_type\": random.choice([\"desktop\", \"tablet\", \"mobile\"]),\n        \"location\": {\n          \"country\": random.choice([\"US\", \"CA\", \"UK\", \"DE\", \"FR\"]),\n          \"city\": random.choice([\"New York\", \"London\", \"Paris\", \"Berlin\", \"Toronto\"])\n        },\n        \"metadata\": {\n          \"simulation\": True,\n          \"session_tracking\": True,\n          \"event_sequence\": event_counter\n        }\n      }\n\n    expression: (user_id, activity)\n    resultType: (string, json)\n\nproducers:\n  user_activity_producer:\n    generator: generate_user_activity\n    interval: 2s\n    to:\n      topic: user_activity\n      keyType: string\n      valueType: json\n</code></pre> Basic Key-Value Store Processor - click to expand <pre><code># Processor demonstrating basic key-value store for session tracking\n\nstreams:\n  user_activity:\n    topic: user_activity\n    keyType: string\n    valueType: json\n\nstores:\n  user_session_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  track_user_sessions:\n    type: valueTransformer\n    stores:\n      - user_session_store\n    code: |\n      import json\n      import time\n\n      # Extract fields from JSON activity\n      if not value:\n        return None\n\n      action = value.get(\"action\")\n      session_id = value.get(\"session_id\")\n      duration = value.get(\"duration_ms\")\n      timestamp = value.get(\"timestamp\")\n      event_id = value.get(\"event_id\")\n      page = value.get(\"page\")\n      browser = value.get(\"browser\")\n      device_type = value.get(\"device_type\")\n      location = value.get(\"location\", {})\n\n      if not action or not session_id or duration is None:\n        return None\n\n      # Get existing session data\n      session_data_str = user_session_store.get(key)\n      if session_data_str:\n        session_data = json.loads(session_data_str)\n      else:\n        session_data = {\n          \"user_id\": key,\n          \"current_session\": None,\n          \"total_sessions\": 0,\n          \"total_time_ms\": 0,\n          \"actions_count\": {},\n          \"first_seen\": timestamp,\n          \"last_activity\": timestamp,\n          \"devices_used\": set(),\n          \"browsers_used\": set(),\n          \"countries_visited\": set(),\n          \"pages_visited\": set()\n        }\n        # Convert sets to lists for JSON serialization\n        session_data[\"devices_used\"] = []\n        session_data[\"browsers_used\"] = []\n        session_data[\"countries_visited\"] = []\n        session_data[\"pages_visited\"] = []\n\n      # Track session changes\n      session_ended = False\n      if session_data[\"current_session\"] != session_id:\n        if session_data[\"current_session\"] is not None:\n          # Session changed\n          session_data[\"total_sessions\"] += 1\n          session_ended = True\n          log.info(\"Session ended for user {}: {}\", key, session_data[\"current_session\"])\n\n        session_data[\"current_session\"] = session_id\n        log.info(\"New session started for user {}: {}\", key, session_id)\n\n      # Update activity tracking\n      session_data[\"total_time_ms\"] += duration\n      session_data[\"last_activity\"] = timestamp\n\n      if action in session_data[\"actions_count\"]:\n        session_data[\"actions_count\"][action] += 1\n      else:\n        session_data[\"actions_count\"][action] = 1\n\n      # Track device/browser/location usage\n      if device_type and device_type not in session_data[\"devices_used\"]:\n        session_data[\"devices_used\"].append(device_type)\n      if browser and browser not in session_data[\"browsers_used\"]:\n        session_data[\"browsers_used\"].append(browser)\n      if location.get(\"country\") and location[\"country\"] not in session_data[\"countries_visited\"]:\n        session_data[\"countries_visited\"].append(location[\"country\"])\n      if page and page not in session_data[\"pages_visited\"]:\n        session_data[\"pages_visited\"].append(page)\n\n      # Store updated session data\n      user_session_store.put(key, json.dumps(session_data))\n\n      # Generate structured session summary\n      result = {\n        \"stats_type\": \"USER_SESSION_STATS\",\n        \"user_id\": key,\n        \"current_session\": session_id,\n        \"session_ended\": session_ended,\n        \"activity\": {\n          \"event_id\": event_id,\n          \"action\": action,\n          \"page\": page,\n          \"duration_ms\": duration,\n          \"timestamp\": timestamp\n        },\n        \"session_totals\": {\n          \"total_sessions\": session_data[\"total_sessions\"],\n          \"total_time_ms\": session_data[\"total_time_ms\"],\n          \"unique_actions\": len(session_data[\"actions_count\"]),\n          \"action_counts\": session_data[\"actions_count\"]\n        },\n        \"user_profile\": {\n          \"first_seen\": session_data[\"first_seen\"],\n          \"last_activity\": session_data[\"last_activity\"],\n          \"devices_used\": session_data[\"devices_used\"],\n          \"browsers_used\": session_data[\"browsers_used\"],\n          \"countries_visited\": session_data[\"countries_visited\"],\n          \"pages_visited\": len(session_data[\"pages_visited\"]),\n          \"most_visited_pages\": session_data[\"pages_visited\"][-5:] if len(session_data[\"pages_visited\"]) &gt; 5 else session_data[\"pages_visited\"]\n        },\n        \"current_context\": {\n          \"device_type\": device_type,\n          \"browser\": browser,\n          \"location\": location\n        }\n      }\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  session_tracking_pipeline:\n    from: user_activity\n    via:\n      - type: mapValues\n        mapper: track_user_sessions\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: user_session_stats\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>JSON serialization for complex state objects</li> <li>Session boundary detection</li> <li>Persistent state with caching enabled</li> </ul>"},{"location":"tutorials/advanced/custom-state-stores/#window-store","title":"Window Store","text":"<p>Window stores organize data by time windows, enabling time-based aggregations and analytics.</p> <p>What it does:</p> <ul> <li>Produces server metrics: Creates metrics like cpu_usage, memory_usage, disk_io with varying values (base + sine wave + noise) for different servers</li> <li>Creates time windows: Divides timestamps into 5-minute buckets (300,000ms), creates unique window keys like \"server1:cpu_usage:1640995200000\" </li> <li>Accumulates window stats: For each metric in a time window, stores running count, sum, min, max, and recent sample list in state store</li> <li>Calculates aggregates: When outputting, computes average from sum/count, range from max-min, tracks categorical data like datacenter/environment</li> <li>Outputs window results: Returns complete window statistics only when window has enough samples, showing aggregated metrics with alerting thresholds and metadata</li> </ul> Metrics Data Producer - click to expand <pre><code># Producer for window store demo - generates time-series metrics\n\nfunctions:\n  generate_metrics:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      import math\n      metric_counter = 0\n      metrics = [\"cpu_usage\", \"memory_usage\", \"disk_io\", \"network_io\"]\n      servers = [\"server_001\", \"server_002\", \"server_003\"]\n    code: |\n      global metric_counter, metrics, servers\n\n      metric_counter += 1\n      server = random.choice(servers)\n      metric_name = random.choice(metrics)\n\n      # Generate realistic metric values\n      base_value = {\n        \"cpu_usage\": 30,\n        \"memory_usage\": 60,\n        \"disk_io\": 1000,\n        \"network_io\": 500\n      }[metric_name]\n\n      # Add some variation and trends\n      trend = math.sin(metric_counter / 20) * 10\n      noise = random.uniform(-5, 5)\n      value = max(0, base_value + trend + noise)\n\n      current_timestamp = int(time.time() * 1000)\n\n      # Create structured JSON metric for better readability in Kowl UI\n      metric = {\n        \"server_id\": server,\n        \"metric_name\": metric_name,\n        \"value\": round(value, 2),\n        \"timestamp\": current_timestamp,\n        \"metric_id\": f\"metric_{metric_counter:06d}\",\n        \"unit\": {\n          \"cpu_usage\": \"percent\",\n          \"memory_usage\": \"percent\", \n          \"disk_io\": \"MB/s\",\n          \"network_io\": \"MB/s\"\n        }[metric_name],\n        \"datacenter\": random.choice([\"dc1\", \"dc2\", \"dc3\"]),\n        \"environment\": random.choice([\"prod\", \"staging\", \"test\"]),\n        \"service\": random.choice([\"web\", \"api\", \"database\", \"cache\"]),\n        \"alerting\": {\n          \"enabled\": True,\n          \"threshold_high\": base_value * 1.5,\n          \"threshold_critical\": base_value * 2.0\n        },\n        \"metadata\": {\n          \"simulation\": True,\n          \"window_aggregation\": True,\n          \"baseline_value\": base_value,\n          \"trend_component\": round(trend, 2),\n          \"noise_component\": round(noise, 2)\n        }\n      }\n\n    expression: (server, metric)\n    resultType: (string, json)\n\nproducers:\n  metrics_producer:\n    generator: generate_metrics\n    interval: 1s\n    to:\n      topic: server_metrics\n      keyType: string\n      valueType: json\n</code></pre> Window Store Processor - click to expand <pre><code># Processor demonstrating window store for time-based aggregations\n\nstreams:\n  server_metrics:\n    topic: server_metrics\n    keyType: string\n    valueType: json\n\nstores:\n  metrics_window_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  aggregate_metrics:\n    type: valueTransformer\n    stores:\n      - metrics_window_store\n    code: |\n      import json\n      import time\n\n      # Extract fields from JSON metric\n      if not value:\n        return None\n\n      metric_name = value.get(\"metric_name\")\n      metric_value = value.get(\"value\")\n      timestamp = value.get(\"timestamp\")\n      metric_id = value.get(\"metric_id\")\n      unit = value.get(\"unit\")\n      datacenter = value.get(\"datacenter\")\n      environment = value.get(\"environment\")\n      service = value.get(\"service\")\n      alerting = value.get(\"alerting\", {})\n      metadata = value.get(\"metadata\", {})\n\n      if not metric_name or metric_value is None or not timestamp:\n        return None\n\n      # Create window key (5-minute windows)\n      window_size_ms = 5 * 60 * 1000  # 5 minutes\n      window_start = (timestamp // window_size_ms) * window_size_ms\n      window_key = f\"{key}:{metric_name}:{window_start}\"\n\n      # Get existing window data\n      window_data_str = metrics_window_store.get(window_key)\n      if window_data_str:\n        window_data = json.loads(window_data_str)\n      else:\n        window_data = {\n          \"server_id\": key,\n          \"metric_name\": metric_name,\n          \"unit\": unit,\n          \"window_start\": window_start,\n          \"window_end\": window_start + window_size_ms,\n          \"window_size_ms\": window_size_ms,\n          \"count\": 0,\n          \"sum\": 0,\n          \"min\": metric_value,\n          \"max\": metric_value,\n          \"first_timestamp\": timestamp,\n          \"last_timestamp\": timestamp,\n          \"values\": [],\n          \"datacenters\": set(),\n          \"environments\": set(),\n          \"services\": set()\n        }\n        # Convert sets to lists for JSON serialization\n        window_data[\"datacenters\"] = []\n        window_data[\"environments\"] = []\n        window_data[\"services\"] = []\n\n      # Update window aggregates\n      window_data[\"count\"] += 1\n      window_data[\"sum\"] += metric_value\n      window_data[\"min\"] = min(window_data[\"min\"], metric_value)\n      window_data[\"max\"] = max(window_data[\"max\"], metric_value)\n      window_data[\"last_timestamp\"] = timestamp\n\n      # Track categorical data\n      if datacenter and datacenter not in window_data[\"datacenters\"]:\n        window_data[\"datacenters\"].append(datacenter)\n      if environment and environment not in window_data[\"environments\"]:\n        window_data[\"environments\"].append(environment)\n      if service and service not in window_data[\"services\"]:\n        window_data[\"services\"].append(service)\n\n      # Keep recent sample values for analysis (last 10 for memory efficiency)\n      window_data[\"values\"].append({\n        \"value\": metric_value,\n        \"timestamp\": timestamp,\n        \"metric_id\": metric_id\n      })\n      if len(window_data[\"values\"]) &gt; 10:\n        window_data[\"values\"] = window_data[\"values\"][-10:]\n\n      # Calculate statistics\n      avg = window_data[\"sum\"] / window_data[\"count\"]\n      window_duration = window_data[\"last_timestamp\"] - window_data[\"first_timestamp\"]\n\n      # Check against alerting thresholds\n      alert_status = \"normal\"\n      if alerting.get(\"enabled\", False):\n        if avg &gt;= alerting.get(\"threshold_critical\", float('inf')):\n          alert_status = \"critical\"\n        elif avg &gt;= alerting.get(\"threshold_high\", float('inf')):\n          alert_status = \"warning\"\n\n      # Store updated window\n      metrics_window_store.put(window_key, json.dumps(window_data))\n\n      # Generate comprehensive window aggregation result\n      result = {\n        \"aggregation_type\": \"WINDOW_AGGREGATION\",\n        \"server_id\": key,\n        \"metric_name\": metric_name,\n        \"unit\": unit,\n        \"window\": {\n          \"start_timestamp\": window_start,\n          \"end_timestamp\": window_start + window_size_ms,\n          \"duration_ms\": window_size_ms,\n          \"actual_duration_ms\": window_duration,\n          \"window_key\": window_key\n        },\n        \"statistics\": {\n          \"count\": window_data[\"count\"],\n          \"average\": round(avg, 2),\n          \"minimum\": round(window_data[\"min\"], 2),\n          \"maximum\": round(window_data[\"max\"], 2),\n          \"sum\": round(window_data[\"sum\"], 2),\n          \"range\": round(window_data[\"max\"] - window_data[\"min\"], 2)\n        },\n        \"alerting\": {\n          \"status\": alert_status,\n          \"thresholds\": alerting,\n          \"current_average\": round(avg, 2)\n        },\n        \"distribution\": {\n          \"datacenters\": window_data[\"datacenters\"],\n          \"environments\": window_data[\"environments\"],\n          \"services\": window_data[\"services\"]\n        },\n        \"sampling\": {\n          \"recent_values\": window_data[\"values\"][-3:],  # Show last 3 values\n          \"total_samples\": window_data[\"count\"]\n        },\n        \"processing_info\": {\n          \"window_updated\": True,\n          \"latest_metric_id\": metric_id,\n          \"processing_timestamp\": int(time.time() * 1000)\n        }\n      }\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  window_aggregation_pipeline:\n    from: server_metrics\n    via:\n      - type: mapValues\n        mapper: aggregate_metrics\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: windowed_metrics\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Time window calculation and management</li> <li>Running aggregations (min, max, sum, count)</li> <li>Memory-efficient value storage</li> </ul>"},{"location":"tutorials/advanced/custom-state-stores/#session-store","title":"Session Store","text":"<p>Session stores organize data by session windows, automatically handling session boundaries based on inactivity gaps.</p> <p>What it does:</p> <ul> <li>Produces click events: Creates user page visits with timestamps, occasionally adding 1-5 second gaps (20% chance) to simulate session breaks</li> <li>Tracks session timeouts: Uses 30-second inactivity threshold - if time since last click &gt; 30s, starts new session and increments session counter</li> <li>Stores session state: Keeps running data per user including current session ID, start time, page count, total duration, devices used</li> <li>Detects session boundaries: When timeout exceeded, logs session end, resets counters, starts fresh session with new session ID</li> <li>Outputs session analytics: Returns comprehensive session data showing current session metrics, lifetime totals, device/page patterns, and conversion events</li> </ul> User Clicks Producer - click to expand <pre><code># Producer for session store demo - generates user click events with gaps\n\nfunctions:\n  generate_click_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      click_counter = 0\n      users = [\"user_A\", \"user_B\", \"user_C\"]\n      pages = [\"/home\", \"/products\", \"/cart\", \"/checkout\", \"/profile\"]\n    code: |\n      global click_counter, users, pages\n\n      click_counter += 1\n      user_id = random.choice(users)\n      page = random.choice(pages)\n\n      # Simulate session gaps by occasionally pausing\n      gap_probability = 0.2  # 20% chance of gap\n      has_gap = False\n      gap_duration = 0\n      if random.random() &lt; gap_probability:\n        # Simulate longer gap between sessions\n        gap_duration = random.randint(1, 5)  # 1-5 second gap\n        has_gap = True\n        log.info(\"Simulating {}s gap for user {}\", gap_duration, user_id)\n\n      current_timestamp = int(time.time() * 1000)\n      duration_on_page = random.randint(1000, 30000)  # 1-30 seconds\n\n      # Create structured JSON click event for better readability in Kowl UI\n      click_event = {\n        \"user_id\": user_id,\n        \"page\": page,\n        \"timestamp\": current_timestamp,\n        \"click_id\": f\"click_{click_counter:04d}\",\n        \"duration_on_page\": duration_on_page,\n        \"sequence_number\": click_counter,\n        \"user_agent\": random.choice([\"Chrome/91.0\", \"Firefox/89.0\", \"Safari/14.1\", \"Edge/91.0\"]),\n        \"device_info\": {\n          \"type\": random.choice([\"desktop\", \"tablet\", \"mobile\"]),\n          \"os\": random.choice([\"Windows\", \"MacOS\", \"Linux\", \"iOS\", \"Android\"]),\n          \"screen_resolution\": random.choice([\"1920x1080\", \"1366x768\", \"1440x900\", \"375x812\"])\n        },\n        \"referrer\": random.choice([None, \"/home\", \"/products\", \"/search\", \"external\"]),\n        \"session_info\": {\n          \"has_simulated_gap\": has_gap,\n          \"gap_duration_hint\": gap_duration if has_gap else 0,\n          \"expected_session_timeout_ms\": 30000\n        },\n        \"page_metadata\": {\n          \"category\": {\n            \"/home\": \"landing\",\n            \"/products\": \"catalog\", \n            \"/cart\": \"commerce\",\n            \"/checkout\": \"commerce\",\n            \"/profile\": \"account\"\n          }.get(page, \"other\"),\n          \"requires_auth\": page in [\"/cart\", \"/checkout\", \"/profile\"],\n          \"is_conversion_page\": page == \"/checkout\"\n        },\n        \"interaction\": {\n          \"click_type\": random.choice([\"navigation\", \"button\", \"link\", \"form\"]),\n          \"coordinates\": {\n            \"x\": random.randint(0, 1920),\n            \"y\": random.randint(0, 1080)\n          }\n        },\n        \"metadata\": {\n          \"simulation\": True,\n          \"session_tracking\": True,\n          \"gap_simulation\": has_gap\n        }\n      }\n\n    expression: (user_id, click_event)\n    resultType: (string, json)\n\nproducers:\n  click_event_producer:\n    generator: generate_click_events\n    interval: 3s  # 3 second intervals to create natural session boundaries\n    to:\n      topic: user_clicks\n      keyType: string\n      valueType: json\n</code></pre> Session Store Processor - click to expand <pre><code># Processor demonstrating session store for session-based analytics\n\nstreams:\n  user_clicks:\n    topic: user_clicks\n    keyType: string\n    valueType: json\n\nstores:\n  user_session_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  track_user_sessions:\n    type: valueTransformer\n    stores:\n      - user_session_store\n    code: |\n      import json\n      import time\n\n      # Extract fields from JSON click event\n      if not value:\n        return None\n\n      page = value.get(\"page\")\n      duration = value.get(\"duration_on_page\")\n      timestamp = value.get(\"timestamp\")\n      click_id = value.get(\"click_id\")\n      sequence_number = value.get(\"sequence_number\")\n      user_agent = value.get(\"user_agent\")\n      device_info = value.get(\"device_info\", {})\n      referrer = value.get(\"referrer\")\n      session_info = value.get(\"session_info\", {})\n      page_metadata = value.get(\"page_metadata\", {})\n      interaction = value.get(\"interaction\", {})\n\n      if not page or duration is None or not timestamp:\n        return None\n\n      # Session timeout: 30 seconds of inactivity\n      session_timeout_ms = 30 * 1000\n      current_time = int(time.time() * 1000)\n\n      # Get existing session data\n      session_data_str = user_session_store.get(key)\n      if session_data_str:\n        session_data = json.loads(session_data_str)\n      else:\n        session_data = {\n          \"user_id\": key,\n          \"current_session_id\": None,\n          \"session_start\": None,\n          \"last_activity\": 0,\n          \"session_page_count\": 0,\n          \"session_total_duration\": 0,\n          \"total_sessions\": 0,\n          \"pages_visited\": [],\n          \"devices_used\": [],\n          \"user_agents\": [],\n          \"page_categories\": [],\n          \"conversion_events\": 0,\n          \"referrer_sources\": []\n        }\n\n      # Check if this starts a new session\n      last_activity = session_data.get(\"last_activity\", 0)\n      time_since_last = timestamp - last_activity\n      session_ended = False\n\n      if (session_data[\"current_session_id\"] is None or \n          time_since_last &gt; session_timeout_ms):\n\n        # End previous session if exists\n        if session_data[\"current_session_id\"] is not None:\n          session_duration = last_activity - session_data[\"session_start\"]\n          log.info(\"Session ended for {}: duration={}ms, pages={}\", \n                   key, session_duration, session_data[\"session_page_count\"])\n          session_data[\"total_sessions\"] += 1\n          session_ended = True\n\n        # Start new session\n        session_data[\"current_session_id\"] = f\"session_{key}_{timestamp}\"\n        session_data[\"session_start\"] = timestamp\n        session_data[\"session_page_count\"] = 0\n        session_data[\"session_total_duration\"] = 0\n        session_data[\"pages_visited\"] = []\n\n        log.info(\"New session started for {}: {}\", key, session_data[\"current_session_id\"])\n\n      # Update current session\n      session_data[\"last_activity\"] = timestamp\n      session_data[\"session_page_count\"] += 1\n      session_data[\"session_total_duration\"] += duration\n      session_data[\"pages_visited\"].append(page)\n\n      # Track additional session metadata\n      device_type = device_info.get(\"type\")\n      if device_type and device_type not in session_data[\"devices_used\"]:\n        session_data[\"devices_used\"].append(device_type)\n\n      if user_agent and user_agent not in session_data[\"user_agents\"]:\n        session_data[\"user_agents\"].append(user_agent)\n\n      page_category = page_metadata.get(\"category\")\n      if page_category and page_category not in session_data[\"page_categories\"]:\n        session_data[\"page_categories\"].append(page_category)\n\n      if page_metadata.get(\"is_conversion_page\", False):\n        session_data[\"conversion_events\"] += 1\n\n      if referrer and referrer != \"None\" and referrer not in session_data[\"referrer_sources\"]:\n        session_data[\"referrer_sources\"].append(referrer)\n\n      # Store updated session\n      user_session_store.put(key, json.dumps(session_data))\n\n      # Calculate session metrics\n      session_duration_so_far = timestamp - session_data[\"session_start\"]\n\n      # Generate comprehensive session analytics result\n      result = {\n        \"analytics_type\": \"SESSION_ANALYTICS\",\n        \"user_id\": key,\n        \"session_id\": session_data[\"current_session_id\"],\n        \"session_status\": \"ended\" if session_ended else \"active\",\n        \"current_event\": {\n          \"click_id\": click_id,\n          \"page\": page,\n          \"page_category\": page_category,\n          \"duration_on_page\": duration,\n          \"timestamp\": timestamp,\n          \"sequence_number\": sequence_number,\n          \"interaction_type\": interaction.get(\"click_type\"),\n          \"requires_auth\": page_metadata.get(\"requires_auth\", False),\n          \"is_conversion\": page_metadata.get(\"is_conversion_page\", False)\n        },\n        \"session_metrics\": {\n          \"pages_visited_count\": session_data[\"session_page_count\"],\n          \"total_time_ms\": session_data[\"session_total_duration\"],\n          \"session_duration_ms\": session_duration_so_far,\n          \"pages_visited\": session_data[\"pages_visited\"][-5:],  # Last 5 pages\n          \"unique_categories\": session_data[\"page_categories\"],\n          \"conversion_events\": session_data[\"conversion_events\"],\n          \"session_start\": session_data[\"session_start\"],\n          \"last_activity\": session_data[\"last_activity\"]\n        },\n        \"user_profile\": {\n          \"total_sessions\": session_data[\"total_sessions\"],\n          \"devices_used\": session_data[\"devices_used\"],\n          \"user_agents\": session_data[\"user_agents\"],\n          \"referrer_sources\": session_data[\"referrer_sources\"]\n        },\n        \"device_context\": {\n          \"current_device\": device_info,\n          \"current_user_agent\": user_agent,\n          \"current_referrer\": referrer\n        },\n        \"session_insights\": {\n          \"time_since_last_activity\": time_since_last,\n          \"session_timeout_ms\": session_timeout_ms,\n          \"has_simulated_gap\": session_info.get(\"has_simulated_gap\", False),\n          \"avg_time_per_page\": session_data[\"session_total_duration\"] / session_data[\"session_page_count\"] if session_data[\"session_page_count\"] &gt; 0 else 0\n        }\n      }\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  session_analytics_pipeline:\n    from: user_clicks\n    via:\n      - type: mapValues\n        mapper: track_user_sessions\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: session_analytics\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Session timeout handling</li> <li>Automatic session boundary detection</li> <li>Session lifecycle management</li> </ul>"},{"location":"tutorials/advanced/custom-state-stores/#optimized-store-configuration","title":"Optimized Store Configuration","text":"<p>For high-volume scenarios, proper store configuration is crucial for performance.</p> <p>What it does:</p> <ul> <li>Produces device events: Creates high-frequency events (sensor_reading, status_update, error, heartbeat) for multiple devices with facility/zone info</li> <li>Stores compact state: Keeps minimal JSON per device with just current status, last_temp, error_count, heartbeat_count, location info</li> <li>Processes selectively: Updates state for all events, but only outputs alerts when specific conditions met (temp &gt;75\u00b0C, errors, status changes)</li> <li>Optimizes for volume: Uses efficient JSON storage, processes fast, emits only critical alerts to reduce downstream message volume  </li> <li>Tracks device health: Monitors temperature trends, error accumulation, heartbeat patterns, status transitions with location context</li> </ul> High Volume Events Producer - click to expand <pre><code># Producer for optimized store demo - generates high-volume events\n\nfunctions:\n  generate_high_volume_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      device_ids = [f\"device_{i:03d}\" for i in range(1, 21)]  # 20 devices\n      event_types = [\"sensor_reading\", \"status_update\", \"error\", \"heartbeat\"]\n    code: |\n      global event_counter, device_ids, event_types\n\n      event_counter += 1\n      device_id = random.choice(device_ids)\n      event_type = random.choice(event_types)\n\n      # Generate JSON event data for better readability in Kowl UI\n      current_timestamp = int(time.time() * 1000)\n\n      if event_type == \"sensor_reading\":\n        value_data = round(random.uniform(20.0, 80.0), 2)  # Temperature\n        unit = \"celsius\"\n      elif event_type == \"status_update\":\n        value_data = random.choice([\"online\", \"offline\", \"maintenance\"])\n        unit = None\n      elif event_type == \"error\":\n        value_data = f\"error_code_{random.randint(100, 999)}\"\n        unit = None\n      else:  # heartbeat\n        value_data = \"ok\"\n        unit = None\n\n      # Create structured JSON event for better readability in Kowl UI\n      device_event = {\n        \"device_id\": device_id,\n        \"event_type\": event_type,\n        \"value\": value_data,\n        \"timestamp\": current_timestamp,\n        \"event_id\": f\"evt_{event_counter:06d}\",\n        \"unit\": unit,\n        \"facility\": random.choice([\"factory_a\", \"factory_b\", \"warehouse_c\"]),\n        \"zone\": random.choice([\"zone_1\", \"zone_2\", \"zone_3\", \"zone_4\"]),\n        \"metadata\": {\n          \"simulation\": True,\n          \"high_volume\": True,\n          \"optimized_processing\": True,\n          \"sequence\": event_counter\n        }\n      }\n\n    expression: (device_id, device_event)\n    resultType: (string, json)\n\nproducers:\n  high_volume_producer:\n    generator: generate_high_volume_events\n    interval: 1s  # High frequency\n    to:\n      topic: device_events\n      keyType: string\n      valueType: json\n</code></pre> Optimized Store Processor - click to expand <pre><code># Processor demonstrating optimized state store configuration\n\nstreams:\n  device_events:\n    topic: device_events\n    keyType: string\n    valueType: json\n\nstores:\n  device_state_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n    # Optimized for high-volume scenarios\n\nfunctions:\n  process_device_events:\n    type: valueTransformer\n    stores:\n      - device_state_store\n    code: |\n      import json\n      import time\n\n      # Extract fields from JSON event\n      if not value:\n        return None\n\n      event_type = value.get(\"event_type\")\n      event_value = value.get(\"value\")\n      timestamp = value.get(\"timestamp\")\n      event_id = value.get(\"event_id\")\n      facility = value.get(\"facility\")\n      zone = value.get(\"zone\")\n\n      if not event_type or event_value is None or not timestamp:\n        return None\n\n      # Get existing device state\n      state_str = device_state_store.get(key)\n      if state_str:\n        device_state = json.loads(state_str)\n      else:\n        device_state = {\n          \"device_id\": key,\n          \"status\": \"unknown\", \n          \"last_temp\": None, \n          \"error_count\": 0, \n          \"last_seen\": timestamp,\n          \"heartbeat_count\": 0,\n          \"facility\": facility,\n          \"zone\": zone\n        }\n\n      # Process event efficiently with JSON output\n      result_event = None\n\n      if event_type == \"sensor_reading\":\n        device_state[\"last_temp\"] = float(event_value)\n        device_state[\"status\"] = \"active\"\n\n        # Check for temperature alerts\n        if device_state[\"last_temp\"] &gt; 75:\n          result_event = {\n            \"alert_type\": \"TEMPERATURE_ALERT\",\n            \"device_id\": key,\n            \"temperature\": device_state[\"last_temp\"],\n            \"threshold\": 75,\n            \"facility\": facility,\n            \"zone\": zone,\n            \"event_id\": event_id,\n            \"timestamp\": timestamp\n          }\n          log.warn(\"Temperature alert for device {}: {:.1f}C\", key, device_state[\"last_temp\"])\n\n      elif event_type == \"status_update\":\n        device_state[\"status\"] = event_value\n        result_event = {\n          \"alert_type\": \"STATUS_UPDATE\",\n          \"device_id\": key,\n          \"status\": event_value,\n          \"facility\": facility,\n          \"zone\": zone,\n          \"event_id\": event_id,\n          \"timestamp\": timestamp\n        }\n\n      elif event_type == \"error\":\n        device_state[\"error_count\"] += 1\n        device_state[\"status\"] = \"error\"\n        result_event = {\n          \"alert_type\": \"ERROR\",\n          \"device_id\": key,\n          \"error_code\": event_value,\n          \"error_count\": device_state[\"error_count\"],\n          \"facility\": facility,\n          \"zone\": zone,\n          \"event_id\": event_id,\n          \"timestamp\": timestamp\n        }\n        log.error(\"Error on device {}: {} (total errors: {})\", key, event_value, device_state[\"error_count\"])\n\n      elif event_type == \"heartbeat\":\n        device_state[\"status\"] = \"online\"\n        device_state[\"heartbeat_count\"] = device_state.get(\"heartbeat_count\", 0) + 1\n        # Only emit heartbeat summary every 10th heartbeat to reduce output volume\n        if device_state[\"heartbeat_count\"] % 10 == 0:\n          result_event = {\n            \"alert_type\": \"HEARTBEAT_SUMMARY\",\n            \"device_id\": key,\n            \"status\": \"online\",\n            \"heartbeat_count\": device_state[\"heartbeat_count\"],\n            \"facility\": facility,\n            \"zone\": zone,\n            \"timestamp\": timestamp\n          }\n\n      # Update last seen and store state\n      device_state[\"last_seen\"] = timestamp\n      device_state_store.put(key, json.dumps(device_state))\n\n      return result_event\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  optimized_processing_pipeline:\n    from: device_events\n    via:\n      - type: mapValues\n        mapper: process_device_events\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: device_alerts\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Compact state representation for performance</li> <li>Selective event emission</li> <li>Error counting and alerting</li> </ul>"},{"location":"tutorials/advanced/custom-state-stores/#multi-store-pattern","title":"Multi-Store Pattern","text":"<p>Complex applications often require multiple state stores working together to manage different aspects of state.</p> <p>What it does:</p> <ul> <li>Produces order events: Creates order status updates (created, shipped, delivered) with product IDs, quantities, prices as pipe-delimited strings</li> <li>Uses three state stores: Updates order_state_store (current order status), customer_metrics_store (totals per customer), product_inventory_store (stock levels)</li> <li>Coordinates updates: For each order event, atomically updates all three stores - order status, customer spending totals, inventory levels</li> <li>Tracks relationships: Maps orders to customers via hash function, maintains order history lists, tracks inventory changes per product</li> <li>Outputs comprehensive results: Returns formatted string combining data from all three stores showing order details, customer analytics, and inventory impact</li> </ul> Order Events Producer - click to expand <pre><code># Producer for multi-store demo - generates order processing events as pipe-delimited strings\n\nfunctions:\n  generate_order_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      order_counter = 0\n      products = [\"prod_A\", \"prod_B\", \"prod_C\", \"prod_D\"]\n      statuses = [\"created\", \"paid\", \"shipped\", \"delivered\", \"cancelled\"]\n    code: |\n      global order_counter, products, statuses\n\n      order_counter += 1\n\n      # Generate order event as pipe-delimited string: \"status:product_id:quantity:price:timestamp\"\n      status = random.choice(statuses)\n      product_id = random.choice(products)\n      quantity = random.randint(1, 5)\n      price = round(random.uniform(10.0, 100.0), 2)\n      timestamp = int(time.time() * 1000)\n\n      # Create pipe-delimited string format expected by processor\n      order_value = f\"{status}:{product_id}:{quantity}:{price}:{timestamp}\"\n      order_key = f\"order_{order_counter:04d}\"\n\n      log.info(\"Generating order: key={}, value={}\", order_key, order_value)\n\n    expression: (order_key, order_value)\n    resultType: (string, string)\n\nproducers:\n  order_event_producer:\n    generator: generate_order_events\n    interval: 2s\n    to:\n      topic: order_events\n      keyType: string\n      valueType: string\n</code></pre> Multi-Store Processor - click to expand <pre><code># Processor demonstrating multi-store pattern for complex order processing\n\nstreams:\n  order_events:\n    topic: order_events\n    keyType: string\n    valueType: string\n\nstores:\n  # Store for current order state\n  order_state_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\n  # Store for customer metrics\n  customer_metrics_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\n  # Store for product inventory tracking\n  product_inventory_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  process_order_with_multiple_stores:\n    type: valueTransformer\n    stores:\n      - order_state_store\n      - customer_metrics_store\n      - product_inventory_store\n    code: |\n      import json\n\n      # Parse order event: \"status:product_id:quantity:price:timestamp\"\n      parts = value.split(\":\")\n      if len(parts) != 5:\n        return None\n\n      status = parts[0]\n      product_id = parts[1]\n      quantity = int(parts[2])\n      price = float(parts[3])\n      timestamp = int(parts[4])\n\n      order_id = key\n\n      # 1. Update order state store\n      order_state = {\n        \"order_id\": order_id,\n        \"status\": status,\n        \"product_id\": product_id,\n        \"quantity\": quantity,\n        \"price\": price,\n        \"last_updated\": timestamp\n      }\n      order_state_store.put(order_id, json.dumps(order_state))\n\n      # 2. Update customer metrics (extract customer from order_id or use a lookup)\n      customer_id = f\"cust_{hash(order_id) % 4 + 1:03d}\"  # Simple customer mapping\n\n      customer_data_str = customer_metrics_store.get(customer_id)\n      if customer_data_str:\n        customer_data = json.loads(customer_data_str)\n      else:\n        customer_data = {\n          \"customer_id\": customer_id,\n          \"total_orders\": 0,\n          \"total_spent\": 0,\n          \"order_history\": []\n        }\n\n      # Update customer metrics based on status\n      if status == \"created\":\n        customer_data[\"total_orders\"] += 1\n        customer_data[\"order_history\"].append(order_id)\n        # Keep only last 10 orders for memory efficiency\n        if len(customer_data[\"order_history\"]) &gt; 10:\n          customer_data[\"order_history\"] = customer_data[\"order_history\"][-10:]\n\n      elif status == \"paid\":\n        customer_data[\"total_spent\"] += price * quantity\n\n      customer_metrics_store.put(customer_id, json.dumps(customer_data))\n\n      # 3. Update product inventory\n      inventory_data_str = product_inventory_store.get(product_id)\n      if inventory_data_str:\n        inventory_data = json.loads(inventory_data_str)\n      else:\n        inventory_data = {\n          \"product_id\": product_id,\n          \"reserved_quantity\": 0,\n          \"sold_quantity\": 0,\n          \"available_stock\": 100  # Default stock\n        }\n\n      # Update inventory based on order status\n      if status == \"created\":\n        inventory_data[\"reserved_quantity\"] += quantity\n        inventory_data[\"available_stock\"] -= quantity\n      elif status == \"shipped\":\n        inventory_data[\"reserved_quantity\"] -= quantity\n        inventory_data[\"sold_quantity\"] += quantity\n      elif status == \"cancelled\":\n        inventory_data[\"reserved_quantity\"] -= quantity\n        inventory_data[\"available_stock\"] += quantity\n\n      product_inventory_store.put(product_id, json.dumps(inventory_data))\n\n      # Generate comprehensive order summary using all stores\n      summary = {\n        \"order_id\": order_id,\n        \"status\": status,\n        \"customer_total_orders\": customer_data[\"total_orders\"],\n        \"customer_total_spent\": customer_data[\"total_spent\"],\n        \"product_available\": inventory_data[\"available_stock\"],\n        \"product_reserved\": inventory_data[\"reserved_quantity\"]\n      }\n\n      result = f\"ORDER_PROCESSED:{order_id}:status={status}:customer_orders={customer_data['total_orders']}:stock_left={inventory_data['available_stock']}\"\n\n      log.info(\"Processed order {} with status {}: customer has {} orders, product {} has {} stock left\", \n               order_id, status, customer_data[\"total_orders\"], product_id, inventory_data[\"available_stock\"])\n\n      return result\n\n    expression: result if result else None\n    resultType: string\n\npipelines:\n  multi_store_order_processing:\n    from: order_events\n    via:\n      - type: mapValues\n        mapper: process_order_with_multiple_stores\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: order_processing_results\n      keyType: string\n      valueType: string\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Multiple state stores in single function</li> <li>Coordinated state updates</li> <li>Cross-store data correlation</li> </ul>"},{"location":"tutorials/advanced/custom-state-stores/#state-store-types-summary","title":"State Store Types Summary","text":"Store Type Use Case Key Features Key-Value General state, caching, counters Simple key-to-value mapping Window Time-based aggregations Automatic time partitioning Session User sessions, activity tracking Inactivity-based boundaries"},{"location":"tutorials/advanced/custom-state-stores/#conclusion","title":"Conclusion","text":"<p>Custom state stores in KSML provide powerful capabilities for building stateful stream processing applications. By understanding the different store types, configuration options, and optimization techniques, you can build efficient and scalable applications that maintain state effectively across events.</p> <p>For foundational concepts and basic configuration patterns, refer back to the State Stores Tutorial.</p>"},{"location":"tutorials/advanced/external-integration/","title":"Integration with External Systems in KSML","text":"<p>This tutorial covers how to integrate KSML with external systems like databases and APIs using JSON data formats for better observability.</p>"},{"location":"tutorials/advanced/external-integration/#introduction","title":"Introduction","text":"<p>Stream processing often needs external data. This tutorial shows patterns for enriching events with database lookups, API calls, and async integrations.</p>"},{"location":"tutorials/advanced/external-integration/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic enriched_user_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic enriched_product_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic order_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic external_requests &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic external_responses &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/advanced/external-integration/#integration-patterns","title":"Integration Patterns","text":""},{"location":"tutorials/advanced/external-integration/#api-enrichment-pattern","title":"API Enrichment Pattern","text":"<p>The API enrichment pattern calls external REST APIs to add additional data to streaming events. This is useful when you need real-time data that can't be cached locally.</p> <p>What it does:</p> <ul> <li>Produces user events: Creates events (login, purchase, page_view) with user IDs, session info, device details, page URLs, referrer sources</li> <li>Calls mock API: For each user_id, fetches profile from hardcoded lookup table simulating external REST API call with user details</li> <li>Handles API failures: Uses try-catch to gracefully handle missing users, returning fallback \"Unknown User\" profile data</li> <li>Enriches with profiles: Combines original event data with fetched profile (name, tier, location, preferences, lifetime_value) </li> <li>Outputs enriched events: Returns JSON combining event details with user profile, API call timing, and computed recommendations</li> </ul> <p>Key concepts demonstrated:</p> <ul> <li>Making external API calls from KSML functions with JSON data structures</li> <li>Handling API failures gracefully with structured fallback strategies</li> <li>Managing API latency and timeouts in stream processing</li> <li>Enriching events with external data while maintaining stream processing semantics</li> </ul> User Events Producer (API enrichment demo) - click to expand <pre><code># Producer for API enrichment demo - generates user events that need external data\n\nfunctions:\n  generate_user_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      user_ids = [\"user_001\", \"user_002\", \"user_003\", \"user_004\", \"user_005\"]\n      event_types = [\"login\", \"view_product\", \"add_to_cart\", \"purchase\"]\n    code: |\n      global event_counter, user_ids, event_types\n\n      event_counter += 1\n      user_id = random.choice(user_ids)\n      event_type = random.choice(event_types)\n\n      # Create structured JSON user event for better readability in Kowl UI\n      user_event = {\n        \"event_id\": f\"event_{event_counter:06d}\",\n        \"event_type\": event_type,\n        \"user_id\": user_id,\n        \"timestamp\": int(time.time() * 1000),\n        \"sequence_number\": event_counter,\n        \"session_id\": f\"session_{user_id}_{event_counter // 5}\",  # Change session every 5 events\n        \"device_info\": {\n          \"platform\": random.choice([\"web\", \"mobile\", \"tablet\"]),\n          \"user_agent\": random.choice([\"Chrome\", \"Firefox\", \"Safari\", \"Edge\"]),\n          \"ip_address\": f\"192.168.1.{random.randint(1, 255)}\"\n        },\n        \"context\": {\n          \"page_url\": random.choice([\"/home\", \"/products\", \"/cart\", \"/checkout\"]),\n          \"referrer\": random.choice([None, \"google.com\", \"facebook.com\", \"direct\"]),\n          \"campaign\": random.choice([None, \"summer_sale\", \"new_user\", \"retargeting\"])\n        },\n        \"metadata\": {\n          \"simulation\": True,\n          \"api_enrichment\": True,\n          \"needs_external_data\": True\n        }\n      }\n\n    expression: (f\"event_{event_counter:06d}\", user_event)\n    resultType: (string, json)\n\nproducers:\n  user_event_producer:\n    generator: generate_user_events\n    interval: 3s\n    to:\n      topic: user_events\n      keyType: string\n      valueType: json\n</code></pre> API Enrichment Processor (external API calls) - click to expand <pre><code># Processor demonstrating API enrichment pattern\n\nstreams:\n  user_events:\n    topic: user_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  enrich_with_api_data:\n    type: valueTransformer\n    globalCode: |\n      import time\n      import random\n\n      # Mock API client (simulates external REST API)\n      def get_user_profile(user_id):\n        \"\"\"Simulate API call to get user profile\"\"\"\n        try:\n          # Simulate API latency\n          time.sleep(0.1)\n\n          # Mock user profiles based on user_id\n          profiles = {\n            \"user_001\": {\n              \"name\": \"Alice Johnson\", \"tier\": \"premium\", \"location\": \"New York\",\n              \"age\": 32, \"email\": \"alice@example.com\", \"preferences\": [\"tech\", \"gaming\"],\n              \"lifetime_value\": 2500.00, \"registration_date\": \"2022-01-15\"\n            },\n            \"user_002\": {\n              \"name\": \"Bob Smith\", \"tier\": \"standard\", \"location\": \"London\", \n              \"age\": 28, \"email\": \"bob@example.com\", \"preferences\": [\"sports\", \"music\"],\n              \"lifetime_value\": 850.00, \"registration_date\": \"2023-03-22\"\n            },\n            \"user_003\": {\n              \"name\": \"Charlie Davis\", \"tier\": \"premium\", \"location\": \"Tokyo\",\n              \"age\": 45, \"email\": \"charlie@example.com\", \"preferences\": [\"travel\", \"food\"],\n              \"lifetime_value\": 3200.00, \"registration_date\": \"2021-11-08\"\n            },\n            \"user_004\": {\n              \"name\": \"Diana Wilson\", \"tier\": \"basic\", \"location\": \"Sydney\",\n              \"age\": 24, \"email\": \"diana@example.com\", \"preferences\": [\"fashion\", \"art\"],\n              \"lifetime_value\": 320.00, \"registration_date\": \"2024-01-10\"\n            },\n            \"user_005\": {\n              \"name\": \"Eve Brown\", \"tier\": \"standard\", \"location\": \"Berlin\",\n              \"age\": 35, \"email\": \"eve@example.com\", \"preferences\": [\"books\", \"movies\"],\n              \"lifetime_value\": 1150.00, \"registration_date\": \"2022-08-17\"\n            }\n          }\n\n          profile = profiles.get(user_id, {\n            \"name\": \"Unknown User\", \"tier\": \"basic\", \"location\": \"Unknown\",\n            \"age\": 0, \"email\": \"unknown@example.com\", \"preferences\": [],\n            \"lifetime_value\": 0.0, \"registration_date\": \"unknown\"\n          })\n          log.info(\"Fetched profile for user {}: {}\", user_id, profile[\"name\"])\n          return profile\n        except Exception as e:\n          log.warn(\"API request failed for user {}: {}\", user_id, str(e))\n          return None\n\n    code: |\n      import time\n\n      # Extract fields from JSON event\n      if not value:\n        return None\n\n      event_id = value.get(\"event_id\")\n      event_type = value.get(\"event_type\")\n      user_id = value.get(\"user_id\")\n      timestamp = value.get(\"timestamp\")\n      sequence_number = value.get(\"sequence_number\")\n      session_id = value.get(\"session_id\")\n      device_info = value.get(\"device_info\", {})\n      context = value.get(\"context\", {})\n      metadata = value.get(\"metadata\", {})\n\n      if not event_type or not user_id or not timestamp:\n        return None\n\n      # Call API to get user profile data\n      api_start_time = int(time.time() * 1000)\n      profile_data = get_user_profile(user_id)\n      api_end_time = int(time.time() * 1000)\n      api_latency = api_end_time - api_start_time\n\n      # Create enriched event with comprehensive data structure\n      if profile_data:\n        enriched_event = {\n          \"enrichment_status\": \"SUCCESS\",\n          \"enrichment_type\": \"API_PROFILE_DATA\",\n          \"original_event\": {\n            \"event_id\": event_id,\n            \"event_type\": event_type,\n            \"user_id\": user_id,\n            \"timestamp\": timestamp,\n            \"sequence_number\": sequence_number,\n            \"session_id\": session_id,\n            \"device_info\": device_info,\n            \"context\": context\n          },\n          \"enriched_data\": {\n            \"user_profile\": profile_data,\n            \"computed_metrics\": {\n              \"user_tier_level\": {\"premium\": 3, \"standard\": 2, \"basic\": 1}.get(profile_data.get(\"tier\"), 0),\n              \"is_high_value\": profile_data.get(\"lifetime_value\", 0) &gt; 1000,\n              \"account_age_days\": (timestamp - 1640995200000) // (24 * 60 * 60 * 1000) if profile_data.get(\"registration_date\") != \"unknown\" else 0\n            },\n            \"recommendations\": {\n              \"personalized\": profile_data.get(\"preferences\", [])[:2] if profile_data.get(\"preferences\") else [],\n              \"tier_benefits\": f\"Available benefits for {profile_data.get('tier', 'basic')} tier\",\n              \"location_offers\": f\"Special offers in {profile_data.get('location', 'Unknown')}\"\n            }\n          },\n          \"api_metrics\": {\n            \"api_call_duration_ms\": api_latency,\n            \"api_endpoint\": \"mock_user_profile_api\",\n            \"api_success\": True,\n            \"cache_hit\": False  # This is a live API call\n          },\n          \"processing_info\": {\n            \"enrichment_timestamp\": api_end_time,\n            \"processor_version\": \"1.0\",\n            \"enrichment_rules\": [\"basic_profile\", \"tier_computation\", \"recommendations\"]\n          }\n        }\n        return enriched_event\n      else:\n        # Return event with failure information if API fails\n        fallback_event = {\n          \"enrichment_status\": \"FAILED\", \n          \"enrichment_type\": \"API_PROFILE_DATA\",\n          \"original_event\": {\n            \"event_id\": event_id,\n            \"event_type\": event_type,\n            \"user_id\": user_id,\n            \"timestamp\": timestamp,\n            \"sequence_number\": sequence_number,\n            \"session_id\": session_id,\n            \"device_info\": device_info,\n            \"context\": context\n          },\n          \"enriched_data\": {\n            \"user_profile\": None,\n            \"fallback_data\": {\n              \"default_tier\": \"basic\",\n              \"estimated_location\": \"Unknown\",\n              \"default_preferences\": []\n            }\n          },\n          \"api_metrics\": {\n            \"api_call_duration_ms\": api_latency,\n            \"api_endpoint\": \"mock_user_profile_api\", \n            \"api_success\": False,\n            \"error_reason\": \"Profile not found or API unavailable\"\n          },\n          \"processing_info\": {\n            \"enrichment_timestamp\": api_end_time,\n            \"processor_version\": \"1.0\",\n            \"fallback_applied\": True\n          }\n        }\n        return fallback_event\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  api_enrichment_pipeline:\n    from: user_events\n    via:\n      - type: mapValues\n        mapper: enrich_with_api_data\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: enriched_user_events\n      keyType: string\n      valueType: json\n</code></pre> <p>API enrichment benefits:</p> <ul> <li>Real-time enrichment: Access to the most current external data</li> <li>Flexible data sources: Can integrate with any REST API</li> <li>Graceful degradation: Continues processing even when external systems fail</li> <li>Simple implementation: Straightforward request-response pattern</li> </ul>"},{"location":"tutorials/advanced/external-integration/#database-lookup-pattern","title":"Database Lookup Pattern","text":"<p>This pattern shows how to enrich streaming events with data that would normally come from a database.</p> <p>What happens in this example:</p> <p>Imagine you have an e-commerce website. Users view products, add them to cart, and make purchases. Your stream only has basic event data like <code>{\"product_id\": \"PROD001\", \"event_type\": \"purchased\", \"quantity\": 2}</code>. But you want to know the product name, price, and category too.</p> <p>The Producer creates these basic product events every 2 seconds: <pre><code>{\"product_id\": \"PROD001\", \"event_type\": \"purchased\", \"quantity\": 2, \"user_id\": \"user_1234\"}\n</code></pre></p> <p>The Processor enriches each event by:</p> <ol> <li>First time only: Loads a product catalog into memory (like a mini database):<ul> <li>PROD001 \u2192 \"Wireless Headphones\", $99.99, \"Electronics\"  </li> <li>PROD002 \u2192 \"Coffee Mug\", $12.50, \"Kitchen\"</li> </ul> </li> <li>For each event: Looks up the product_id and adds the details:    <pre><code>{\n  \"product_id\": \"PROD001\", \n  \"event_type\": \"purchased\", \n  \"quantity\": 2,\n  \"enriched_data\": {\n    \"name\": \"Wireless Headphones\",\n    \"category\": \"Electronics\", \n    \"unit_price\": 99.99,\n    \"total_price\": 199.98\n  }\n}\n</code></pre></li> </ol> <p>This way you get rich product information without hitting a database for every single event.</p> Product Events Producer (database lookup demo) - click to expand <pre><code># Producer for database lookup demo - generates product events\n\nfunctions:\n  generate_product_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      product_ids = [\"PROD001\", \"PROD002\", \"PROD003\", \"PROD004\", \"PROD005\"]\n      event_types = [\"viewed\", \"added_to_cart\", \"purchased\", \"removed_from_cart\"]\n    code: |\n      global event_counter, product_ids, event_types\n\n      event_counter += 1\n      product_id = random.choice(product_ids)\n      event_type = random.choice(event_types)\n      quantity = random.randint(1, 5) if event_type in [\"added_to_cart\", \"purchased\"] else 1\n      current_timestamp = int(time.time() * 1000)\n\n      # Create structured JSON product event for better readability in Kowl UI\n      product_event = {\n        \"event_id\": f\"product_event_{event_counter:06d}\",\n        \"event_type\": event_type,\n        \"product_id\": product_id,\n        \"quantity\": quantity,\n        \"timestamp\": current_timestamp,\n        \"user_id\": f\"user_{random.randint(1000, 9999)}\",\n        \"session_id\": f\"session_{event_counter // 10}_{random.randint(100, 999)}\",\n        \"page_location\": random.choice([\"/products\", \"/category\", \"/search\"]),\n        \"metadata\": {\n          \"simulation\": True,\n          \"database_lookup\": True\n        }\n      }\n\n    expression: (product_id, product_event)\n    resultType: (string, json)\n\nproducers:\n  product_event_producer:\n    generator: generate_product_events\n    interval: 2s\n    to:\n      topic: product_events\n      keyType: string\n      valueType: json\n</code></pre> Database Lookup Processor (cached reference data) - click to expand <pre><code># Processor demonstrating database lookup pattern with state store caching\n\nstreams:\n  product_events:\n    topic: product_events\n    keyType: string\n    valueType: json\n\nstores:\n  product_reference_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: true\n    caching: true\n\nfunctions:\n  load_product_reference_data:\n    type: forEach\n    globalCode: |\n      import json\n\n      # Track if we've already loaded the data\n      data_loaded = False\n\n      def load_product_catalog():\n        \"\"\"Simulate loading product data from database\"\"\"\n        # Mock product catalog (simulates database query results)\n        products = {\n          \"PROD001\": {\"name\": \"Wireless Headphones\", \"price\": 99.99, \"category\": \"Electronics\"},\n          \"PROD002\": {\"name\": \"Coffee Mug\", \"price\": 12.50, \"category\": \"Kitchen\"},\n          \"PROD003\": {\"name\": \"Running Shoes\", \"price\": 129.99, \"category\": \"Sports\"},\n          \"PROD004\": {\"name\": \"Notebook\", \"price\": 5.99, \"category\": \"Office\"},\n          \"PROD005\": {\"name\": \"Smartphone\", \"price\": 699.99, \"category\": \"Electronics\"}\n        }\n\n        log.info(\"Loaded {} products into reference data store\", len(products))\n        return products\n\n    code: |\n      global data_loaded\n\n      # Only load data once\n      if not data_loaded:\n        # Load product data into state store (simulates database loading)\n        products = load_product_catalog()\n\n        for product_id, product_data in products.items():\n          # Store as JSON string to avoid ForeignObject issues\n          product_reference_store.put(product_id, json.dumps(product_data))\n\n        data_loaded = True\n        log.info(\"Product reference data loaded into state store\")\n    stores:\n      - product_reference_store\n\n  enrich_with_product_data:\n    type: valueTransformer\n    code: |\n      import json\n\n      # Extract fields from JSON product event using .get() method\n      if not value:\n        return None\n\n      event_id = str(value.get(\"event_id\", \"\"))\n      event_type = str(value.get(\"event_type\", \"\"))\n      product_id = str(value.get(\"product_id\", \"\"))\n      quantity = int(value.get(\"quantity\", 0))\n      timestamp = int(value.get(\"timestamp\", 0))\n      user_id = str(value.get(\"user_id\", \"\"))\n      session_id = str(value.get(\"session_id\", \"\"))\n      page_location = str(value.get(\"page_location\", \"\"))\n\n      if not event_type or not product_id or quantity == 0:\n        return None\n\n      # Look up product data from state store (cached database data)\n      product_data_str = product_reference_store.get(product_id)\n\n      if product_data_str:\n        # Parse JSON string to get product data\n        product_data = json.loads(product_data_str)\n\n        # Calculate total price for purchase events\n        unit_price = product_data.get(\"price\", 0.0)\n        total_price = unit_price * quantity if event_type in [\"purchased\", \"added_to_cart\"] else 0.0\n\n        # Create enriched event with product details\n        enriched_event = {\n          \"event_id\": event_id,\n          \"event_type\": event_type,\n          \"product_id\": product_id,\n          \"quantity\": quantity,\n          \"timestamp\": timestamp,\n          \"user_id\": user_id,\n          \"session_id\": session_id,\n          \"page_location\": page_location,\n          \"enriched_data\": {\n            \"name\": product_data.get(\"name\", \"Unknown\"),\n            \"category\": product_data.get(\"category\", \"Unknown\"),\n            \"unit_price\": unit_price,\n            \"total_price\": total_price\n          },\n          \"cache_hit\": True\n        }\n\n        log.info(\"Enriched {} event for product: {} ({})\", event_type, product_data.get(\"name\"), product_id)\n        return enriched_event\n      else:\n        # Product not found in cache\n        fallback_event = {\n          \"event_id\": event_id,\n          \"event_type\": event_type,\n          \"product_id\": product_id,\n          \"quantity\": quantity,\n          \"timestamp\": timestamp,\n          \"user_id\": user_id,\n          \"session_id\": session_id,\n          \"page_location\": page_location,\n          \"enriched_data\": {\n            \"name\": \"Unknown Product\",\n            \"category\": \"Unknown\",\n            \"unit_price\": 0.0,\n            \"total_price\": 0.0\n          },\n          \"cache_hit\": False\n        }\n\n        log.warn(\"Product not found in reference data: {}\", product_id)\n        return fallback_event\n\n    expression: result if result else None\n    resultType: json\n    stores:\n      - product_reference_store\n\npipelines:\n  # Process product events with database lookup\n  product_enrichment_pipeline:\n    from: product_events\n    via:\n      - type: peek\n        forEach: load_product_reference_data\n      - type: mapValues\n        mapper: enrich_with_product_data\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: enriched_product_events\n      keyType: string\n      valueType: json\n</code></pre> <p>Key concepts demonstrated:</p> <ul> <li>Loading reference data into state stores as cache</li> <li>Fast local lookups without external database calls</li> <li>JSON enrichment with cached data</li> </ul>"},{"location":"tutorials/advanced/external-integration/#async-integration-pattern","title":"Async Integration Pattern","text":"<p>The async integration pattern uses separate Kafka topics for communication with external systems, providing loose coupling and better resilience.</p> <p>What it does:</p> <ul> <li>Produces order events: Creates orders with status (created, paid, shipped) containing customer info, amounts, payment methods, business context</li> <li>Filters paid orders: Only processes orders with status=\"paid\", creates external payment processing requests with correlation IDs</li> <li>Sends to request topic: Outputs JSON payment requests to external_requests topic with order details, customer tier, processing priority</li> <li>Processes responses: Reads responses from external_responses topic, matches by correlation_id, handles success/failure scenarios</li> <li>Outputs results: Returns JSON combining original order with external processing results, transaction IDs, and follow-up actions</li> </ul> <p>Key concepts demonstrated:</p> <ul> <li>Creating request topics for external system communication with JSON payloads</li> <li>Generating correlation IDs for request-response tracking</li> <li>Processing responses asynchronously through separate topics with structured data</li> <li>Building event-driven integration patterns using JSON messaging</li> </ul> Order Events Producer (async integration demo) - click to expand <pre><code># Producer for async integration demo - generates order events\n\nfunctions:\n  generate_orders:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      order_counter = 0\n      customer_ids = [\"CUST001\", \"CUST002\", \"CUST003\", \"CUST004\"]\n      statuses = [\"created\", \"paid\", \"shipped\", \"delivered\"]\n    code: |\n      global order_counter, customer_ids, statuses\n\n      order_counter += 1\n      order_id = f\"ORDER_{order_counter:04d}\"\n      customer_id = random.choice(customer_ids)\n      status = random.choice(statuses)\n      amount = round(random.uniform(10.0, 500.0), 2)\n      current_timestamp = int(time.time() * 1000)\n\n      # Create structured JSON order event for better readability in Kowl UI\n      order_event = {\n        \"order_id\": order_id,\n        \"customer_id\": customer_id,\n        \"status\": status,\n        \"amount\": amount,\n        \"timestamp\": current_timestamp,\n        \"sequence_number\": order_counter,\n        \"order_details\": {\n          \"currency\": \"USD\",\n          \"payment_method\": random.choice([\"credit_card\", \"debit_card\", \"paypal\", \"bank_transfer\"]),\n          \"shipping_method\": random.choice([\"standard\", \"express\", \"overnight\", \"pickup\"]),\n          \"order_source\": random.choice([\"web\", \"mobile\", \"phone\", \"store\"])\n        },\n        \"customer_info\": {\n          \"customer_tier\": random.choice([\"bronze\", \"silver\", \"gold\", \"platinum\"]),\n          \"loyalty_points\": random.randint(0, 5000),\n          \"previous_orders\": random.randint(0, 50)\n        },\n        \"fulfillment\": {\n          \"warehouse_id\": f\"WH_{random.randint(1, 5):02d}\",\n          \"estimated_ship_date\": current_timestamp + random.randint(1, 7) * 24 * 60 * 60 * 1000,  # 1-7 days\n          \"shipping_address\": {\n            \"country\": random.choice([\"US\", \"CA\", \"UK\", \"DE\", \"FR\"]),\n            \"region\": random.choice([\"North\", \"South\", \"East\", \"West\", \"Central\"])\n          }\n        },\n        \"business_context\": {\n          \"requires_external_processing\": status == \"paid\",\n          \"high_value\": amount &gt; 200,\n          \"priority\": \"high\" if amount &gt; 300 else \"normal\",\n          \"async_processing_needed\": True\n        },\n        \"metadata\": {\n          \"simulation\": True,\n          \"async_integration\": True,\n          \"correlation_id\": f\"corr_{order_id}_{current_timestamp}\",\n          \"processing_version\": \"1.0\"\n        }\n      }\n\n    expression: (order_id, order_event)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_orders\n    interval: 3s\n    to:\n      topic: order_events\n      keyType: string\n      valueType: json\n</code></pre> Async Integration Processor (request-response pattern) - click to expand <pre><code># Processor demonstrating async integration pattern\n\nstreams:\n  order_events:\n    topic: order_events\n    keyType: string\n    valueType: json\n  external_requests:\n    topic: external_requests\n    keyType: string\n    valueType: json\n\nfunctions:\n  filter_paid_orders:\n    type: predicate\n    code: |\n      # Extract status from JSON order event\n      if not value:\n        return False\n      status = value.get(\"status\")\n      # Only process 'paid' orders\n      return status == \"paid\"\n    expression: result\n\n  create_external_request:\n    type: keyValueTransformer\n    code: |\n      import time\n\n      # Extract fields from JSON order event\n      if not value:\n        return None\n\n      order_id = value.get(\"order_id\")\n      customer_id = value.get(\"customer_id\")\n      status = value.get(\"status\")\n      amount = value.get(\"amount\")\n      timestamp = value.get(\"timestamp\")\n      sequence_number = value.get(\"sequence_number\")\n      order_details = value.get(\"order_details\", {})\n      customer_info = value.get(\"customer_info\", {})\n      fulfillment = value.get(\"fulfillment\", {})\n      business_context = value.get(\"business_context\", {})\n      metadata = value.get(\"metadata\", {})\n\n      # Create comprehensive request for external payment processing system\n      request_id = f\"REQ_{order_id}_{timestamp}\"\n\n      external_request = {\n        \"request_id\": request_id,\n        \"request_type\": \"PAYMENT_PROCESSING\",\n        \"original_order\": {\n          \"order_id\": order_id,\n          \"customer_id\": customer_id,\n          \"amount\": amount,\n          \"timestamp\": timestamp,\n          \"sequence_number\": sequence_number\n        },\n        \"payment_details\": {\n          \"amount\": amount,\n          \"currency\": order_details.get(\"currency\", \"USD\"),\n          \"payment_method\": order_details.get(\"payment_method\"),\n          \"customer_tier\": customer_info.get(\"customer_tier\"),\n          \"loyalty_points\": customer_info.get(\"loyalty_points\")\n        },\n        \"processing_context\": {\n          \"priority\": business_context.get(\"priority\", \"normal\"),\n          \"high_value\": business_context.get(\"high_value\", False),\n          \"customer_previous_orders\": customer_info.get(\"previous_orders\", 0),\n          \"order_source\": order_details.get(\"order_source\")\n        },\n        \"async_metadata\": {\n          \"correlation_id\": metadata.get(\"correlation_id\", f\"corr_{request_id}\"),\n          \"created_at\": int(time.time() * 1000),\n          \"timeout_ms\": 30000,  # 30 second timeout\n          \"retry_count\": 0,\n          \"expected_response_topic\": \"external_responses\"\n        },\n        \"external_system_info\": {\n          \"target_system\": \"payment_processor\",\n          \"api_version\": \"v2.1\",\n          \"request_format\": \"async_json\",\n          \"callback_required\": True\n        }\n      }\n\n      log.info(\"Created external payment request for order {}: amount=${:.2f}, priority={}\", \n               order_id, amount, business_context.get(\"priority\", \"normal\"))\n\n      return (request_id, external_request)\n\n    expression: result\n    resultType: (string, json)\n\n  process_external_response:\n    type: valueTransformer \n    globalCode: |\n      import time\n      import random\n    code: |\n      # Simulate processing external system response\n      # In real scenario, this would come from external system response topic\n\n      if not value:\n        return None\n\n      # Extract request information\n      request_id = value.get(\"request_id\")\n      request_type = value.get(\"request_type\")\n      original_order = value.get(\"original_order\", {})\n      payment_details = value.get(\"payment_details\", {})\n      processing_context = value.get(\"processing_context\", {})\n      async_metadata = value.get(\"async_metadata\", {})\n\n      # Simulate external system processing with realistic delays and outcomes\n      processing_time_ms = random.randint(100, 2000)  # 0.1 to 2 seconds\n      success_rate = 0.9 if processing_context.get(\"high_value\") else 0.95\n      is_successful = random.random() &lt; success_rate\n\n      current_timestamp = int(time.time() * 1000)\n\n      # Create comprehensive response\n      response_data = {\n        \"response_id\": f\"RESP_{request_id}_{current_timestamp}\",\n        \"request_id\": request_id,\n        \"response_type\": \"PAYMENT_PROCESSING_RESULT\",\n        \"status\": \"success\" if is_successful else \"failed\",\n        \"original_request\": {\n          \"order_id\": original_order.get(\"order_id\"),\n          \"customer_id\": original_order.get(\"customer_id\"),\n          \"amount\": payment_details.get(\"amount\"),\n          \"request_timestamp\": original_order.get(\"timestamp\")\n        },\n        \"processing_result\": {\n          \"transaction_id\": f\"TXN_{current_timestamp}_{random.randint(1000, 9999)}\" if is_successful else None,\n          \"authorization_code\": f\"AUTH_{random.randint(100000, 999999)}\" if is_successful else None,\n          \"processing_status\": \"approved\" if is_successful else \"declined\",\n          \"reason_code\": \"000\" if is_successful else random.choice([\"051\", \"061\", \"065\", \"075\"]),\n          \"reason_message\": \"Transaction approved\" if is_successful else random.choice([\n            \"Insufficient funds\", \"Invalid card\", \"Expired card\", \"Fraud suspected\"\n          ])\n        },\n        \"financial_details\": {\n          \"processed_amount\": payment_details.get(\"amount\") if is_successful else 0.0,\n          \"currency\": payment_details.get(\"currency\", \"USD\"),\n          \"fee_amount\": round(payment_details.get(\"amount\", 0) * 0.029, 2) if is_successful else 0.0,  # 2.9% fee\n          \"settlement_date\": current_timestamp + 86400000 if is_successful else None  # Next day\n        },\n        \"system_metadata\": {\n          \"external_system\": \"payment_processor_v2.1\",\n          \"processing_time_ms\": processing_time_ms,\n          \"processed_at\": current_timestamp,\n          \"correlation_id\": async_metadata.get(\"correlation_id\"),\n          \"retry_attempt\": async_metadata.get(\"retry_count\", 0) + 1,\n          \"final_response\": True\n        },\n        \"business_context\": {\n          \"customer_impact\": \"order_confirmed\" if is_successful else \"order_cancelled\",\n          \"requires_notification\": True,\n          \"follow_up_actions\": [\"send_confirmation_email\", \"update_inventory\"] if is_successful else [\"send_decline_email\", \"release_inventory\"],\n          \"priority_level\": processing_context.get(\"priority\", \"normal\")\n        }\n      }\n\n      log.info(\"Processed external system response for order {}: status={}, amount=${:.2f}\", \n               original_order.get(\"order_id\"), response_data[\"status\"], payment_details.get(\"amount\", 0))\n\n      return response_data\n\n    expression: result\n    resultType: json\n\npipelines:\n  # Send requests to external system\n  external_request_pipeline:\n    from: order_events\n    via:\n      - type: filter\n        if: filter_paid_orders\n      - type: transformKeyValue\n        mapper: create_external_request\n    to:\n      topic: external_requests\n      keyType: string\n      valueType: json\n\n  # Process responses from external system (mock processing)\n  external_response_pipeline:\n    from: external_requests\n    via:\n      - type: mapValues\n        mapper: process_external_response\n    to:\n      topic: external_responses\n      keyType: string\n      valueType: json\n</code></pre> <p>Async integration features:</p> <ul> <li>Loose coupling: External systems communicate via topics, not direct calls</li> <li>Scalability: Multiple consumers can process requests and responses</li> <li>Reliability: Messages are persisted in Kafka topics</li> <li>Monitoring: Easy to monitor request/response flows through topic metrics</li> </ul>"},{"location":"tutorials/advanced/external-integration/#key-metrics-to-track","title":"Key Metrics to Track","text":"<p>Monitor these metrics to ensure integration health:</p> <ul> <li>Track latency of external API calls and database queries</li> <li>Monitor the percentage of successful external system interactions</li> <li>Track different types of errors (timeouts, authentication, etc.)</li> <li>For lookup patterns, monitor state store cache hit rates</li> <li>For async patterns, monitor request and response topic lag</li> </ul>"},{"location":"tutorials/advanced/external-integration/#alert-configuration","title":"Alert Configuration","text":"<p>Set up alerts for integration issues:</p> <pre><code>alerts:\n  high_api_latency:\n    condition: avg_api_response_time &gt; 5000ms\n    duration: 2_minutes\n    severity: warning\n\n  external_system_down:\n    condition: api_error_rate &gt; 50%\n    duration: 1_minute\n    severity: critical\n\n  cache_miss_rate_high:\n    condition: cache_miss_rate &gt; 20%\n    duration: 5_minutes\n    severity: warning\n</code></pre>"},{"location":"tutorials/advanced/external-integration/#advanced-integration-patterns","title":"Advanced Integration Patterns","text":""},{"location":"tutorials/advanced/external-integration/#hybrid-event-database-pattern","title":"Hybrid Event-Database Pattern","text":"<p>Combine streaming events with database lookups for complex business logic:</p> <pre><code>functions:\n  hybrid_processor:\n    type: valueTransformer\n    code: |\n      # Process streaming event\n      event_data = parse_event(value)\n\n      # Look up reference data from cached database\n      reference_data = get_reference_data(event_data.entity_id)\n\n      # Apply business rules using both streaming and reference data\n      result = apply_business_rules(event_data, reference_data)\n\n      # Optionally trigger external API call based on result\n      if result.requires_notification:\n        send_notification(result)\n\n      return result\n</code></pre>"},{"location":"tutorials/advanced/external-integration/#multi-system-coordination","title":"Multi-System Coordination","text":"<p>Coordinate operations across multiple external systems:</p> <pre><code>functions:\n  multi_system_coordinator:\n    type: keyValueTransformer\n    code: |\n      # Create requests for multiple external systems\n      payment_request = create_payment_request(value)\n      inventory_request = create_inventory_request(value)\n\n      # Use correlation ID to track related requests\n      correlation_id = generate_correlation_id()\n\n      # Return multiple outputs for different external systems\n      return [\n        (f\"payment_{correlation_id}\", payment_request),\n        (f\"inventory_{correlation_id}\", inventory_request)\n      ]\n</code></pre>"},{"location":"tutorials/advanced/external-integration/#conclusion","title":"Conclusion","text":"<p>External integration is vital for stream processing. Using API enrichment, database lookups, and async integration, you can build scalable KSML applications that enhance streaming data without sacrificing performance.</p> <p>Choose the right pattern: API enrichment for real-time data, database lookups for reference data, and async integration for multi-system workflows.</p>"},{"location":"tutorials/advanced/performance-optimization/","title":"Performance Optimization in KSML","text":"<p>This tutorial covers optimization techniques for KSML applications to improve throughput, reduce latency, and minimize resource usage.</p>"},{"location":"tutorials/advanced/performance-optimization/#introduction","title":"Introduction","text":"<p>Performance optimization helps you process more data efficiently while reducing costs and maintaining low latency under high loads.</p>"},{"location":"tutorials/advanced/performance-optimization/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic mixed_quality_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic filtered_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic enriched_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic final_processed_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic binary_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic optimized_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic readable_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_activity &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_metrics_summary &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic high_volume_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic processed_events &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/advanced/performance-optimization/#core-performance-concepts","title":"Core Performance Concepts","text":""},{"location":"tutorials/advanced/performance-optimization/#identifying-bottlenecks","title":"Identifying Bottlenecks","text":"<p>Before optimizing, identify where performance bottlenecks exist:</p> <ul> <li>Processing time: How long it takes to process each message</li> <li>Throughput: Messages processed per second</li> <li>State store size: How much data is stored in state stores</li> <li>Memory usage: JVM heap and non-heap memory</li> <li>GC activity: Frequency and duration of garbage collection</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#key-optimization-areas","title":"Key Optimization Areas","text":"<ol> <li>Python Function Efficiency: Optimize code for minimal object creation and fast execution</li> <li>State Store Configuration: Configure stores for your specific workload</li> <li>Pipeline Design: Structure pipelines to filter early and process efficiently</li> <li>Data Serialization: Choose efficient formats and minimize serialization overhead</li> </ol>"},{"location":"tutorials/advanced/performance-optimization/#efficient-processing-patterns","title":"Efficient Processing Patterns","text":"<p>This example demonstrates optimized Python code and efficient data handling techniques.</p> <p>What it does:</p> <ul> <li>Produces high-volume events: Creates user events (click, purchase, view) with categories, values, but also includes low-value events (scroll, hover)</li> <li>Filters early: Immediately discards uninteresting events (scroll, hover) to avoid processing overhead downstream</li> <li>Pre-computes efficiently: Uses global dictionaries for priority events and category multipliers, calculates scores with minimal object creation</li> <li>Processes with JSON: Extracts fields from JSON using <code>.get()</code>, builds result objects for Kowl UI readability</li> <li>Logs selectively: Only logs high-score events to reduce I/O overhead while maintaining performance visibility</li> </ul> High-volume producer (compact format) - click to expand <pre><code># Producer for performance optimization demo - generates high volume events\n\nfunctions:\n  generate_high_volume_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      user_ids = [f\"user_{i:04d}\" for i in range(1, 101)]  # 100 users\n      event_types = [\"view\", \"click\", \"purchase\", \"search\", \"scroll\", \"hover\"]\n      product_categories = [\"electronics\", \"clothing\", \"books\", \"home\", \"sports\"]\n    code: |\n      global event_counter, user_ids, event_types, product_categories\n\n      event_counter += 1\n      user_id = random.choice(user_ids)\n      event_type = random.choice(event_types)\n\n      # Generate realistic event data for performance testing\n      event_data = {\n        \"event_id\": f\"evt_{event_counter:08d}\",\n        \"type\": event_type,\n        \"user_id\": user_id,\n        \"timestamp\": int(time.time() * 1000),\n        \"product_id\": f\"prod_{random.randint(1, 1000):04d}\",\n        \"category\": random.choice(product_categories),\n        \"value\": round(random.uniform(1.0, 100.0), 2) if event_type == \"purchase\" else 0,\n        \"session_id\": f\"sess_{hash(user_id) % 50:03d}\",\n        \"metadata\": {\n          \"source\": \"web\",\n          \"device\": random.choice([\"desktop\", \"mobile\", \"tablet\"]),\n          \"browser\": random.choice([\"chrome\", \"firefox\", \"safari\"])\n        }\n      }\n\n    expression: (user_id, event_data)\n    resultType: (string, json)\n\nproducers:\n  high_volume_producer:\n    generator: generate_high_volume_events\n    interval: 1s  # High frequency for performance testing\n    to:\n      topic: high_volume_events\n      keyType: string\n      valueType: json\n</code></pre> Efficient processor (optimized Python) - click to expand <pre><code># Processor demonstrating efficient processing patterns\n\nstreams:\n  high_volume_events:\n    topic: high_volume_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  efficient_event_processor:\n    type: valueTransformer\n    globalCode: |\n      # Pre-compute expensive operations outside the processing loop\n      PRIORITY_EVENTS = {\"purchase\", \"search\"}\n      MULTIPLIERS = {\"electronics\": 1.5, \"clothing\": 1.2, \"books\": 1.0, \"home\": 1.3, \"sports\": 1.1}\n\n      # Use efficient data structures and avoid object creation\n      def calculate_score(event_type, category, value):\n        \"\"\"Efficient score calculation with minimal object creation\"\"\"\n        base_score = 10 if event_type in PRIORITY_EVENTS else 5\n        category_multiplier = MULTIPLIERS.get(category, 1.0)\n        value_component = min(value * 0.1, 10)  # Cap value component\n        return round(base_score * category_multiplier + value_component, 2)\n\n    code: |\n      # Early filtering - discard uninteresting events immediately\n      event_type = value.get(\"type\")\n      if event_type in [\"scroll\", \"hover\"]:\n        return None\n\n      # Extract needed fields from JSON\n      user_id = value.get(\"user_id\")\n      category = value.get(\"category\")\n      value_amount = value.get(\"value\", 0)\n      timestamp = value.get(\"timestamp\")\n      product_id = value.get(\"product_id\")\n\n      # Efficient processing with pre-computed values\n      score = calculate_score(event_type, category, value_amount)\n\n      # Build result as JSON for better readability in Kowl UI\n      result = {\n        \"status\": \"PROCESSED\",\n        \"event_type\": event_type,\n        \"user_id\": user_id,\n        \"product_id\": product_id,\n        \"category\": category,\n        \"score\": score,\n        \"value\": round(value_amount, 2),\n        \"timestamp\": timestamp,\n        \"metadata\": value.get(\"metadata\")\n      }\n\n      # Log only important events to reduce I/O\n      if event_type in PRIORITY_EVENTS:\n        log.info(\"High-priority event processed: {} for user {} with score {:.2f}\", event_type, user_id, score)\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  efficient_processing_pipeline:\n    from: high_volume_events\n    via:\n      - type: mapValues\n        mapper: efficient_event_processor\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: processed_events\n      keyType: string\n      valueType: json\n</code></pre> <p>Key optimization techniques:</p> <ul> <li>Global code optimization: Pre-compute expensive operations outside processing loops</li> <li>Early filtering: Discard unwanted events immediately to reduce downstream processing</li> <li>Efficient logging: Log only important events to reduce I/O overhead</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#state-store-optimization","title":"State Store Optimization","text":"<p>This example shows how to configure and use state stores efficiently for high-performance scenarios.</p> <p>What it does:</p> <ul> <li>Produces activity events: Creates user activities (login, page_view, click, purchase, logout) with scores, durations, timestamps in JSON format</li> <li>Stores metrics compactly: Converts JSON to compact string format \"total_count:login_count:page_view_count:click_count:purchase_count:logout_count:total_score\"</li> <li>Updates efficiently: Parses compact string to array, updates counters by index mapping, avoids object creation during updates</li> <li>Uses optimized store: Configures state store with increased cache size (16MB) and segments (32) for better performance</li> <li>Outputs JSON summaries: Returns readable JSON results with averages and totals while keeping internal storage compact</li> </ul> User metrics producer (activity data) - click to expand <pre><code># Producer for state store optimization demo - generates user activity metrics\n\nfunctions:\n  generate_user_activity:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      activity_counter = 0\n      user_ids = [f\"user_{i:03d}\" for i in range(1, 21)]  # 20 users for state demo\n      activities = [\"login\", \"page_view\", \"click\", \"purchase\", \"logout\"]\n    code: |\n      global activity_counter, user_ids, activities\n\n      activity_counter += 1\n      user_id = random.choice(user_ids)\n      activity = random.choice(activities)\n\n      # Create activity with metrics that need aggregation\n      activity_data = {\n        \"activity_type\": activity,\n        \"score\": random.randint(1, 100),\n        \"timestamp\": int(time.time() * 1000),\n        \"session_id\": f\"sess_{hash(user_id) % 10:02d}\",\n        \"duration_ms\": random.randint(100, 5000)\n      }\n\n    expression: (user_id, activity_data)\n    resultType: (string, json)\n\nproducers:\n  user_activity_producer:\n    generator: generate_user_activity\n    interval: 1s\n    to:\n      topic: user_activity\n      keyType: string\n      valueType: json\n</code></pre> Optimized state processor (compact storage) - click to expand <pre><code># Processor demonstrating optimized state store configuration and usage\n\nstreams:\n  user_activity:\n    topic: user_activity\n    keyType: string\n    valueType: json\n\nstores:\n  # Optimized state store configuration\n  user_metrics_store:\n    type: keyValue\n    keyType: string\n    valueType: string  # Using compact string format for state storage performance\n    persistent: true\n    caching: true\n\nfunctions:\n  update_user_metrics:\n    type: valueTransformer\n    globalCode: |\n      # Use compact string format for state storage to reduce serialization overhead\n      def parse_metrics(metrics_str):\n        \"\"\"Parse compact metrics format: count:login:page_view:click:purchase:logout:total_score\"\"\"\n        if not metrics_str:\n          return [0, 0, 0, 0, 0, 0, 0]  # Default metrics\n        parts = metrics_str.split(\":\")\n        return [int(x) for x in parts] if len(parts) == 7 else [0, 0, 0, 0, 0, 0, 0]\n\n      def format_metrics(metrics_list):\n        \"\"\"Format metrics into compact string\"\"\"\n        return \":\".join(str(x) for x in metrics_list)\n\n    code: |\n      # Extract activity data from JSON\n      activity_type = value.get(\"activity_type\")\n      if not activity_type:\n        return None\n\n      score = value.get(\"score\", 0)\n      timestamp = value.get(\"timestamp\", 0)\n      duration_ms = value.get(\"duration_ms\", 0)\n\n      # Get current metrics from optimized state store\n      current_metrics_str = user_metrics_store.get(key)\n      metrics = parse_metrics(current_metrics_str)\n\n      # Update metrics efficiently - avoid object creation\n      metrics[0] += 1  # total_count\n\n      # Update activity-specific counters using index mapping\n      activity_indices = {\n        \"login\": 1, \"page_view\": 2, \"click\": 3, \"purchase\": 4, \"logout\": 5\n      }\n\n      if activity_type in activity_indices:\n        metrics[activity_indices[activity_type]] += 1\n\n      metrics[6] += score  # total_score\n\n      # Store back in compact format\n      user_metrics_store.put(key, format_metrics(metrics))\n\n      # Calculate derived metrics efficiently\n      avg_score = metrics[6] / metrics[0] if metrics[0] &gt; 0 else 0\n\n      # Return summary as JSON for better readability in Kowl UI\n      result = {\n        \"user_id\": key,\n        \"metrics\": {\n          \"total_count\": metrics[0],\n          \"login_count\": metrics[1],\n          \"page_view_count\": metrics[2],\n          \"click_count\": metrics[3],\n          \"purchase_count\": metrics[4],\n          \"logout_count\": metrics[5],\n          \"total_score\": metrics[6],\n          \"avg_score\": round(avg_score, 2)\n        },\n        \"last_activity\": {\n          \"type\": activity_type,\n          \"score\": score,\n          \"duration_ms\": duration_ms,\n          \"timestamp\": timestamp\n        }\n      }\n\n      # Log only significant changes to reduce I/O\n      if metrics[0] % 10 == 0:  # Log every 10th event\n        log.info(\"User {} metrics: {} total activities, avg score: {:.2f}\", key, metrics[0], avg_score)\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n    stores:\n      - user_metrics_store\n\npipelines:\n  optimized_state_pipeline:\n    from: user_activity\n    via:\n      - type: mapValues\n        mapper: update_user_metrics\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: user_metrics_summary\n      keyType: string\n      valueType: json\n</code></pre> <p>State store optimizations:</p> <ul> <li>JSON input/output: Uses JSON for better debugging while maintaining compact string storage internally</li> <li>Hybrid approach: JSON for input/output messages, compact strings for state storage efficiency</li> <li>Increased cache: Configure larger cache sizes (50MB) for better performance  </li> <li>Direct field access: Extract JSON fields using <code>value.get()</code> instead of string parsing</li> <li>Conditional logging: Log only significant changes to reduce I/O</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#pipeline-optimization","title":"Pipeline Optimization","text":"<p>This example demonstrates optimized pipeline design with early filtering and staged processing.</p> <p>What it does:</p> <ul> <li>Produces mixed quality events: Creates events with valid/invalid data, various priorities (high, low, spam), different event types for filtering tests</li> <li>Filters early: Uses predicate function to immediately discard invalid events, spam, and bot traffic before expensive processing</li> <li>Processes in stages: Stage 1 = lightweight enrichment (add status, extract fields), Stage 2 = heavy processing (complex calculations)</li> <li>Separates concerns: Lightweight operations (field extraction) happen first, expensive operations (calculations) happen on filtered data only</li> <li>Outputs progressively: filtered_events \u2192 enriched_events \u2192 final_processed_events, each stage adds more data while maintaining JSON readability</li> </ul> Mixed events producer (quality testing) - click to expand <pre><code># Producer for pipeline optimization demo - generates mixed quality events\n\nfunctions:\n  generate_mixed_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      event_counter = 0\n      event_types = [\"valid_purchase\", \"valid_view\", \"spam\", \"invalid\", \"test\", \"bot_traffic\"]\n      priorities = [\"high\", \"medium\", \"low\", \"spam\"]\n    code: |\n      global event_counter, event_types, priorities\n\n      event_counter += 1\n      event_type = random.choice(event_types)\n      priority = random.choice(priorities)\n\n      # Mix of valid and invalid events to demonstrate filtering optimization\n      is_valid = event_type.startswith(\"valid\") and priority != \"spam\"\n\n      # Create event data with quality indicators as JSON\n      event_data = {\n        \"event_type\": event_type,\n        \"priority\": priority,\n        \"is_valid\": is_valid,\n        \"score\": random.randint(1, 1000),\n        \"timestamp\": int(time.time() * 1000),\n        \"batch_id\": event_counter // 100,  # Group events in batches\n        \"source\": \"pipeline_test\"\n      }\n\n    expression: (f\"event_{event_counter:06d}\", event_data)\n    resultType: (string, json)\n\nproducers:\n  mixed_events_producer:\n    generator: generate_mixed_events\n    interval: 1s  # High frequency to test filtering efficiency\n    to:\n      topic: mixed_quality_events\n      keyType: string\n      valueType: json\n</code></pre> Pipeline optimization processor (staged processing) - click to expand <pre><code># Processor demonstrating optimized pipeline design with early filtering\n\nstreams:\n  mixed_quality_events:\n    topic: mixed_quality_events\n    keyType: string\n    valueType: json\n\n  filtered_events:\n    topic: filtered_events\n    keyType: string\n    valueType: json\n\n  enriched_events:\n    topic: enriched_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  early_filter:\n    type: predicate\n    code: |\n      # Early filtering - extract minimal data and filter out unwanted events immediately\n      event_type = value.get(\"event_type\", \"\")\n      priority = value.get(\"priority\", \"\")\n      is_valid = value.get(\"is_valid\", False)\n\n      # Filter out spam, invalid, and bot traffic early to reduce downstream processing\n      return is_valid and event_type.startswith(\"valid\") and priority != \"spam\"\n\n    expression: result\n    resultType: boolean\n\n  lightweight_enricher:\n    type: valueTransformer\n    code: |\n      # Lightweight enrichment - add only essential data\n      event_type = value.get(\"event_type\", \"\")\n      priority = value.get(\"priority\", \"\")\n      score = value.get(\"score\", 0)\n      timestamp = value.get(\"timestamp\", 0)\n\n      # Add minimal enrichment data as JSON for better readability\n      enriched = {\n        \"status\": \"ENRICHED\",\n        \"event_type\": event_type,\n        \"priority\": priority,\n        \"score\": score,\n        \"timestamp\": timestamp,\n        \"batch_id\": value.get(\"batch_id\", 0),\n        \"source\": value.get(\"source\", \"unknown\")\n      }\n\n      return enriched\n\n    expression: result if result else None\n    resultType: json\n\n  heavy_processor:\n    type: valueTransformer\n    globalCode: |\n      # Expensive operations that should only run on filtered data\n      def calculate_complex_score(event_type, priority, base_score):\n        \"\"\"Simulate expensive calculation\"\"\"\n        multipliers = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}\n        type_bonus = 50 if \"purchase\" in event_type else 10\n\n        # Simulate complex computation\n        complex_score = base_score * multipliers.get(priority, 1.0) + type_bonus\n\n        # Additional expensive operations\n        for i in range(100):  # Simulate computational overhead\n          complex_score += (i % 5) * 0.1\n\n        return round(complex_score, 2)\n\n    code: |\n      # Heavy processing - only runs on pre-filtered valid events\n      event_type = value.get(\"event_type\", \"\")\n      priority = value.get(\"priority\", \"\")\n      base_score = value.get(\"score\", 0)\n      timestamp = value.get(\"timestamp\", 0)\n\n      # Expensive calculation only on filtered data\n      complex_score = calculate_complex_score(event_type, priority, base_score)\n\n      # Generate comprehensive result as JSON for better readability in Kowl UI\n      result = {\n        \"status\": \"PROCESSED\",\n        \"event_type\": event_type,\n        \"priority\": priority,\n        \"scores\": {\n          \"base_score\": base_score,\n          \"complex_score\": complex_score\n        },\n        \"timestamp\": timestamp,\n        \"batch_id\": value.get(\"batch_id\", 0),\n        \"source\": value.get(\"source\", \"unknown\"),\n        \"processing_stage\": \"final\"\n      }\n\n      log.info(\"Completed heavy processing for {}: score {:.2f}\", event_type, complex_score)\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  # Stage 1: Early filtering to reduce data volume\n  filter_stage:\n    from: mixed_quality_events\n    via:\n      - type: filter\n        if: early_filter  # Filter out 70-80% of events early\n    to:\n      topic: filtered_events\n      keyType: string\n      valueType: json\n\n  # Stage 2: Lightweight enrichment on filtered data\n  enrich_stage:\n    from: filtered_events\n    via:\n      - type: mapValues\n        mapper: lightweight_enricher\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: enriched_events\n      keyType: string\n      valueType: json\n\n  # Stage 3: Heavy processing only on high-quality filtered data\n  process_stage:\n    from: enriched_events\n    via:\n      - type: mapValues\n        mapper: heavy_processor\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: final_processed_events\n      keyType: string\n      valueType: json\n</code></pre> <p>Pipeline design principles:</p> <ul> <li>Filter early: Remove unwanted data before expensive processing using JSON field checks</li> <li>Staged processing: Separate lightweight and heavy operations into different stages</li> <li>Efficient predicates: Use <code>value.get()</code> for fast field-based filtering</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#serialization-optimization","title":"Serialization Optimization","text":"<p>This example shows efficient data format usage and minimal serialization overhead.</p> <p>What it does:</p> <ul> <li>Produces compact data: Creates events with numeric event_type_ids (1=view, 2=click, 3=purchase) instead of strings for efficiency</li> <li>Uses lookup tables: Pre-computes event type mappings in globalCode for fast ID-to-name conversions without string operations</li> <li>Filters by score: Early filtering discards events with score &lt;10 to reduce processing volume</li> <li>Processes by type: Applies different logic based on event_type_id (purchases get 10% value bonus + 20 score bonus)</li> </ul> Binary data producer (compact format) - click to expand <pre><code># Producer for serialization optimization demo - generates binary data for performance\n\nfunctions:\n  generate_binary_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      import struct\n      event_counter = 0\n      user_ids = list(range(1, 101))  # Numeric user IDs for binary encoding\n    code: |\n      global event_counter, user_ids\n\n      event_counter += 1\n      user_id = random.choice(user_ids)\n\n      # Generate data for binary serialization comparison\n      timestamp = int(time.time() * 1000)\n      event_type_id = random.randint(1, 5)  # Encoded event types\n      value = random.randint(1, 10000)\n      score = random.randint(1, 100)\n\n      # Create structured data in JSON format (better than compact strings for readability)\n      # While JSON has more overhead than binary, it provides better debugging/monitoring\n      event_data = {\n        \"user_id\": user_id,\n        \"timestamp\": timestamp,\n        \"event_type_id\": event_type_id,\n        \"value\": value,\n        \"score\": score,\n        \"metadata\": {\n          \"batch\": event_counter // 50,\n          \"version\": 1\n        }\n      }\n\n    expression: (str(user_id), event_data)\n    resultType: (string, json)\n\nproducers:\n  binary_data_producer:\n    generator: generate_binary_events\n    interval: 1s  # High frequency for performance testing\n    to:\n      topic: binary_events\n      keyType: string\n      valueType: json\n</code></pre> Serialization optimization processor (efficient parsing) - click to expand <pre><code># Processor demonstrating serialization optimization patterns\n\nstreams:\n  binary_events:\n    topic: binary_events\n    keyType: string\n    valueType: json\n\n  optimized_events:\n    topic: optimized_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  efficient_binary_processor:\n    type: valueTransformer\n    globalCode: |\n      # Lookup tables for efficient conversion (avoid string operations)\n      EVENT_TYPES = {1: \"view\", 2: \"click\", 3: \"purchase\", 4: \"search\", 5: \"logout\"}\n\n      def create_optimized_output(data, event_type):\n        \"\"\"Create optimized JSON output with structured data\"\"\"\n        return {\n          \"processed_event\": {\n            \"type\": event_type,\n            \"user_id\": data[\"user_id\"],\n            \"value\": data[\"value\"],\n            \"score\": data[\"score\"],\n            \"timestamp\": data[\"timestamp\"]\n          },\n          \"metadata\": data.get(\"metadata\", {}),\n          \"processing_info\": {\n            \"optimized\": True,\n            \"version\": \"v2\"\n          }\n        }\n\n    code: |\n      # Extract data from JSON (more readable than binary parsing)\n      user_id = value.get(\"user_id\")\n      timestamp = value.get(\"timestamp\")\n      event_type_id = value.get(\"event_type_id\")\n      current_value = value.get(\"value\", 0)\n      score = value.get(\"score\", 0)\n\n      # Early filtering based on score\n      if score &lt; 10:  # Filter low-quality events\n        return None\n\n      # Efficient processing with direct field access\n      if event_type_id == 3:  # Purchase events\n        # Apply purchase-specific logic\n        processed_value = int(current_value * 1.1)  # 10% bonus\n        final_score = min(score + 20, 100)  # Bonus capped at 100\n      else:\n        processed_value = current_value\n        final_score = score\n\n      # Get event type name for output\n      event_type = EVENT_TYPES.get(event_type_id, \"unknown\")\n\n      # Create optimized JSON output\n      optimized_data = {\n        \"user_id\": user_id,\n        \"timestamp\": timestamp,\n        \"event_type_id\": event_type_id,\n        \"value\": processed_value,\n        \"score\": final_score\n      }\n\n      result = create_optimized_output(optimized_data, event_type)\n\n      return result\n\n    expression: result if result else None\n    resultType: json\n\n  convert_to_readable:\n    type: valueTransformer\n    code: |\n      # Convert optimized JSON to human-readable final format\n      processed_event = value.get(\"processed_event\", {})\n      if not processed_event:\n        return None\n\n      event_type = processed_event.get(\"type\", \"unknown\")\n      user_id = processed_event.get(\"user_id\")\n      event_value = processed_event.get(\"value\")\n      score = processed_event.get(\"score\")\n      timestamp = processed_event.get(\"timestamp\")\n\n      # Create final readable JSON output for Kowl UI\n      readable = {\n        \"summary\": f\"EVENT:{event_type} USER:{user_id} VALUE:{event_value} SCORE:{score}\",\n        \"details\": {\n          \"event_type\": event_type,\n          \"user_id\": user_id,\n          \"value\": event_value,\n          \"score\": score,\n          \"timestamp\": timestamp\n        },\n        \"metadata\": value.get(\"metadata\", {}),\n        \"processing_info\": value.get(\"processing_info\", {})\n      }\n\n      return readable\n\n    expression: result if result else None\n    resultType: json\n\npipelines:\n  # High-performance processing pipeline\n  binary_processing_pipeline:\n    from: binary_events\n    via:\n      - type: mapValues\n        mapper: efficient_binary_processor\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: optimized_events\n      keyType: string\n      valueType: json\n\n  # Convert to readable format for final output\n  readable_output_pipeline:\n    from: optimized_events\n    via:\n      - type: mapValues\n        mapper: convert_to_readable\n      - type: filter\n        if:\n          expression: value is not None\n    to:\n      topic: readable_events\n      keyType: string\n      valueType: json\n</code></pre> <p>Serialization best practices:</p> <ul> <li>Lookup tables: Pre-compute mappings to avoid repeated string operations</li> <li>Progressive transformation: Transform data from raw \u2192 optimized \u2192 final readable formats</li> <li>Field-based access: Use <code>value.get()</code> instead of string parsing for cleaner, faster code</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#configuration-optimization","title":"Configuration Optimization","text":""},{"location":"tutorials/advanced/performance-optimization/#kafka-streams-configuration","title":"Kafka Streams Configuration","text":"<p>Optimize Kafka Streams configuration for your workload based on KSML's internal configuration system:</p> <pre><code>runner:\n  type: streams\n  config:\n    application.id: optimized-ksml-app\n    bootstrap.servers: kafka:9092\n\n    # Threading Configuration\n    num.stream.threads: 8                    # Parallel processing threads (default: 1)\n\n    # State Store Caching\n    cache.max.bytes.buffering: 104857600     # 100MB total cache (default: 10MB)\n    commit.interval.ms: 30000                # Commit frequency in ms (default: 30000)\n\n    # Topology Optimization\n    topology.optimization: all               # Enable all optimizations (default: all)\n\n    # Producer Performance Settings\n    producer.linger.ms: 100                  # Wait for batching (default: 0)\n    producer.batch.size: 16384               # Batch size in bytes (default: 16384)\n    producer.buffer.memory: 33554432         # 32MB producer buffer (default: 32MB)\n    producer.acks: 1                         # Acknowledgment level (1 = leader only)\n\n    # Consumer Performance Settings\n    consumer.fetch.max.bytes: 52428800       # 50MB max fetch (default: 52428800)\n    consumer.max.poll.records: 500           # Records per poll (default: 500)\n    consumer.session.timeout.ms: 45000       # Session timeout (default: 45000)\n    consumer.heartbeat.interval.ms: 3000     # Heartbeat frequency (default: 3000)\n\n    # Processing Guarantees\n    processing.guarantee: at_least_once       # Options: at_least_once, exactly_once_v2\n\n    # Error Handling\n    default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndContinueExceptionHandler\n    default.production.exception.handler: org.apache.kafka.streams.errors.DefaultProductionExceptionHandler\n</code></pre> <p>Key Configuration Details:</p> <ul> <li>num.stream.threads: Controls parallelism. Set based on CPU cores and partition count</li> <li>cache.max.bytes.buffering: KSML automatically enables caching when <code>caching: true</code> is set on stores</li> <li>topology.optimization: KSML defaults to <code>StreamsConfig.OPTIMIZE</code> for performance</li> <li>commit.interval.ms: Balance between throughput and latency. Higher values = better throughput</li> <li>processing.guarantee: <code>exactly_once_v2</code> provides stronger guarantees but lower performance</li> </ul> <p>KSML-Specific Optimizations:</p> <p>KSML automatically configures several optimizations:</p> <ul> <li>Custom exception handlers for production and deserialization errors</li> <li>Automatic header cleanup interceptors for all consumers</li> <li>Optimized client suppliers for resolving configurations</li> <li>Built-in metrics reporters with topology enrichment</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#state-store-configuration","title":"State Store Configuration","text":"<p>Configure state stores for optimal performance based on KSML's store configuration system:</p> <pre><code>stores:\n  # High-performance key-value store\n  optimized_keyvalue_store:\n    type: keyValue\n    keyType: string\n    valueType: string                # Use string over JSON for performance-critical stores\n    persistent: true                 # Enable durability (uses RocksDB)\n    caching: true                    # Enable caching for frequent access\n    logging: true                    # Enable changelog for fault tolerance\n    timestamped: false               # Disable if timestamps not needed\n    versioned: false                 # Disable if versioning not needed\n\n  # Memory-optimized store for temporary data\n  temp_cache_store:\n    type: keyValue\n    keyType: string\n    valueType: string\n    persistent: false                # In-memory only for speed\n    caching: true                    # Still enable caching\n    logging: false                   # No changelog needed for temp data\n\n  # Window store with retention\n  metrics_window_store:\n    type: window\n    keyType: string\n    valueType: json                  # JSON acceptable for metrics\n    windowSize: PT5M                 # 5-minute windows\n    retention: PT1H                  # Keep 1 hour of data\n    persistent: true\n    caching: true\n    logging: true\n\n  # Session store for user sessions  \n  user_session_store:\n    type: session\n    keyType: string\n    valueType: string                # Compact string format for sessions\n    retention: PT30M                 # 30-minute session timeout\n    persistent: true\n    caching: true\n    logging: true\n</code></pre> <p>Store Type Performance Characteristics:</p> Store Type Use Case Performance Notes keyValue General caching, counters, lookups Fastest access, least memory overhead window Time-based aggregations Automatic cleanup, good for time-series session User sessions, activity tracking Session-aware, handles gaps automatically <p>Configuration Parameter Details:</p> <ul> <li> <p>persistent: </p> <ul> <li><code>true</code>: Uses RocksDB for disk persistence, survives restarts</li> <li><code>false</code>: In-memory only, faster but data lost on restart</li> </ul> </li> <li> <p>caching: </p> <ul> <li><code>true</code>: KSML enables write caching to reduce downstream traffic</li> <li><code>false</code>: Direct writes, lower latency but higher network usage</li> </ul> </li> <li> <p>logging: </p> <ul> <li><code>true</code>: Creates changelog topic for fault tolerance and exactly-once processing</li> <li><code>false</code>: No changelog, faster but no fault tolerance</li> </ul> </li> <li> <p>timestamped: </p> <ul> <li><code>true</code>: Stores values with timestamps for time-aware processing</li> <li><code>false</code>: Plain values, more efficient when timestamps not needed</li> </ul> </li> <li> <p>versioned: </p> <ul> <li><code>true</code>: Maintains multiple versions of values with configurable retention</li> <li><code>false</code>: Single version only, more memory efficient</li> </ul> </li> </ul> <p>KSML Store Optimizations:</p> <p>KSML automatically optimizes store configuration:</p> <ul> <li>Uses efficient Serde types based on configured data types</li> <li>Applies proper supplier selection (persistent vs. in-memory)</li> <li>Configures caching and logging based on specified options</li> <li>Handles timestamped and versioned variants automatically</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#python-function-best-practices","title":"Python Function Best Practices","text":""},{"location":"tutorials/advanced/performance-optimization/#optimize-code-structure","title":"Optimize Code Structure","text":"<pre><code>functions:\n  optimized_function:\n    type: valueTransformer\n    globalCode: |\n      # Pre-compute expensive operations\n      MULTIPLIERS = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}\n\n      def efficient_calculation(value, priority):\n        return value * MULTIPLIERS.get(priority, 1.0)\n\n    code: |\n      # Use pre-computed values and avoid object creation\n      priority = value.get(\"priority\", \"low\")\n      result = efficient_calculation(value.get(\"amount\", 0), priority)\n      return f\"processed:{result:.2f}\"\n</code></pre> <p>Key techniques:</p> <ul> <li>Use globalCode: Pre-compute expensive operations outside the processing loop</li> <li>Minimize object creation: Reuse objects and data structures when possible</li> <li>Use built-in functions: They're typically faster than custom implementations</li> <li>Avoid unnecessary computations: Compute values only when needed</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#efficient-data-structures","title":"Efficient Data Structures","text":"<p>Choose optimal data structures based on KSML's data type system and your use case:</p>"},{"location":"tutorials/advanced/performance-optimization/#ksml-data-types-performance-ranking","title":"KSML Data Types (Performance Ranking)","text":"<pre><code># Fastest - Use for high-throughput scenarios\nkeyType: string         # Most efficient for keys\nvalueType: string       # Fastest serialization\n\n# Fast - Good balance of performance and functionality  \nkeyType: long          # Efficient numeric keys\nvalueType: avro:Schema # Efficient binary format with schema\n\n# Moderate - Flexible but more overhead\nkeyType: json          # Flexible structure  \nvalueType: json        # Easy to work with, good for Kowl UI\n\n# Slower - Use only when necessary\nvalueType: xml         # XML processing overhead\nvalueType: soap        # SOAP envelope overhead\n</code></pre>"},{"location":"tutorials/advanced/performance-optimization/#python-data-structure-guidelines","title":"Python Data Structure Guidelines","text":"<p>Optimal patterns for different scenarios:</p> <ol> <li> <p>Fast counters/accumulators <pre><code>count = int(store.get(key) or \"0\")  # String storage, int operations\ncount += 1\nstore.put(key, str(count))\n</code></pre></p> </li> <li> <p>Compact State Representation <pre><code># Instead of JSON: {\"count\": 5, \"sum\": 100, \"avg\": 20}\ncompact_state = f\"{count}:{total_sum}:{average}\"  # String format\nstore.put(key, compact_state)\n</code></pre></p> </li> <li> <p>Efficient Collections (limit size) <pre><code>items = json.loads(store.get(key) or \"[]\")\nitems.append(new_item)\nitems = items[-100:]  # Keep only last 100 items\nstore.put(key, json.dumps(items))\n</code></pre></p> </li> <li> <p>Fast Lookup Tables (use globalCode) <pre><code>STATUS_CODES = {1: \"active\", 2: \"inactive\", 3: \"pending\"}  # Pre-computed\nstatus = STATUS_CODES.get(status_id, \"unknown\")  # O(1) lookup\n</code></pre></p> </li> <li> <p>Efficient String Operations <pre><code># avoid\nAvoid: result = f\"processed:{type}:{user}:{score}\"\n# use this instead\nUse: result = \"processed:\" + type + \":\" + user + \":\" + str(score)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/advanced/performance-optimization/#memory-efficient-patterns","title":"Memory-Efficient Patterns","text":"<p>Optimal memory usage techniques:</p> <ol> <li> <p>Reuse Objects <pre><code>result = {\"status\": \"ok\", \"count\": 0}  # Create once in globalCode\nresult[\"count\"] = new_count            # Reuse, don't recreate\n</code></pre></p> </li> <li> <p>Generator Patterns <pre><code>def process_batch(items):\n    for item in items:  # Memory efficient iteration\n        yield process_item(item)\n</code></pre></p> </li> <li> <p>Lazy Evaluation  <pre><code>expensive_result = None\nif condition_needs_it:  # Only compute when needed\n    expensive_result = expensive_calculation()\n</code></pre></p> </li> <li> <p>Compact Data Types <pre><code># use this:\ntimestamp = int(time.time())      # 4-8 bytes\n# instead of this\ntimestamp = str(time.time())      # ~20 bytes + overhead\n</code></pre></p> </li> </ol> <p>KSML-Specific Optimizations:</p> <ul> <li>Serde Selection: KSML automatically chooses optimal serializers based on data type</li> <li>Union Types: For flexible schemas, KSML uses efficient UnionSerde implementation  </li> <li>Type Flattening: Complex types are automatically flattened for performance</li> <li>Caching: Serde instances are cached and reused across operations</li> </ul> <p>Data Type Performance Comparison:</p> Data Type Serialization Speed Size Efficiency Schema Evolution Use Case string Fastest Good Limited Simple values, IDs, compact formats long/int Fastest Excellent None Counters, timestamps, numeric keys avro Fast Excellent Excellent Complex schemas, production systems json Moderate Good Good Development, debugging, flexible data protobuf(coming soon) Fast Excellent Good High-performance, cross-language xml/soap Slow Poor Limited Legacy systems, specific protocols"},{"location":"tutorials/advanced/performance-optimization/#monitoring-performance","title":"Monitoring Performance","text":""},{"location":"tutorials/advanced/performance-optimization/#key-metrics-to-track","title":"Key Metrics to Track","text":"<p>Monitor these metrics to identify performance issues:</p> <pre><code>functions:\n  performance_monitor:\n    type: forEach\n    code: |\n      # Record processing time\n      start_time = time.time()\n      process_message(key, value)\n      processing_time = (time.time() - start_time) * 1000\n\n      # Log performance metrics periodically\n      if processing_time &gt; 100:  # Log slow operations\n        log.warn(\"Slow processing detected: {:.2f}ms for key {}\", processing_time, key)\n</code></pre> <p>Important metrics:</p> <ul> <li>Processing latency: Time to process individual messages</li> <li>Throughput: Messages processed per second</li> <li>Memory usage: JVM heap utilization</li> <li>State store size: Growth rate and total size</li> <li>Error rates: Failed processing attempts</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"tutorials/advanced/performance-optimization/#dos","title":"Do's","text":"<ul> <li>Measure first: Establish baselines before optimizing</li> <li>Filter early: Remove unwanted data as soon as possible</li> <li>Use caching: Configure appropriate cache sizes for state stores</li> <li>Optimize hot paths: Focus on frequently executed code</li> <li>Pre-compute values: Move expensive operations to globalCode</li> <li>Choose efficient formats: Use compact data representations</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#donts","title":"Don'ts","text":"<ul> <li>Don't over-optimize: Focus on actual bottlenecks, not theoretical ones</li> <li>Don't ignore trade-offs: Balance throughput, latency, and resource usage</li> <li>Don't optimize without measuring: Always measure the impact of changes</li> <li>Don't create unnecessary objects: Reuse data structures when possible</li> <li>Don't log excessively: Reduce I/O overhead from logging</li> <li>Don't use complex serialization: Avoid JSON when simple formats suffice</li> </ul>"},{"location":"tutorials/advanced/performance-optimization/#conclusion","title":"Conclusion","text":"<p>Performance optimization in KSML involves a combination of efficient Python code, optimized configuration, and smart pipeline design. By applying these techniques systematically and measuring their impact, you can build KSML applications that efficiently process high volumes of data with consistent performance.</p> <p>The key is to start with measurement, identify actual bottlenecks, and apply optimizations incrementally while monitoring their effectiveness.</p>"},{"location":"tutorials/beginner/","title":"Beginner Tutorials","text":"<p>Welcome to the KSML beginner tutorials! These tutorials are designed for users who are new to KSML and want to learn the basics of building data pipelines with KSML.</p> <p>Each tutorial provides step-by-step instructions and explanations to help you understand not just how to use KSML, but why certain approaches are recommended.</p>"},{"location":"tutorials/beginner/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/beginner/#ksml-basics-tutorial","title":"KSML Basics Tutorial","text":"<p>If you did not complete this yet, this tutorial guides you through building a simple data pipeline that filters and transforms temperature data. You'll learn:</p> <ul> <li>How to define streams for input and output</li> <li>How to create and use functions</li> <li>How to build a pipeline with filter, transform, and logging operations</li> <li>How to run and test your KSML application</li> </ul>"},{"location":"tutorials/beginner/#filtering-and-transforming-data","title":"Filtering and Transforming Data","text":"<p>This tutorial expands on the basics by exploring more advanced filtering and transformation techniques:</p> <ul> <li>Using complex filter conditions</li> <li>Applying multiple transformations</li> <li>Working with nested data structures</li> <li>Error handling in transformations</li> </ul>"},{"location":"tutorials/beginner/#working-with-different-data-formats","title":"Working with Different Data Formats","text":"<p>Learn how to work with various data formats in KSML:</p> <ul> <li>JSON data processing</li> <li>Avro schema integration</li> <li>CSV data handling</li> <li>Converting between formats</li> </ul>"},{"location":"tutorials/beginner/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>This tutorial focuses on how to add effective logging and monitoring to your KSML pipelines:</p> <ul> <li>Using the peek operation for logging</li> <li>Creating custom logging functions</li> <li>Adding metrics to your pipelines</li> <li>Troubleshooting with logs</li> </ul>"},{"location":"tutorials/beginner/#learning-path","title":"Learning Path","text":"<p>We recommend following these tutorials in order, as each one builds on concepts introduced in previous tutorials.</p>"},{"location":"tutorials/beginner/#next-steps","title":"Next Steps","text":"<p>After completing these beginner tutorials, you're ready to move on to more advanced topics:</p> <p>\ud83d\udc49 Continue to Intermediate Tutorials to learn about aggregations, joins, and windowing operations.</p>"},{"location":"tutorials/beginner/data-formats/","title":"Working with Different Data Formats","text":"<p>Learn how to process, convert, and validate data using KSML's supported formats through practical, hands-on examples. This tutorial provides complete working examples for each data format.</p> <p>For comprehensive syntax reference and format details, see the Data Types and Formats Reference.</p>"},{"location":"tutorials/beginner/data-formats/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of Kafka concepts (topics, messages)</li> <li>Familiarity with basic KSML concepts (streams, functions, pipelines)</li> </ul>"},{"location":"tutorials/beginner/data-formats/#supported-data-formats","title":"Supported Data Formats","text":"<ul> <li>String: Plain text</li> <li>JSON: Structured data without schema validation</li> <li>Avro: Binary format with schema registry integration</li> <li>CSV: Tabular data with optional schema</li> <li>XML: Hierarchical data with XSD schema support</li> <li>Binary: Raw bytes for custom protocols</li> <li>SOAP: Web service messaging format</li> </ul>"},{"location":"tutorials/beginner/data-formats/#specifying-data-formats","title":"Specifying Data Formats","text":"<p>When defining streams in KSML, you specify the data format using the <code>keyType</code> and <code>valueType</code> properties:</p> Specifying data formats (click to expand) <pre><code>streams:\n  json_stream:\n    topic: example_json_topic\n    keyType: string\n    valueType: json\n\n  avro_stream:\n    topic: example_avro_topic\n    keyType: string\n    valueType: avro:SensorData\n</code></pre> <p>Schema-based formats (Avro, XML, CSV) require a schema name: <code>format:SchemaName</code> (e.g., <code>avro:SensorData</code>).</p>"},{"location":"tutorials/beginner/data-formats/#working-with-avro-data","title":"Working with Avro Data","text":"<p>Avro provides schema-based binary serialization with validation, evolution support, and compact encoding.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements-for-avro","title":"Setup Requirements for Avro","text":"<p>Avro format requires a schema registry, so this tutorial needs a different docker-compose.yml than other tutorials.</p> <p>Create <code>docker-compose.yml</code> with schema registry and pre-created topics:</p> Docker Compose Configuration (click to expand) <pre><code>networks:\n  ksml:\n    name: ksml_example\n    driver: bridge\n\nservices:\n  broker:\n    image: apache/kafka:3.8.0\n    hostname: broker\n    ports:\n      - \"9092:9092\"\n    networks:\n      - ksml\n    restart: always\n    environment:\n      KAFKA_PROCESS_ROLES: 'controller,broker'\n      KAFKA_BROKER_ID: 0\n      KAFKA_NODE_ID: 0\n      KAFKA_CONTROLLER_QUORUM_VOTERS: '0@broker:9090'\n      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n\n      KAFKA_ADVERTISED_LISTENERS: 'INNER://broker:9093,OUTER://localhost:9092'\n      KAFKA_LISTENERS: 'INNER://broker:9093,OUTER://broker:9092,CONTROLLER://broker:9090'\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INNER:PLAINTEXT,OUTER:PLAINTEXT,CONTROLLER:PLAINTEXT'\n      KAFKA_LOG_CLEANUP_POLICY: delete\n      KAFKA_LOG_RETENTION_MINUTES: 10\n      KAFKA_INTER_BROKER_LISTENER_NAME: INNER\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'\n      KAFKA_MIN_INSYNC_REPLICAS: 1\n      KAFKA_NUM_PARTITIONS: 1\n      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'\n    healthcheck:\n      # If the kafka topics can list data, the broker is healthy\n      test: /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9093 --list\n      interval: 5s\n      timeout: 10s\n      retries: 10\n      start_period: 5s\n\n  ksml:\n    image: registry.axual.io/opensource/images/axual/ksml:1.1.0\n    networks:\n      - ksml\n    container_name: ksml\n    working_dir: /ksml\n    volumes:\n      - ./examples:/ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n      kafka-setup:\n        condition: service_completed_successfully\n      schema_registry:\n        condition: service_healthy\n\n\n  schema_registry:\n    image: apicurio/apicurio-registry:3.0.2\n    hostname: schema-registry\n    depends_on:\n      broker:\n        condition: service_healthy\n    ports:\n      - \"8081:8081\"\n    networks:\n      - ksml\n    restart: always\n    environment:\n      QUARKUS_HTTP_PORT: 8081\n      QUARKUS_HTTP_CORS_ORIGINS: '*'\n      QUARKUS_PROFILE: \"prod\"\n      APICURIO_STORAGE_KIND: kafkasql\n      APICURIO_KAFKASQL_BOOTSTRAP_SERVERS: 'broker:9093'\n      APICURIO_KAFKASQL_TOPIC: '_apciurio-kafkasql-store'\n    healthcheck:\n      # If the api endpoint is available, the service is considered healthy\n      test: curl http://localhost:8081/apis\n      interval: 15s\n      timeout: 10s\n      retries: 10\n      start_period: 10s\n\n  kafka-ui:\n    image: quay.io/cloudhut/kowl:master\n    platform: linux/amd64\n    container_name: kowl\n    restart: always\n    ports:\n      - 8080:8080\n    volumes:\n      - ./kowl-ui-config.yaml:/config/kowl-ui-config.yaml:ro\n    environment:\n      CONFIG_FILEPATH: \"/config/kowl-ui-config.yaml\"\n    depends_on:\n      broker:\n        condition: service_healthy\n    networks:\n      - ksml\n\n  # This \"container\" is a workaround to pre-create topics, because setting KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE to true results in Kafka Streams errors\n  kafka-setup:\n    image: apache/kafka:3.8.0\n    hostname: kafka-setup\n    networks:\n      - ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n    restart: on-failure\n    command: \"bash -c 'echo Creating topics for data formats tutorial... &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_avro &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_json &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_transformed'\"\n</code></pre> <p>Create <code>kowl-ui-config.yaml</code> for Kafka UI:</p> Kafka UI Configuration (click to expand) <pre><code>server:\n  listenPort: 8080\n  listenAddress: 0.0.0.0\n\nkafka:\n  brokers:\n    - broker:9093\n  schemaRegistry:\n    enabled: true\n    urls:\n      - http://schema-registry:8081/apis/ccompat/v7\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#ksml-runner-configuration-options","title":"KSML Runner Configuration Options","text":"<p>KSML supports two Avro implementations. Choose one based on your schema registry setup:</p>"},{"location":"tutorials/beginner/data-formats/#option-1-confluent_avro-confluent-schema-registry-compatible","title":"Option 1: confluent_avro (Confluent Schema Registry Compatible)","text":"<p>Use this configuration when working with Confluent Schema Registry or when you need full Confluent compatibility:</p> KSML Runner Configuration - confluent_avro (click to expand) <pre><code>ksml:\n  definitions:\n    producer: producer-avro.yaml\n    processor: processor-avro-transform.yaml\n  schemaRegistries:\n    my_schema_registry:\n      config:\n        schema.registry.url: http://schema-registry:8081/apis/ccompat/v7\n  notations:\n    avro:\n      type: confluent_avro\n      schemaRegistry: my_schema_registry\n      config:\n        normalize.schemas: true\n        auto.register.schemas: true\n  storageDirectory: /ksml/state\n  createStorageDirectory: true\n\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.schema.registry.test\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre> <p>Key features:</p> <ul> <li>Uses Confluent compatibility API (<code>/apis/ccompat/v7</code>)</li> <li>Full compatibility with Confluent Schema Registry ecosystem</li> <li>Works seamlessly with Kowl UI for schema viewing</li> </ul>"},{"location":"tutorials/beginner/data-formats/#option-2-apicurio_avro-apicurio-native-api","title":"Option 2: apicurio_avro (Apicurio Native API)","text":"<p>Use this configuration when working with Apicurio Schema Registry's native capabilities:</p> KSML Runner Configuration - apicurio_avro (click to expand) <pre><code>ksml:\n  definitions:\n    producer: producer-avro.yaml\n    processor: processor-avro-convert.yaml\n  schemaRegistries:\n    my_schema_registry:\n      config:\n        apicurio.registry.url: http://schema-registry:8081/apis/registry/v2\n        apicurio.registry.dereference-schema: true\n        apicurio.registry.serializer.dereference-schema: true\n        apicurio.registry.deserializer.dereference-schema: true\n        apicurio.registry.auto-register: true\n        apicurio.registry.auto-register.if-exists: RETURN\n  notations:\n    avro:\n      type: apicurio_avro\n      schemaRegistry: my_schema_registry\n      config:\n        normalize.schemas: true\n        auto.register.schemas: true\n  storageDirectory: /ksml/state\n  createStorageDirectory: true\n\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.apicurio.schema.registry.test\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre> <p>Key features:</p> <ul> <li>Uses Apicurio native API (<code>/apis/registry/v2</code>)</li> <li>Apicurio-specific features and configuration options</li> <li>KSML 1.1.0 supports <code>/v2</code> endpoint only (does not support <code>/v3</code> endpoint)</li> </ul> <p>Important Notes:</p> <ul> <li>Kowl UI compatibility warning: When using <code>apicurio_avro</code>, you may see compatibility mode warnings in Kowl. This is expected because Kowl only supports the Confluent compatibility endpoint (<code>/apis/ccompat/v7</code>) for viewing schemas, while <code>apicurio_avro</code> uses the native Apicurio API.</li> <li>The warning doesn't affect schema viewing in Kowl UI nor KSML functionality. Everything works correctly.</li> </ul>"},{"location":"tutorials/beginner/data-formats/#running-the-examples","title":"Running the Examples","text":"<ul> <li>For each example, create <code>producer.yaml</code> and <code>processor.yaml</code> files and reference them from your chosen <code>ksml-runner.yaml</code></li> <li>Restart KSML: <code>docker compose down &amp; docker compose up -d &amp;&amp; docker compose logs ksml -f</code> (which is faster than <code>docker compose restart ksml</code>)</li> </ul>"},{"location":"tutorials/beginner/data-formats/#avro-examples","title":"Avro Examples","text":"<p>This producer generates JSON data that KSML automatically converts to Avro format using the schema registry:</p> Producer definition for Avro messages (click to expand) <pre><code>functions:\n  generate_sensordata_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate random sensor measurement data\n      value = {\n        \"name\": key,\n        \"timestamp\": round(time.time()*1000),     # long timestamp (not string)\n        \"value\": str(random.randrange(0, 100)),\n        \"type\": random.choice([\"AREA\", \"HUMIDITY\", \"LENGTH\", \"STATE\", \"TEMPERATURE\"]),  # Valid enum values\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\", \"m2\", \"m\", \"boolean\"]),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]) if random.random() &gt; 0.3 else None,\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"]) if random.random() &gt; 0.3 else None,\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]) if random.random() &gt; 0.3 else None\n      }\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Indicate the type of key and value\n\nproducers:\n  # Produce an Avro SensorData message every 3 seconds\n  sensordata_avro_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: sensor_data_avro\n      keyType: string\n      valueType: avro:SensorData\n</code></pre> <p>Create <code>examples/SensorData.avsc</code> schema file (JSON format, auto-loaded from working directory):</p> Avro Schema for examples below (click to expand) <pre><code>{\n  \"namespace\": \"io.axual.ksml.example\",\n  \"doc\": \"Emulated sensor data with a few additional attributes\",\n  \"name\": \"SensorData\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"doc\": \"The name of the sensor\",\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The timestamp of the sensor reading\",\n      \"name\": \"timestamp\",\n      \"type\": \"long\"\n    },\n    {\n      \"doc\": \"The value of the sensor, represented as string\",\n      \"name\": \"value\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The type of the sensor\",\n      \"name\": \"type\",\n      \"type\": {\n        \"name\": \"SensorType\",\n        \"type\": \"enum\",\n        \"doc\": \"The type of a sensor\",\n        \"symbols\": [\n          \"AREA\",\n          \"HUMIDITY\",\n          \"LENGTH\",\n          \"STATE\",\n          \"TEMPERATURE\"\n        ]\n      }\n    },\n    {\n      \"doc\": \"The unit of the sensor\",\n      \"name\": \"unit\",\n      \"type\": \"string\"\n    },\n    {\n      \"doc\": \"The color of the sensor\",\n      \"name\": \"color\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    },\n    {\n      \"doc\": \"The city of the sensor\",\n      \"name\": \"city\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    },\n    {\n      \"doc\": \"The owner of the sensor\",\n      \"name\": \"owner\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    }\n  ]\n}\n</code></pre> <p>This processor converts Avro messages to JSON using the <code>convertValue</code> operation:</p> Avro to JSON conversion processor (click to expand) <pre><code>streams:\n  avro_input:\n    topic: sensor_data_avro\n    keyType: string\n    valueType: avro:SensorData\n    offsetResetPolicy: latest\n\n  json_output:\n    topic: sensor_data_json\n    keyType: string\n    valueType: json\n\npipelines:\n  avro_to_json_pipeline:\n    from: avro_input\n    via:\n      # Log the incoming Avro data\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Original Avro: sensor={}, type={}, value={}{}, timestamp={}, owner={}\",\n                      value.get(\"name\"), value.get(\"type\"), value.get(\"value\"), value.get(\"unit\"), \n                      value.get(\"timestamp\"), value.get(\"owner\"))\n            else:\n              log.info(\"Received null message with key={}\", key)\n\n      # Explicitly convert from Avro to JSON\n      - type: convertValue\n        into: json\n\n      # Log the converted JSON data\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Converted to JSON: sensor={}, type={}, city={}, color={}\", \n                      value.get(\"name\"), value.get(\"type\"), value.get(\"city\"), value.get(\"color\"))\n            else:\n              log.info(\"JSON conversion result: null\")\n\n    to: json_output\n</code></pre> <p>This processor transforms Avro data (uppercases sensor names) while maintaining the Avro format:</p> Avro transformation processor (click to expand) <pre><code>streams:\n  avro_input:\n    topic: sensor_data_avro\n    keyType: string\n    valueType: avro:SensorData\n    offsetResetPolicy: latest\n\n  transformed_output:\n    topic: sensor_data_transformed\n    keyType: string\n    valueType: avro:SensorData\n\nfunctions:\n  uppercase_sensor_name:\n    type: valueTransformer\n    code: |\n      # Simple transformation: uppercase the sensor name\n      result = dict(value) if value else None\n      if result and result.get(\"name\"):\n        result[\"name\"] = result[\"name\"].upper()\n    expression: result\n    resultType: avro:SensorData\n\npipelines:\n  transformation_pipeline:\n    from: avro_input\n    via:\n      # Step 1: Apply transformation\n      - type: transformValue\n        mapper: uppercase_sensor_name\n\n      # Step 2: Log the transformation\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Transformed sensor: name={}, type={}, timestamp={}, value={}{}\", \n                      value.get(\"name\"), value.get(\"type\"), value.get(\"timestamp\"), \n                      value.get(\"value\"), value.get(\"unit\"))\n            else:\n              log.info(\"Transformed sensor: null\")\n\n    to: transformed_output \n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-jsonschema-data","title":"Working with JsonSchema Data","text":"<p>JsonSchema provides structured JSON data validation with schema registry support and strict type enforcement, enabling schema evolution and compatibility checks.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements-for-jsonschema","title":"Setup Requirements for JsonSchema","text":"<p>JsonSchema requires a schema registry setup with manual schema registration. Use this Docker Compose configuration:</p> Docker Compose setup with JsonSchema support (click to expand) <pre><code>networks:\n  ksml:\n    name: ksml_example\n    driver: bridge\n\nservices:\n  broker:\n    image: apache/kafka:3.8.0\n    hostname: broker\n    ports:\n      - \"9092:9092\"\n    networks:\n      - ksml\n    restart: always\n    environment:\n      KAFKA_PROCESS_ROLES: 'controller,broker'\n      KAFKA_BROKER_ID: 0\n      KAFKA_NODE_ID: 0\n      KAFKA_CONTROLLER_QUORUM_VOTERS: '0@broker:9090'\n      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n\n      KAFKA_ADVERTISED_LISTENERS: 'INNER://broker:9093,OUTER://localhost:9092'\n      KAFKA_LISTENERS: 'INNER://broker:9093,OUTER://broker:9092,CONTROLLER://broker:9090'\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INNER:PLAINTEXT,OUTER:PLAINTEXT,CONTROLLER:PLAINTEXT'\n      KAFKA_LOG_CLEANUP_POLICY: delete\n      KAFKA_LOG_RETENTION_MINUTES: 10\n      KAFKA_INTER_BROKER_LISTENER_NAME: INNER\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'\n      KAFKA_MIN_INSYNC_REPLICAS: 1\n      KAFKA_NUM_PARTITIONS: 1\n      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'\n    healthcheck:\n      # If the kafka topics can list data, the broker is healthy\n      test: /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9093 --list\n      interval: 5s\n      timeout: 10s\n      retries: 10\n      start_period: 5s\n\n  ksml:\n    image: registry.axual.io/opensource/images/axual/ksml:1.1.0\n    networks:\n      - ksml\n    container_name: ksml\n    working_dir: /ksml\n    volumes:\n      - ./examples:/ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n      kafka-setup:\n        condition: service_completed_successfully\n      schema_registry:\n        condition: service_healthy\n      schema-registration:\n        condition: service_completed_successfully\n\n\n  schema_registry:\n    image: apicurio/apicurio-registry:3.0.2\n    hostname: schema-registry\n    depends_on:\n      broker:\n        condition: service_healthy\n    ports:\n      - \"8081:8081\"\n    networks:\n      - ksml\n    restart: always\n    environment:\n      QUARKUS_HTTP_PORT: 8081\n      QUARKUS_HTTP_CORS_ORIGINS: '*'\n      QUARKUS_PROFILE: \"prod\"\n      APICURIO_STORAGE_KIND: kafkasql\n      APICURIO_KAFKASQL_BOOTSTRAP_SERVERS: 'broker:9093'\n      APICURIO_KAFKASQL_TOPIC: '_apciurio-kafkasql-store'\n    healthcheck:\n      # If the api endpoint is available, the service is considered healthy\n      test: curl http://localhost:8081/apis\n      interval: 15s\n      timeout: 10s\n      retries: 10\n      start_period: 10s\n\n  kafka-ui:\n    image: quay.io/cloudhut/kowl:master\n    platform: linux/amd64\n    container_name: kowl\n    restart: always\n    ports:\n      - 8080:8080\n    volumes:\n      - ./kowl-ui-config.yaml:/config/kowl-ui-config.yaml:ro\n    environment:\n      CONFIG_FILEPATH: \"/config/kowl-ui-config.yaml\"\n    depends_on:\n      broker:\n        condition: service_healthy\n    networks:\n      - ksml\n\n  # This \"container\" is a workaround to pre-create topics, because setting KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE to true results in Kafka Streams errors\n  kafka-setup:\n    image: apache/kafka:3.8.0\n    hostname: kafka-setup\n    networks:\n      - ksml\n    depends_on:\n      broker:\n        condition: service_healthy\n    restart: on-failure\n    command: \"bash -c 'echo Creating topics for JsonSchema tutorial... &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_avro &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_transformed &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_jsonschema &amp;&amp; \\\n                       /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_data_jsonschema_processed'\"\n\n  # Service to register JsonSchema schemas in Apicurio\n  schema-registration:\n    image: alpine:latest\n    hostname: schema-registration\n    networks:\n      - ksml\n    depends_on:\n      schema_registry:\n        condition: service_healthy\n    volumes:\n      - ./examples:/schemas:ro\n    command:\n      - /bin/sh\n      - -c\n      - |\n        apk add --no-cache curl jq\n        echo \"Waiting for Apicurio Schema Registry to be ready...\"\n        until curl -s http://schema-registry:8081/apis &gt; /dev/null 2&gt;&amp;1; do\n          echo \"Schema Registry not ready yet, waiting...\"\n          sleep 5\n        done\n        echo \"Schema Registry is ready. Registering schemas...\"\n        SCHEMA_CONTENT=$$(cat /schemas/SensorData.json)\n        ESCAPED_SCHEMA=$$(echo \"$$SCHEMA_CONTENT\" | jq -Rs .)\n        PAYLOAD=$$(echo \"{\\\"schema\\\": $$ESCAPED_SCHEMA, \\\"schemaType\\\": \\\"JSON\\\"}\")\n        echo \"Registering SensorData schema...\"\n        curl -X POST -H \"Content-Type: application/json\" -d \"$$PAYLOAD\" \\\n          \"http://schema-registry:8081/apis/ccompat/v7/subjects/sensor_data_jsonschema-value/versions?normalize=true\"\n        curl -X POST -H \"Content-Type: application/json\" -d \"$$PAYLOAD\" \\\n          \"http://schema-registry:8081/apis/ccompat/v7/subjects/sensor_data_jsonschema_processed-value/versions?normalize=true\"\n        echo \"Schema registration completed!\"\n</code></pre> <p>Key features:</p> <ul> <li>Includes automatic JsonSchema schema registration service (<code>schema-registration</code>)</li> <li>Creates topics for JsonSchema examples</li> <li>Uses Apicurio Schema Registry with both Confluent compatibility API and native Apicurio API endpoints</li> </ul> <p>Create the required Kafka UI configuration file for schema registry integration:</p> Kafka UI Configuration (kowl-ui-config.yaml) (click to expand) <pre><code>server:\n  listenPort: 8080\n  listenAddress: 0.0.0.0\n\nkafka:\n  brokers:\n    - broker:9093\n  schemaRegistry:\n    enabled: true\n    urls:\n      - http://schema-registry:8081/apis/ccompat/v7\n</code></pre> <p>Note: This configuration file is essential for the Kafka UI (Kowl) to connect to both Kafka brokers and the schema registry for viewing schemas and deserializing messages.</p> <p>Configure KSML runner for JsonSchema processing:</p> KSML Runner configuration for JsonSchema (click to expand) <pre><code>ksml:\n  definitions:\n    jsonschema-producer: jsonschema-producer.yaml\n    jsonschema-processor: jsonschema-processor.yaml\n  schemaRegistries:\n    my_apicurio_registry:\n      config:\n        apicurio.registry.url: http://schema-registry:8081/apis/registry/v2\n  notations:\n    jsonschema:\n      type: apicurio_jsonschema\n      schemaRegistry: my_apicurio_registry\n      config: {}\n  storageDirectory: /ksml/state\n  createStorageDirectory: true\n\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: io.ksml.jsonschema.example\n  security.protocol: PLAINTEXT\n  acks: all\n</code></pre> <p>Important configuration details:</p> <ul> <li>Defines schema registry: <code>my_apicurio_registry</code> (for JsonSchema)</li> <li>Shows how to configure <code>apicurio_jsonschema</code> notation</li> <li>JsonSchema schemas must be manually registered with Apicurio (auto-registration not supported by Apicurio)</li> </ul>"},{"location":"tutorials/beginner/data-formats/#jsonschema-examples","title":"JsonSchema Examples","text":"<p>This producer generates JSON data that KSML validates against JsonSchema format using the schema registry:</p> Producer definition for JsonSchema messages (click to expand) <pre><code>functions:\n  generate_sensordata_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate random sensor measurement data\n      value = {\n        \"name\": key,\n        \"timestamp\": str(round(time.time()*1000)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"AREA\", \"LENGTH\", \"STATE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"value\": str(random.randrange(0, 100)),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"])\n      }\n\n      log.info(\"Generating sensor data: {}\", key)\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Indicate the type of key and value\n\nproducers:\n  # Produce a JsonSchema SensorData message every 3 seconds\n  sensordata_jsonschema_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: sensor_data_jsonschema\n      keyType: string\n      valueType: jsonschema:SensorData\n</code></pre> <p>Create <code>examples/SensorData.json</code> schema file (JSON Schema format, manually registered via Docker service):</p> JsonSchema Schema for examples below (click to expand) <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"$id\": \"https://ksml.io/sensordata.json\",\n  \"title\": \"SensorData\",\n  \"description\": \"Emulated sensor data with a few additional attributes\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": {\n      \"description\": \"The name of the sensor\",\n      \"type\": \"string\"\n    },\n    \"timestamp\": {\n      \"description\": \"The timestamp of the sensor reading\",\n      \"type\": \"number\"\n    },\n    \"value\": {\n      \"description\": \"The value of the sensor, represented as string\",\n      \"type\": \"string\"\n    },\n    \"type\": {\n      \"description\": \"The type of the sensor\",\n      \"type\": \"string\",\n      \"enum\": [\n        \"AREA\",\n        \"HUMIDITY\",\n        \"LENGTH\",\n        \"STATE\",\n        \"TEMPERATURE\"\n      ]\n    },\n    \"unit\": {\n      \"description\": \"The unit of the sensor\",\n      \"type\": \"string\"\n    },\n    \"color\": {\n      \"description\": \"The color of the sensor\",\n      \"type\": \"string\",\n      \"default\": null\n    },\n    \"city\": {\n      \"description\": \"The city of the sensor\",\n      \"type\": \"string\",\n      \"default\": null\n    },\n    \"owner\": {\n      \"description\": \"The owner of the sensor\",\n      \"type\": \"string\",\n      \"default\": null\n    }\n  }\n}\n</code></pre> <p>This processor transforms JsonSchema data (adds processing timestamp and uppercase sensor ID) then converts to JSON format:</p> JsonSchema transformation processor (click to expand) <pre><code>streams:\n  jsonschema_input:\n    topic: sensor_data_jsonschema\n    keyType: string\n    valueType: jsonschema:SensorData\n    offsetResetPolicy: earliest\n\n  jsonschema_processed:\n    topic: sensor_data_jsonschema_processed\n    keyType: string\n    valueType: json\n\nfunctions:\n  enrich_sensor_data:\n    type: valueTransformer\n    code: |\n      # Simple transformation: add processing timestamp\n      import time\n      result = dict(value) if value else {}\n      result[\"processed_at\"] = str(int(time.time() * 1000))\n      # Uppercase the sensor name\n      if result.get(\"name\"):\n        result[\"sensor_id\"] = result[\"name\"].upper()\n    expression: result\n    resultType: json\n\npipelines:\n  # Main processing pipeline\n  jsonschema_processing:\n    from: jsonschema_input\n    via:      \n      # Apply transformation to enrich data\n      - type: transformValue\n        mapper: enrich_sensor_data\n\n      # Log the processed data\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processed JsonSchema sensor: name={}, sensor_id={}, processed_at={}, full_value={}\", \n                     value.get(\"name\") if value else \"null\",\n                     value.get(\"sensor_id\") if value else \"null\",\n                     value.get(\"processed_at\") if value else \"null\",\n                     str(value))\n\n    to: jsonschema_processed\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-schemaless-json-data","title":"Working with Schemaless JSON Data","text":"<p>JSON provides flexible, human-readable structured data without schema validation requirements.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements","title":"Setup Requirements","text":"<p>JSON data processing does not require a schema registry. Make sure there is a running Docker Compose KSML environment as described in the Quick Start.</p> <p>This producer generates JSON sensor data directly (no format conversion needed):</p> Producer definition for JSON messages (click to expand) <pre><code>functions:\n  generate_json_sensordata:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate random sensor measurement data as JSON\n      value = {\n        \"name\": key,\n        \"timestamp\": str(round(time.time()*1000)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"PRESSURE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"value\": str(random.randrange(0, 100)),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"])\n      }\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Indicate the type of key and value\n\nproducers:\n  # Produce JSON sensor data messages every 2 seconds\n  json_sensor_producer:\n    generator: generate_json_sensordata\n    interval: 2s\n    to:\n      topic: sensor_data_json_raw\n      keyType: string\n      valueType: json\n</code></pre> <p>This processor demonstrates key-value transformation using <code>keyValueTransformer</code> to modify both message keys and values:</p> Processor definition for JSON messages (click to expand) <pre><code>streams:\n  json_input:\n    topic: sensor_data_json_raw\n    keyType: string\n    valueType: json\n    offsetResetPolicy: latest\n\n  json_output:\n    topic: sensor_data_json_processed\n    keyType: string\n    valueType: json\n\nfunctions:\n  add_processing_info:\n    type: keyValueTransformer\n    code: |\n      # Simple transformation: add processing info\n      import time\n      new_value = dict(value) if value else {}\n      new_value[\"processed\"] = True\n      new_value[\"processed_at\"] = str(int(time.time() * 1000))\n      new_key = f\"processed_{key}\"\n    expression: (new_key, new_value)\n    resultType: (string, json)\n\npipelines:\n  json_processing_pipeline:\n    from: json_input\n    via:\n      # Transform both key and value\n      - type: transformKeyValue\n        mapper: add_processing_info\n\n      # Log the transformed data\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Transformed: key={}, sensor={}, processed_at={}\",\n                    key, value.get(\"name\"), value.get(\"processed_at\"))\n\n    to: json_output\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-csv-data","title":"Working with CSV Data","text":"<p>CSV handles tabular data with schema-based column definitions and structured object access.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements_1","title":"Setup Requirements","text":"<p>CSV data processing does not require a schema registry. Make sure there is a running Docker Compose KSML environment as described in the Quick Start.</p> <p>Create <code>examples/SensorData.csv</code> schema file (defines column order):</p> CSV schema (click to expand) <pre><code>name,timestamp,value,type,unit,color,city,owner\n</code></pre> <p>This producer generates JSON data that KSML converts to CSV format using the schema:</p> Producer definition for CSV messages (click to expand) <pre><code>functions:\n  generate_sensordata_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate data matching SensorData.csv schema\n      # Schema: name,timestamp,value,type,unit,color,city,owner\n      value = {\n        \"name\": key,\n        \"timestamp\": str(round(time.time()*1000)),\n        \"value\": str(random.randrange(0, 100)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"PRESSURE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"])\n      }\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Generate as JSON, KSML will convert to CSV\n\nproducers:\n  # Produce CSV sensor data messages every 3 seconds\n  sensordata_csv_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: ksml_sensordata_csv\n      keyType: string\n      valueType: csv:SensorData                  # KSML will convert JSON to CSV format\n</code></pre> <p>This processor demonstrates CSV data manipulation (uppercases city names) while maintaining CSV format:</p> Processor definition for CSV messages (click to expand) <pre><code>streams:\n  csv_input:\n    topic: ksml_sensordata_csv\n    keyType: string\n    valueType: csv:SensorData\n    offsetResetPolicy: latest\n\n  csv_output:\n    topic: ksml_sensordata_csv_processed\n    keyType: string\n    valueType: csv:SensorData\n\nfunctions:\n  uppercase_city:\n    type: valueTransformer\n    code: |\n      # When using csv:SensorData, the data comes as a structured object (dict)\n      if value and isinstance(value, dict):\n        # Create a copy and uppercase the city\n        enriched = dict(value)\n        if \"city\" in enriched:\n          enriched[\"city\"] = enriched[\"city\"].upper()\n        result = enriched\n      else:\n        result = value\n    expression: result\n    resultType: csv:SensorData\n\npipelines:\n  process_csv:\n    from: csv_input\n    via:\n      # Log the original CSV data\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Original: sensor={}, city={}\", value.get(\"name\"), value.get(\"city\"))\n\n      # Transform the CSV data - uppercase city\n      - type: transformValue\n        mapper: uppercase_city\n\n      # Log the transformed CSV data\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Transformed: sensor={}, city={}\", value.get(\"name\"), value.get(\"city\"))\n\n    to: csv_output\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-xml-data","title":"Working with XML Data","text":"<p>XML (eXtensible Markup Language) is a structured format for representing hierarchical data with custom tags and attributes.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements_2","title":"Setup Requirements","text":"<p>XML data processing does not require a schema registry. Make sure there is a running Docker Compose KSML environment as described in the Quick Start.</p> <ul> <li>XML data is represented as nested elements with opening and closing tags</li> <li>Elements can contain text content, attributes, and child elements</li> <li>XSD (XML Schema Definition) defines the structure, data types, and constraints for XML documents</li> </ul>"},{"location":"tutorials/beginner/data-formats/#requirements-for-running-ksml-xml-definitions","title":"Requirements for running KSML XML definitions","text":"<p>To run KSML XML processing definitions below, please follow these steps:</p> <ul> <li>Save this <code>examples/SensorData.xsd</code> as XML schema:</li> </ul> XSD schema (click to expand) <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;\n    &lt;xs:element name=\"SensorData\"&gt;\n        &lt;xs:complexType&gt;\n            &lt;xs:sequence&gt;\n                &lt;xs:element name=\"name\" type=\"xs:string\"/&gt;\n                &lt;xs:element name=\"timestamp\" type=\"xs:long\"/&gt;\n                &lt;xs:element name=\"value\" type=\"xs:string\"/&gt;\n                &lt;xs:element name=\"type\" type=\"xs:string\"/&gt;\n                &lt;xs:element name=\"unit\" type=\"xs:string\"/&gt;\n                &lt;xs:element name=\"color\" type=\"xs:string\" minOccurs=\"0\"/&gt;\n                &lt;xs:element name=\"city\" type=\"xs:string\" minOccurs=\"0\"/&gt;\n                &lt;xs:element name=\"owner\" type=\"xs:string\" minOccurs=\"0\"/&gt;\n            &lt;/xs:sequence&gt;\n        &lt;/xs:complexType&gt;\n    &lt;/xs:element&gt;\n&lt;/xs:schema&gt;\n</code></pre> <ul> <li>Use this <code>examples/producer.yaml</code> that produces XML messages that our processing definition below can work with:</li> </ul> Producer definition for XML messages (click to expand) <pre><code>functions:\n  generate_sensordata_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate data matching SensorData.xsd schema\n      value = {\n        \"name\": key,\n        \"timestamp\": round(time.time()*1000),     # timestamp as long, not string\n        \"value\": str(random.randrange(0, 100)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"PRESSURE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"])\n      }\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Generate as JSON, KSML will convert to XML\n\nproducers:\n  # Produce XML sensor data messages every 3 seconds\n  sensordata_xml_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: ksml_sensordata_xml\n      keyType: string\n      valueType: xml:SensorData                   # KSML will convert JSON to XML format\n</code></pre> <p>This processor demonstrates XML data manipulation (uppercases city names) while maintaining XML format:</p> Processor definition for XML messages (click to expand) <pre><code>streams:\n  xml_input:\n    topic: ksml_sensordata_xml\n    keyType: string\n    valueType: xml:SensorData\n    offsetResetPolicy: latest\n\n  xml_output:\n    topic: ksml_sensordata_xml_processed\n    keyType: string\n    valueType: xml:SensorData\n\nfunctions:\n  uppercase_city:\n    type: valueTransformer\n    code: |\n      # When using xml:SensorData, the data comes as a structured object (dict)\n      if value and isinstance(value, dict):\n        # Create a copy and uppercase the city\n        enriched = dict(value)\n        if \"city\" in enriched:\n          enriched[\"city\"] = enriched[\"city\"].upper()\n        result = enriched\n      else:\n        result = value\n    expression: result\n    resultType: xml:SensorData\n\npipelines:\n  process_xml:\n    from: xml_input\n    via:\n      # Log the original XML data\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Original: sensor={}, city={}\", value.get(\"name\"), value.get(\"city\"))\n\n      # Transform the XML data - uppercase city\n      - type: transformValue\n        mapper: uppercase_city\n\n      # Log the transformed XML data\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              log.info(\"Transformed: sensor={}, city={}\", value.get(\"name\"), value.get(\"city\"))\n\n    to: xml_output\n</code></pre> <ul> <li>Please note that <code>kowl</code> is not capable of deserializing XML messages and will display the value of the messages as blank.</li> <li>To read the XML messages use kcat:</li> </ul> <pre><code>kcat -b localhost:9092 -t ksml_sensordata_xml -C -o -1 -c 1 -f '%s\\n' | xmllint --format -\n</code></pre> XML message example (click to expand) <pre><code>&lt;?xml version=\"1.1\" encoding=\"UTF-8\"?&gt;\n&lt;SensorData&gt;\n  &lt;city&gt;Rotterdam&lt;/city&gt;\n  &lt;color&gt;black&lt;/color&gt;\n  &lt;name&gt;sensor6&lt;/name&gt;\n  &lt;owner&gt;Alice&lt;/owner&gt;\n  &lt;timestamp&gt;1754376106863&lt;/timestamp&gt;\n  &lt;type&gt;TEMPERATURE&lt;/type&gt;\n  &lt;unit&gt;%&lt;/unit&gt;\n  &lt;value&gt;12&lt;/value&gt;\n&lt;/SensorData&gt;\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-binary-data","title":"Working with Binary Data","text":"<p>Binary data represents raw bytes as sequences of numeric values ranging from 0 to 255, ideal for handling non-text content like images, files, or custom protocols.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements_3","title":"Setup Requirements","text":"<p>Binary data processing does not require a schema registry. Make sure there is a running Docker Compose KSML environment as described in the Quick Start.</p> <ul> <li>Binary data is represented as arrays of integers where each value corresponds to a single byte</li> <li>Each byte can store values from 0-255, allowing for compact encoding of various data types</li> <li>Binary processing enables direct byte manipulation, bit-level operations, and efficient handling of structured binary formats</li> </ul> <p>This producer creates simple binary messages as byte arrays (7-byte messages with counter, random bytes, and ASCII \"KSML\"):</p> Producer definition for Binary messages (click to expand) <pre><code>functions:\n  generate_binary_message:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n\n      key = \"msg\" + str(counter)\n      counter = (counter + 1) % 100\n\n      # Create simple binary message as list of bytes\n      value = [\n        counter % 256,                    # Counter byte\n        random.randrange(0, 256),         # Random byte\n        ord('K'),                         # ASCII 'K'\n        ord('S'),                         # ASCII 'S'\n        ord('M'),                         # ASCII 'M'\n        ord('L'),                         # ASCII 'L'\n        random.randrange(0, 256)          # Another random byte\n      ]\n\n      log.info(\"Generated binary message: key={}, bytes={}\", key, value)\n    expression: (key, value)\n    resultType: (string, bytes)\n\nproducers:\n  binary_producer:\n    generator: generate_binary_message\n    interval: 3s\n    to:\n      topic: ksml_sensordata_binary\n      keyType: string\n      valueType: bytes\n</code></pre> <p>This processor demonstrates binary data manipulation (increments first byte) while maintaining binary format:</p> Processor definition for Binary messages (click to expand) <pre><code>streams:\n  binary_input:\n    topic: ksml_sensordata_binary\n    keyType: string\n    valueType: bytes\n    offsetResetPolicy: latest\n\n  binary_output:\n    topic: ksml_sensordata_binary_processed\n    keyType: string\n    valueType: bytes\n\nfunctions:\n  increment_first_byte:\n    type: valueTransformer\n    code: |\n      # Simple binary manipulation: increment the first data byte\n      if isinstance(value, list) and len(value) &gt; 0:\n        modified = list(value)\n        modified[0] = (modified[0] + 1) % 256  # Increment and wrap at 256\n        result = modified\n      else:\n        result = value\n    expression: result\n    resultType: bytes\n\npipelines:\n  process_binary:\n    from: binary_input\n    via:\n      # Log input binary\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Binary input: key={}, bytes={}\", key, value)\n\n      # Modify binary data\n      - type: transformValue\n        mapper: increment_first_byte\n\n      # Log output binary\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Binary output: key={}, bytes={}\", key, value)\n\n    to: binary_output\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#working-with-soap-data","title":"Working with SOAP Data","text":"<p>SOAP provides structured web service messaging with envelope/body format and no WSDL requirements.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements_4","title":"Setup Requirements","text":"<p>SOAP data processing does not require a schema registry. Make sure there is a running Docker Compose KSML environment as described in the Quick Start.</p> <p>This producer creates SOAP request messages with envelope/body structure (no WSDL files required):</p> Producer definition for SOAP messages (click to expand) <pre><code>functions:\n  generate_soap_message:\n    type: generator\n    globalCode: |\n      counter = 0\n    code: |\n      global counter\n\n      key = \"msg\" + str(counter)\n      counter = (counter + 1) % 100\n\n      # Create simple SOAP message\n      value = {\n        \"envelope\": {\n          \"body\": {\n            \"elements\": [\n              {\n                \"name\": {\n                  \"localPart\": \"SensorRequest\",\n                  \"namespaceURI\": \"http://example.com/ksml\"\n                },\n                \"value\": {\n                  \"id\": str(counter),\n                  \"type\": \"temperature\"\n                }\n              }\n            ]\n          }\n        }\n      }\n\n      log.info(\"Generated SOAP message: {}\", key)\n    expression: (key, value)\n    resultType: (string, soap)\n\nproducers:\n  soap_producer:\n    generator: generate_soap_message\n    interval: 3s\n    to:\n      topic: ksml_soap_requests\n      keyType: string\n      valueType: soap\n</code></pre> <p>This processor transforms SOAP requests into SOAP responses (extracts request data and creates response with sensor values):</p> Processor definition for SOAP messages (click to expand) <pre><code>streams:\n  soap_input:\n    topic: ksml_soap_requests\n    keyType: string\n    valueType: soap\n    offsetResetPolicy: latest\n\n  soap_output:\n    topic: ksml_soap_responses\n    keyType: string\n    valueType: soap\n\nfunctions:\n  create_response:\n    type: valueTransformer\n    code: |\n      # Extract request data\n      request_elements = value.get(\"envelope\", {}).get(\"body\", {}).get(\"elements\", [])\n      request_data = request_elements[0].get(\"value\", {}) if request_elements else {}\n\n      # Create SOAP response\n      result = {\n        \"envelope\": {\n          \"body\": {\n            \"elements\": [\n              {\n                \"name\": {\n                  \"localPart\": \"SensorResponse\",\n                  \"namespaceURI\": \"http://example.com/ksml\"\n                },\n                \"value\": {\n                  \"id\": request_data.get(\"id\", \"unknown\"),\n                  \"temperature\": \"25\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    expression: result\n    resultType: soap\n\npipelines:\n  process_soap:\n    from: soap_input\n    via:\n      # Log input\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"SOAP request: key={}\", key)\n\n      # Transform to response\n      - type: transformValue\n        mapper: create_response\n\n      # Log output\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"SOAP response: key={}\", key)\n\n    to: soap_output\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#converting-between-data-formats","title":"Converting Between Data Formats","text":"<p>Use the <code>convertValue</code> operation to transform data between formats within a single pipeline.</p>"},{"location":"tutorials/beginner/data-formats/#setup-requirements_5","title":"Setup Requirements","text":"<p>This example converts Avro messages, which requires a schema registry. Use the same Docker Compose setup as described in the Working with Avro Data section above.</p> <p>This producer generates Avro messages for format conversion demonstrations:</p> Producer definition (click to expand) <pre><code>functions:\n  generate_sensordata_message:\n    type: generator\n    globalCode: |\n      import time\n      import random\n      sensorCounter = 0\n    code: |\n      global sensorCounter\n\n      key = \"sensor\"+str(sensorCounter)           # Set the key to return (\"sensor0\" to \"sensor9\")\n      sensorCounter = (sensorCounter+1) % 10      # Increase the counter for next iteration\n\n      # Generate data matching SensorData schema\n      value = {\n        \"name\": key,\n        \"timestamp\": str(round(time.time()*1000)),\n        \"value\": str(random.randrange(0, 100)),\n        \"type\": random.choice([\"TEMPERATURE\", \"HUMIDITY\", \"PRESSURE\"]),\n        \"unit\": random.choice([\"C\", \"F\", \"%\", \"Pa\"]),\n        \"color\": random.choice([\"black\", \"blue\", \"red\", \"yellow\", \"white\"]),\n        \"owner\": random.choice([\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\"]),\n        \"city\": random.choice([\"Amsterdam\", \"Utrecht\", \"Rotterdam\", \"The Hague\", \"Eindhoven\"])\n      }\n\n      # Occasionally send null messages for testing\n      if random.randrange(20) == 0:\n        value = None\n    expression: (key, value)                      # Return a message tuple with the key and value\n    resultType: (string, json)                    # Generate as JSON, KSML will convert to Avro\n\nproducers:\n  # Produce Avro sensor data messages every 3 seconds\n  sensordata_avro_producer:\n    generator: generate_sensordata_message\n    interval: 3s\n    to:\n      topic: sensor_data_avro\n      keyType: string\n      valueType: avro:SensorData\n</code></pre> <p>This processor demonstrates multiple format conversions (Avro \u2192 JSON \u2192 String \u2192 JSON) using <code>convertValue</code>:</p> Processing definition for converting between multiple formats (click to expand) <pre><code>streams:\n  avro_input:\n    topic: sensor_data_avro\n    keyType: string\n    valueType: avro:SensorData\n    offsetResetPolicy: latest\n\n  json_output:\n    topic: sensor_data_converted_formats\n    keyType: string\n    valueType: json\n\npipelines:\n  format_conversion:\n    from: avro_input\n    via:\n      # Log original Avro data\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Original Avro: sensor={}, type={}, value={}{}\",\n                      value.get(\"name\"), value.get(\"type\"), value.get(\"value\"), value.get(\"unit\"))\n            else:\n              log.info(\"Received null Avro message with key={}\", key)\n\n      # Convert from Avro to JSON\n      - type: convertValue\n        into: json\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Converted to JSON: sensor={}, city={}\", value.get(\"name\"), value.get(\"city\"))\n            else:\n              log.info(\"JSON conversion result: null\")\n\n      # Convert from JSON to string to see serialized representation\n      - type: convertValue\n        into: string\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"As string (first 100 chars): {}\", str(value)[:100])\n            else:\n              log.info(\"String conversion result: null\")\n\n      # Convert back to JSON for output\n      - type: convertValue\n        into: json\n      - type: peek\n        forEach:\n          code: |\n            if value is not None:\n              log.info(\"Final JSON output: sensor={}, owner={}\", value.get(\"name\"), value.get(\"owner\"))\n            else:\n              log.info(\"Final conversion result: null\")\n\n    to: json_output\n</code></pre>"},{"location":"tutorials/beginner/data-formats/#format-conversion-and-multiple-formats","title":"Format Conversion and Multiple Formats","text":"<p>For comprehensive information on format conversion requirements, chaining conversions, and working with multiple formats in a single pipeline, see the Data Types and Formats Reference - Type Conversion section.</p>"},{"location":"tutorials/beginner/data-formats/#conclusion","title":"Conclusion","text":"<p>You've learned to work with KSML's data formats through practical examples: JSON, Avro, CSV, XML, Binary, and SOAP. Key concepts covered include format specification, schema usage, conversion operations, and multi-format pipelines.</p> <p>For complete syntax reference, type definitions, and advanced format features, refer to the Data Types and Formats Reference.</p>"},{"location":"tutorials/beginner/data-formats/#next-steps","title":"Next Steps","text":"<ul> <li>Logging and Monitoring for adding effective logging to pipelines</li> <li>Intermediate Tutorials for advanced KSML features</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/","title":"Filtering and Transforming Data in KSML","text":""},{"location":"tutorials/beginner/filtering-transforming/#what-well-build","title":"What We'll Build","text":"<p>In this tutorial, we'll build a data pipeline that:</p> <ol> <li>Reads sensor data from a Kafka topic</li> <li>Applies complex filtering based on multiple conditions</li> <li>Transforms the data using various techniques and by handling nested data structures</li> <li>Handles potential errors in the transformation process</li> <li>Writes the processed data to another Kafka topic</li> </ol>"},{"location":"tutorials/beginner/filtering-transforming/#prerequisites","title":"Prerequisites","text":"<p>Before we begin:</p> <ul> <li>Make sure there is a running Docker Compose KSML environment as described in the Quick Start. </li> <li>We recommend to have completed the KSML Basics Tutorial</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code># Filtering and Transforming Tutorial\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic tutorial_input &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic filtered_data &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic alerts_stream &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/beginner/filtering-transforming/#to-try-out-each-example","title":"To try out each example","text":"<ol> <li>Make sure to update the definitions section in <code>ksml-runner.yaml</code>(the full file in is in the Basics Tutorial: <pre><code>ksml:\n  definitions:\n    producer: producer.yaml\n    processor: processor.yaml\n</code></pre></li> <li>When making changes in either file (<code>producer.yaml</code> or <code>processor.yaml</code>), reload KSML producer and processing definitions:<ul> <li><code>docker compose restart ksml &amp;&amp; docker compose logs ksml -f</code> (slower due to Kafka Streams rebalancing)</li> <li><code>docker compose down &amp;&amp; docker compose up -d &amp;&amp; docker compose logs ksml -f</code> (faster, but topics will be empty again due to Kafka broker restart)</li> </ul> </li> </ol>"},{"location":"tutorials/beginner/filtering-transforming/#creating-test-data","title":"Creating test data","text":"<p>To let KSML produce random test data with the correct format, let's create a file <code>producer.yaml</code> and add this producer definition</p> Test Data Producer Configuration (click to expand) <pre><code>functions:\n  generate_tutorial_data:\n    type: generator\n    globalCode: |\n      import random\n      sensor_id = 0\n      locations = [\"server_room\", \"warehouse\", \"data_center\"]\n    code: |\n      global sensor_id, locations\n      key = \"sensor\" + str(sensor_id)\n      sensor_id = (sensor_id + 1) % 5\n      location = random.choice(locations)\n      sensors = {\"temperature\": random.randrange(150), \"humidity\": random.randrange(90), \"location\": location}\n      value = {\"sensors\": sensors}\n    expression: (key, value)\n    resultType: (string, json)\nproducers:\n  data_producer:\n    generator: generate_tutorial_data\n    interval: 3s\n    to:\n      topic: tutorial_input\n      keyType: string\n      valueType: json\n</code></pre> <p>This will generate simulated sensor data for temperature and humidity, in different locations. The JSON input test data, that we will start from with our filtering and transformations, looks like this: <pre><code>{\n  \"sensors\": {\n    \"humidity\": 53,\n    \"location\": \"server_room\",\n    \"temperature\": 143\n  }\n}\n</code></pre></p>"},{"location":"tutorials/beginner/filtering-transforming/#complex-filtering-techniques","title":"Complex Filtering Techniques","text":""},{"location":"tutorials/beginner/filtering-transforming/#using-multiple-filters","title":"Using Multiple Filters","text":"<p>Let's start by creating a file <code>processor.yaml</code> that filters on multiple conditions:</p> Multiple Filter Conditions Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  temperature_filtered:\n    type: predicate\n    expression: value.get('sensors', {}).get('temperature') &gt; 20 and value.get('sensors', {}).get('humidity') &lt; 80 and value.get('sensors', {}).get('location') == 'warehouse'\n  log_message:\n    type: forEach\n    code: |\n      log.info(\"Processed message: key={}, value={}\", key, value)\n\npipelines:\n  filtering_pipeline:\n    from: input_stream\n    via:\n      - type: filter\n        if: temperature_filtered\n      - type: peek\n        forEach:\n          code: |\n            log_message(key, value)\n    to: output_stream\n</code></pre> <p>This filter only passes messages where:</p> <ul> <li>The temperature is greater than 20\u00b0F</li> <li>The humidity is less than 80%</li> <li>The location is 'warehouse'</li> </ul> <p>Now let's update the definitions section in <code>ksml-runner.yaml</code>:</p> KSML Runner Configuration Update (click to expand) <pre><code>ksml:\n  definitions:\n     producer: producer.yaml\n     processor: processor.yaml\n</code></pre> <ul> <li>Let's test by doing: <pre><code>docker compose restart ksml &amp;&amp; docker compose logs ksml -f\n</code></pre></li> <li>Here are the input messages: http://localhost:8080/topics/tutorial_input</li> <li>Here are the filtered messages: http://localhost:8080/topics/filtered_data</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#using-custom-filter-functions","title":"Using Custom Filter Functions","text":"<p>By following the same as in previous section, let's try to create a custom filter function:</p> Custom Filter Function Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  alerts_stream:\n    topic: alerts_stream\n    keyType: string\n    valueType: json\n\nfunctions:\n  is_critical_sensor:\n    type: predicate\n    code: |\n      # Check location\n      if value.get('sensors', {}).get('location') not in ['server_room', 'data_center']:\n        return False\n\n      # Check temperature threshold based on location\n      if value.get('sensors', {}).get('location') == 'server_room' and value.get('sensors', {}).get('temperature') &gt; 20:\n        return True\n      if value.get('sensors', {}).get('location') == 'data_center' and value.get('sensors', {}).get('temperature') &gt; 30:\n        return True\n\n      return False\n\npipelines:\n  critical_alerts:\n    from: input_stream\n    via:\n      - type: filter\n        if: is_critical_sensor\n    to: alerts_stream\n</code></pre> <p>This function implements complex business logic to determine if a sensor reading indicates a critical situation that requires an alert.</p>"},{"location":"tutorials/beginner/filtering-transforming/#filtering-with-error-handling","title":"Filtering with Error Handling","text":"<p>Sometimes your filter conditions might encounter malformed data. Here's how to handle that:</p> Error Handling in Filters Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  alerts_stream:\n    topic: alerts_stream\n    keyType: string\n    valueType: json\n\nfunctions:\n  safe_filter:\n    type: predicate\n    code: |\n      try:\n        sensors = value.get('sensors', {})\n        temperature = sensors.get('temperature')\n        humidity = sensors.get('humidity')\n\n        if temperature is None or humidity is None:\n          log.warn(\"Missing required fields in message: {}\", value)\n          return False\n\n        return temperature &gt; 70 and humidity &lt; 50\n      except Exception as e:\n        log.error(\"Error in filter: {} - Message: {}\", str(e), value)\n        return False\n\npipelines:\n  robust_filtering:\n    from: input_stream\n    via:\n      - type: filter\n        if: safe_filter\n    to: alerts_stream\n</code></pre> <p>This approach ensures that malformed messages are logged and filtered out rather than causing the pipeline to fail. This will not throw errors currently, to check that errors are correctly logged, change the key to something that doesn't exist, for example: <pre><code>sensors = value.get('sensors2', {})\n</code></pre></p>"},{"location":"tutorials/beginner/filtering-transforming/#advanced-transformation-techniques","title":"Advanced Transformation Techniques","text":"<p>For these examples, let's use a different KSML producer definition:</p> Enhanced Producer Configuration (click to expand) <pre><code>producers:\n  data_producer:\n    generator: generate_tutorial_data\n    interval: 3s\n    to:\n      topic: tutorial_input\n      keyType: string\n      valueType: json\nfunctions:\n  generate_tutorial_data:\n    type: generator\n    globalCode: |\n      import random, time\n      sensor_id = 0\n      locations = [\"server_room\", \"warehouse\", \"data_center\"]\n    code: |\n      global sensor_id, locations\n      key = \"sensor\" + str(sensor_id)\n      sensor_id = (sensor_id + 1) % 5\n      location = random.choice(locations)\n\n      # Each sensor value is now a dict with 'value' and 'unit'\n      sensors = {\n        \"temperature\": {\n          \"value\": random.randint(60, 100),\n          \"unit\": \"F\"\n        },\n        \"humidity\": {\n          \"value\": random.randint(20, 90),\n          \"unit\": \"%\"\n        },\n        \"location\": {\n          \"value\": location,\n          \"unit\": \"text\"\n        }\n      }\n\n      # Add a timestamp in the expected metadata format\n      value = {\n        \"metadata\": {\n          \"timestamp\": int(time.time() * 1000)\n        },\n        \"sensors\": sensors\n      }\n    expression: (key, value)\n    resultType: (string, json)\n</code></pre> <p>This produces messages like these: INPUT message:</p> <ul> <li>key: sensor0</li> <li>value: <pre><code>{\n  \"metadata\": {\n    \"timestamp\": 1753935622755\n  },\n  \"sensors\": {\n    \"humidity\": {\n      \"unit\": \"%\",\n      \"value\": 34\n    },\n    \"location\": {\n      \"unit\": \"text\",\n      \"value\": \"data_center\"\n    },\n    \"temperature\": {\n      \"unit\": \"F\",\n      \"value\": 80\n    }\n  }\n}\n</code></pre></li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#transforming-nested-data-structures","title":"Transforming Nested Data Structures","text":"<p>Let's look at how to transform data with nested structures:</p> Nested Data Transformation Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  transform_nested_data:\n    type: keyValueMapper\n    code: |\n      # Create a new structure with flattened and transformed data\n      result = {\n        \"device_id\": key,\n        \"timestamp\": value.get('metadata', {}).get('timestamp'),\n        \"readings\": {}\n      }\n\n      # Extract and transform sensor readings\n      sensors = value.get('sensors', {})\n      for sensor_type, reading in sensors.items():\n        # Convert temperature from F to C if needed\n        if sensor_type == 'temperature' and reading.get('unit') == 'F':\n          celsius = (reading.get('value') - 32) * 5/9\n          result['readings'][sensor_type] = {\n            'value': round(celsius, 2),\n            'unit': 'C',\n            'original_value': reading.get('value'),\n            'original_unit': 'F'\n          }\n        else:\n          result['readings'][sensor_type] = reading\n\n      # Keep the same key\n      new_key = key\n      new_value = result\n    expression: (new_key, new_value)\n    resultType: (string, json)\n\npipelines:\n  transform_pipeline:\n    from: input_stream\n    via:\n      - type: map\n        mapper: transform_nested_data\n    to: output_stream\n</code></pre> <p>INPUT message:</p> <ul> <li>key: sensor0</li> <li>value:  <pre><code>{\n  \"metadata\": {\n    \"timestamp\": 1753935622755\n  },\n  \"sensors\": {\n    \"humidity\": {\n      \"unit\": \"%\",\n      \"value\": 34\n    },\n    \"location\": {\n      \"unit\": \"text\",\n      \"value\": \"data_center\"\n    },\n    \"temperature\": {\n      \"unit\": \"F\",\n      \"value\": 80\n    }\n  }\n}\n</code></pre></li> </ul> <p>OUTPUT message:</p> <ul> <li>key: sensor0</li> <li>value: <pre><code>  {\n      \"device_id\": \"sensor0\",\n      \"readings\": {\n          \"humidity\": {\n              \"unit\": \"%\",\n              \"value\": 34\n          },\n          \"location\": {\n              \"unit\": \"text\",\n              \"value\": \"data_center\"\n          },\n          \"temperature\": {\n              \"original_unit\": \"F\",\n              \"original_value\": 80,\n              \"unit\": \"C\",\n              \"value\": 26.67\n          }\n      },\n      \"timestamp\": 1753935622755\n  }\n</code></pre></li> </ul> <p>This transformation performs several operations on the incoming sensor data:</p> <ol> <li>The nested <code>metadata.timestamp</code> is extracted and placed at the root level of the output</li> <li>The message key (sensor0) is added to the value as <code>device_id</code>, making the device identifier available in the message body</li> <li>Temperature readings in Fahrenheit are automatically converted to Celsius using the formula (F - 32) \u00d7 5/9</li> <li>When converting temperature, both the original and converted values are retained for audit purposes</li> <li>Only temperature sensors with Fahrenheit units are converted; all other sensor types (<code>humidity</code>, <code>location</code>) pass through unchanged</li> </ol> <p>The transformation maintains the original key while restructuring the value to be more suitable for downstream processing, with standardized temperature units and flattened metadata.</p>"},{"location":"tutorials/beginner/filtering-transforming/#applying-multiple-transformations","title":"Applying Multiple Transformations","text":"<p>You can chain multiple transformations to break down complex logic into manageable steps:</p> Multiple Transformations Pipeline Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n\nfunctions:\n  extract_fields:\n    type: keyValueMapper\n    code: |\n      extracted = {\n        \"device_id\": key,\n        \"temperature\": value.get('sensors', {}).get('temperature', {}).get('value'),\n        \"humidity\": value.get('sensors', {}).get('humidity', {}).get('value'),\n        \"timestamp\": value.get('metadata', {}).get('timestamp')\n      }\n    expression: (key, extracted)\n    resultType: (string, json)\n\n  convert_temperature:\n    type: valueTransformer\n    code: |\n      result = {\n        \"device_id\": value.get('device_id'),\n        \"temperature_c\": round((value.get('temperature') - 32) * 5/9, 2) if value.get('temperature') else None,\n        \"humidity\": value.get('humidity'),\n        \"timestamp\": value.get('timestamp')\n      }\n    expression: result\n    resultType: json\n\n  add_heat_index:\n    type: valueTransformer\n    code: |\n      temp_c = value.get('temperature_c')\n      humidity = value.get('humidity')\n\n      # Calculate heat index if we have both temperature and humidity\n      if temp_c is not None and humidity is not None:\n        # Convert back to F for heat index calculation\n        temp_f = temp_c * 1.8 + 32\n        # Simplified heat index formula\n        heat_index = temp_f - 0.55 * (1 - humidity / 100) * (temp_f - 58)\n        heat_index_c = round((heat_index - 32) * 5/9, 2)\n      else:\n        heat_index_c = None\n\n      result = {\n        \"device_id\": value.get('device_id'),\n        \"temperature_c\": temp_c,\n        \"humidity\": humidity,\n        \"heat_index_c\": heat_index_c,\n        \"timestamp\": value.get('timestamp')\n      }\n    expression: result\n    resultType: json\n\npipelines:\n  multi_transform_pipeline:\n    from: input_stream\n    via:\n      # Step 1: Extract relevant fields\n      - type: map\n        mapper: extract_fields\n\n      # Step 2: Convert temperature from F to C\n      - type: transformValue\n        mapper: convert_temperature\n\n      # Step 3: Add calculated fields\n      - type: transformValue\n        mapper: add_heat_index\n    to: output_stream\n</code></pre> <p>INPUT message:</p> <ul> <li>key: sensor0</li> <li>value: <pre><code>  {\n  \"metadata\": {\n    \"timestamp\": 1753937101388\n  },\n  \"sensors\": {\n    \"humidity\": {\n      \"unit\": \"%\",\n      \"value\": 61\n    },\n    \"location\": {\n      \"unit\": \"text\",\n      \"value\": \"server_room\"\n    },\n    \"temperature\": {\n      \"unit\": \"F\",\n      \"value\": 87\n    }\n  }\n}\n</code></pre></li> </ul> <p>OUTPUT message:</p> <ul> <li>key: sensor0</li> <li>value: <pre><code>{\n  \"device_id\": \"sensor0\",\n  \"heat_index_c\": 27.1,\n  \"humidity\": 61,\n  \"temperature_c\": 30.56,\n  \"timestamp\": 1753937101388\n}\n</code></pre></li> </ul> <p>This pipeline demonstrates a three-stage transformation process:</p> <p>Stage 1: Field Extraction (<code>extract_fields</code>)</p> <ul> <li>Input: Full nested sensor data with metadata and location information</li> <li>Process: Extracts only the essential fields (temperature, humidity, timestamp) and adds the device ID from the message key</li> <li>Output: Simplified structure with just the needed values</li> <li>Note: The location field is intentionally dropped as it's not needed for calculations</li> </ul> <p>Stage 2: Temperature Conversion (<code>convert_temperature</code>)</p> <ul> <li>Input: Extracted data with temperature in Fahrenheit (87\u00b0F)</li> <li>Process: Converts temperature from Fahrenheit to Celsius using the formula (F - 32) \u00d7 5/9</li> <li>Output: Same structure but with temperature_c field containing 30.56\u00b0C</li> <li>Note: Original temperature field is removed, replaced with the converted value</li> </ul> <p>Stage 3: Heat Index Calculation (<code>add_heat_index</code>)</p> <ul> <li>Input: Data with temperature in Celsius and humidity percentage</li> <li>Process: Calculates the heat index (apparent temperature) considering both temperature and humidity</li> <li>Output: Final structure with added heat_index_c field showing 27.1\u00b0C</li> <li>Note: The heat index is lower than actual temperature due to 61% humidity</li> </ul> <p>The transformation reduces the original nested structure from 5 fields across multiple levels to a flat structure with 5 essential fields, while also performing unit conversion and derived calculations. Breaking transformations into steps makes your pipeline easier to understand and maintain.</p>"},{"location":"tutorials/beginner/filtering-transforming/#error-handling-in-transformations","title":"Error Handling in Transformations","text":"<p>This tutorial demonstrates how to implement robust error handling in KSML transformations, ensuring your data pipelines can gracefully handle unexpected data formats and processing errors.</p>"},{"location":"tutorials/beginner/filtering-transforming/#overview","title":"Overview","text":"<p>When processing streaming data, it's crucial to handle errors gracefully without crashing the entire pipeline. This example shows how to:</p> <ul> <li>Safely extract nested data with validation</li> <li>Route successful and failed transformations to different topics</li> <li>Preserve error context for debugging</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#ksml-definition","title":"KSML Definition","text":"Error Handling in Transformations Example (click to expand) <pre><code>streams:\n  input_stream:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  output_stream:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n  error_stream:\n    topic: alerts_stream\n    keyType: string\n    valueType: json\n\nfunctions:\n  safe_transform:\n    type: keyValueMapper\n    code: |\n      import json\n      try:\n        # Safely extract nested sensor data\n        sensors = value.get('sensors', {})\n        temperature_data = sensors.get('temperature', {})\n\n        # Check if temperature exists and has a value\n        if not temperature_data or 'value' not in temperature_data:\n          error_msg = {\n            \"error\": \"Missing temperature data\",\n            \"device_id\": key,\n            \"original\": value,\n            \"status\": \"error\"\n          }\n          new_key = key\n          new_value = error_msg\n        else:\n          # Extract values safely\n          temp_f = temperature_data.get('value')\n          temp_unit = temperature_data.get('unit', 'F')\n\n          # Only convert if unit is Fahrenheit\n          if temp_unit == 'F':\n            temp_c = round((temp_f - 32) * 5/9, 2)\n          else:\n            temp_c = temp_f  # Assume it's already in Celsius\n\n          # Build successful result\n          result = {\n            \"device_id\": key,\n            \"temperature_f\": temp_f,\n            \"temperature_c\": temp_c,\n            \"humidity\": sensors.get('humidity', {}).get('value'),\n            \"timestamp\": value.get('metadata', {}).get('timestamp'),\n            \"status\": \"processed\"\n          }\n          new_key = key\n          new_value = result\n\n      except Exception as e:\n        # Catch any unexpected errors\n        error_msg = {\n          \"error\": f\"Transformation error: {str(e)}\",\n          \"device_id\": key,\n          \"original\": value,\n          \"status\": \"error\"\n        }\n        new_key = key\n        new_value = error_msg\n    expression: (new_key, new_value)\n    resultType: (string, json)\n\npipelines:\n  robust_transformation:\n    from: input_stream\n    via:\n      - type: map\n        mapper: safe_transform\n    branch:\n      - if:\n          expression: value.get('status') == 'processed'\n        to: output_stream\n      - if:\n          expression: value.get('status') == 'error'\n        to: error_stream\n</code></pre>"},{"location":"tutorials/beginner/filtering-transforming/#example-data-flow","title":"Example Data Flow","text":""},{"location":"tutorials/beginner/filtering-transforming/#successful-case","title":"Successful Case","text":"<p>INPUT message:</p> <ul> <li>key: <code>sensor0</code></li> <li>value:</li> </ul> <pre><code>{\n    \"metadata\": {\n        \"timestamp\": 1753939130968\n    },\n    \"sensors\": {\n        \"humidity\": {\n            \"unit\": \"%\",\n            \"value\": 71\n        },\n        \"location\": {\n            \"unit\": \"text\",\n            \"value\": \"server_room\"\n        },\n        \"temperature\": {\n            \"unit\": \"F\",\n            \"value\": 65\n        }\n    }\n}\n</code></pre> <p>OUTPUT message (to <code>filtered_data</code> topic):</p> <ul> <li>key: <code>sensor0</code></li> <li>value: <pre><code>{\n    \"device_id\": \"sensor0\",\n    \"humidity\": 71,\n    \"status\": \"processed\",\n    \"temperature_c\": 18.33,\n    \"temperature_f\": 65,\n    \"timestamp\": 1753939130968\n}\n</code></pre></li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#error-case-missing-temperature","title":"Error Case - Missing Temperature","text":"<p>INPUT message:</p> <ul> <li>key: <code>sensor1</code></li> <li>value: <pre><code>{\n    \"metadata\": {\n        \"timestamp\": 1753939130968\n    },\n    \"sensors\": {\n        \"humidity\": {\n            \"unit\": \"%\",\n            \"value\": 71\n        }\n    }\n}\n</code></pre></li> </ul> <p>OUTPUT message (to <code>alerts_stream</code> topic):</p> <ul> <li>key: <code>sensor1</code></li> <li>value: <pre><code>{\n    \"error\": \"Missing temperature data\",\n    \"device_id\": \"sensor1\",\n    \"original\": {\n        \"metadata\": {\"timestamp\": 1753939130968},\n        \"sensors\": {\"humidity\": {\"unit\": \"%\", \"value\": 71}}\n    },\n    \"status\": \"error\"\n}\n</code></pre></li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#key-features","title":"Key Features","text":""},{"location":"tutorials/beginner/filtering-transforming/#safe-data-processing","title":"Safe Data Processing","text":"<ul> <li>Uses Python's try-except blocks to catch any unexpected errors during transformation</li> <li>Validates data existence before attempting to access nested fields</li> <li>Preserves the original message in error cases for debugging</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#conditional-routing","title":"Conditional Routing","text":"<ul> <li>Successfully processed messages (with <code>status: \"processed\"</code>) are routed to the <code>filtered_data</code> topic</li> <li>Error messages (with <code>status: \"error\"</code>) are sent to the <code>alerts_stream</code> topic for monitoring</li> <li>The branching logic ensures clean separation of successful and failed transformations</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#data-transformation","title":"Data Transformation","text":"<ul> <li>Extracts sensor data from the nested structure</li> <li>Converts temperature from Fahrenheit (65\u00b0F) to Celsius (18.33\u00b0C)</li> <li>Flattens the structure while preserving essential fields</li> <li>Drops the location field as it's not needed in the output</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#error-context","title":"Error Context","text":"<ul> <li>Error messages include the device ID for traceability</li> <li>The original message is preserved in error cases</li> <li>Specific error messages help identify the type of failure (missing data vs. processing error)</li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#benefits","title":"Benefits","text":"<ol> <li>The pipeline continues processing even when encountering malformed data</li> <li>Errors are routed to a dedicated topic for monitoring and alerting</li> <li>Original messages are preserved in error cases for investigation</li> <li>Only valid, successfully processed data reaches the output topic</li> </ol> <p>This approach ensures that transformation errors are caught, logged, and handled gracefully without crashing the pipeline, while maintaining full visibility into what went wrong through the separate error stream.</p>"},{"location":"tutorials/beginner/filtering-transforming/#combining-filtering-and-transformation","title":"Combining Filtering and Transformation","text":"<p>Let's put everything together in a complete example:</p> Complete Filtering and Transformation Pipeline (click to expand) <pre><code>streams:\n  sensor_data:\n    topic: tutorial_input\n    keyType: string\n    valueType: json\n  processed_data:\n    topic: filtered_data\n    keyType: string\n    valueType: json\n  error_data:\n    topic: alerts_stream\n    keyType: string\n    valueType: json\n\nfunctions:\n  validate_sensor_data:\n    type: predicate\n    code: |\n      try:\n        # Check if all required fields are present in the nested structure\n        sensors = value.get('sensors', {})\n        metadata = value.get('metadata', {})\n\n        # Check if temperature data exists and has a value\n        if 'temperature' not in sensors or 'value' not in sensors['temperature']:\n          print(f\"Missing temperature data in message: {value}\")\n          result = False\n        elif 'humidity' not in sensors or 'value' not in sensors['humidity']:\n          print(f\"Missing humidity data in message: {value}\")\n          result = False\n        elif 'timestamp' not in metadata:\n          print(f\"Missing timestamp in message: {value}\")\n          result = False\n        else:\n          # Validate temperature range\n          temp_value = sensors['temperature']['value']\n          if not isinstance(temp_value, (int, float)) or temp_value &lt; -100 or temp_value &gt; 200:\n            print(f\"Invalid temperature value: {temp_value}\")\n            result = False\n          else:\n            # Validate humidity range\n            humidity_value = sensors['humidity']['value']\n            if not isinstance(humidity_value, (int, float)) or humidity_value &lt; 0 or humidity_value &gt; 100:\n              print(f\"Invalid humidity value: {humidity_value}\")\n              result = False\n            else:\n              result = True\n      except Exception as e:\n        print(f\"Error validating sensor data: {str(e)} - Message: {value}\")\n        result = False\n    expression: result\n    resultType: boolean\n\n  transform_sensor_data:\n    type: keyValueTransformer\n    code: |\n      from datetime import datetime\n      import time\n\n      try:\n        # Extract nested sensor data\n        sensors = value.get('sensors', {})\n        metadata = value.get('metadata', {})\n\n        # Get temperature and convert from F to C\n        temp_f = sensors.get('temperature', {}).get('value', 0)\n        temp_c = (temp_f - 32) * 5/9\n\n        # Get humidity\n        humidity = sensors.get('humidity', {}).get('value', 0)\n\n        # Calculate heat index (simplified formula)\n        heat_index = temp_c * 1.8 + 32 - 0.55 * (1 - humidity / 100)\n\n        # Get location\n        location = sensors.get('location', {}).get('value', 'unknown')\n\n        # Format timestamp\n        timestamp = metadata.get('timestamp', 0)\n        if isinstance(timestamp, (int, float)):\n          # Convert Unix timestamp to ISO format\n          formatted_time = datetime.fromtimestamp(timestamp / 1000).isoformat()\n        else:\n          formatted_time = str(timestamp)\n\n        transformed = {\n          \"sensor_id\": key,\n          \"location\": location,\n          \"readings\": {\n            \"temperature\": {\n              \"celsius\": round(temp_c, 2),\n              \"fahrenheit\": temp_f\n            },\n            \"humidity\": humidity,\n            \"heat_index\": round(heat_index, 2)\n          },\n          \"timestamp\": formatted_time,\n          \"processed_at\": int(time.time() * 1000)\n        }\n        new_key = key\n        new_value = transformed\n      except Exception as e:\n        error_data = {\n          \"error\": str(e),\n          \"original\": value,\n          \"sensor_id\": key,\n          \"timestamp\": int(time.time() * 1000)\n        }\n        new_key = key\n        new_value = error_data\n    expression: (new_key, new_value)\n    resultType: (string, json)\n\n  log_processed_data:\n    type: forEach\n    code: |\n      readings = value.get('readings', {})\n      temp = readings.get('temperature', {}).get('celsius')\n      humidity = readings.get('humidity')\n      sensor_id = value.get('sensor_id')\n      print(f\"Processed sensor data for {sensor_id}: temp={temp}C, humidity={humidity}%\")\n\npipelines:\n  process_sensor_data:\n    from: sensor_data\n    via:\n      - type: filter\n        if: validate_sensor_data\n      - type: transformKeyValue\n        mapper: transform_sensor_data\n      - type: peek\n        forEach: log_processed_data\n    branch:\n      - if:\n          expression: \"'error' in value\"\n        to: error_data\n      - to: processed_data\n</code></pre>"},{"location":"tutorials/beginner/filtering-transforming/#example-data-flow_1","title":"Example Data Flow","text":""},{"location":"tutorials/beginner/filtering-transforming/#input-message","title":"INPUT message:","text":"<ul> <li>key: <code>sensor0</code></li> <li>value: <pre><code>{\n    \"metadata\": {\n        \"timestamp\": 1753949513729\n    },\n    \"sensors\": {\n        \"humidity\": {\n            \"unit\": \"%\",\n            \"value\": 88\n        },\n        \"location\": {\n            \"unit\": \"text\",\n            \"value\": \"data_center\"\n        },\n        \"temperature\": {\n            \"unit\": \"F\",\n            \"value\": 62\n        }\n    }\n}\n</code></pre></li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#output-message-to-filtered_data-topic","title":"OUTPUT message (to <code>filtered_data</code> topic):","text":"<ul> <li>key: <code>sensor0</code></li> <li>value: <pre><code>{\n    \"location\": \"data_center\",\n    \"processed_at\": 1753950116699,\n    \"readings\": {\n        \"heat_index\": 60.51,\n        \"humidity\": 88,\n        \"temperature\": {\n            \"celsius\": 16.67,\n            \"fahrenheit\": 62\n        }\n    },\n    \"sensor_id\": \"sensor0\",\n    \"timestamp\": \"2025-07-31T07:31:53.729000\"\n}\n</code></pre></li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#log-output","title":"LOG output:","text":"<pre><code>Processed sensor data for sensor0: temp=16.67\u00b0C, humidity=88%\n</code></pre>"},{"location":"tutorials/beginner/filtering-transforming/#pipeline-processing-steps","title":"Pipeline Processing Steps","text":""},{"location":"tutorials/beginner/filtering-transforming/#step-1-validation-validate_sensor_data","title":"Step 1: Validation (<code>validate_sensor_data</code>)","text":"<ul> <li> <p>Purpose: Ensure data quality by filtering out invalid messages before processing</p> </li> <li> <p>Validation Checks:</p> <ul> <li>Structural Validation: Ensures the nested JSON structure contains required fields (<code>temperature</code>, <code>humidity</code>, <code>timestamp</code>)</li> <li>Data Type Validation: Confirms values are numeric where expected</li> <li>Range Validation:<ul> <li>Temperature must be between -100\u00b0C and 200\u00b0C</li> <li>Humidity must be between 0% and 100%</li> </ul> </li> </ul> </li> <li> <p>Result: Only valid messages pass through; malformed data is filtered out</p> </li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#step-2-transformation-transform_sensor_data","title":"Step 2: Transformation (<code>transform_sensor_data</code>)","text":"<ul> <li> <p>Purpose: Enrich and standardize the data format</p> </li> <li> <p>Transformations Applied:</p> <ul> <li>Temperature Conversion: Converts 62\u00b0F to 16.67\u00b0C using the formula <code>(F - 32) \u00d7 5/9</code></li> <li>Heat Index Calculation: Computes apparent temperature considering humidity (60.51 in this case)</li> <li>Timestamp Formatting: Converts Unix timestamp (1753949513729) to ISO format (2025-07-31T07:31:53.729000)</li> <li>Structure Flattening: Extracts values from nested structure into a cleaner format</li> <li>Metadata Addition: Adds <code>processed_at</code> timestamp to track when the transformation occurred</li> </ul> </li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#step-3-logging-log_processed_data","title":"Step 3: Logging (<code>log_processed_data</code>)","text":"<ul> <li> <p>Purpose: Provide operational visibility</p> </li> <li> <p>Features:</p> <ul> <li>Visibility: Logs key metrics for monitoring pipeline health</li> <li>Non-invasive: Uses <code>peek</code> operation to observe data without modifying it</li> <li>Format: Outputs sensor ID, temperature in Celsius, and humidity percentage</li> </ul> </li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#step-4-routing","title":"Step 4: Routing","text":"<ul> <li> <p>Purpose: Direct messages to appropriate destinations based on processing results</p> </li> <li> <p>Routing Logic:</p> <ul> <li>Error Handling: Messages with errors (containing an 'error' field) are routed to <code>alerts_stream</code></li> <li>Success Path: Successfully processed messages go to <code>filtered_data</code> topic</li> <li>Guaranteed Delivery: Every message is routed somewhere - no data loss</li> </ul> </li> </ul>"},{"location":"tutorials/beginner/filtering-transforming/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to:</p> <ul> <li>Create complex filters with multiple conditions</li> <li>Implement custom filter functions with business logic</li> <li>Handle errors in filtering and transformation</li> <li>Transform nested data structures</li> <li>Apply multiple transformations in sequence</li> <li>Combine filtering and transformation in a robust pipeline</li> </ul> <p>These techniques will help you build more sophisticated and reliable KSML applications that can handle real-world data processing challenges.</p>"},{"location":"tutorials/beginner/filtering-transforming/#next-steps","title":"Next Steps","text":"<ul> <li>Move on to intermediate tutorials to learn about stateful operations and joins</li> <li>Learn about working with different data formats in KSML</li> <li>Explore logging and monitoring to better understand your pipelines</li> </ul>"},{"location":"tutorials/beginner/logging-monitoring/","title":"Logging and Monitoring in KSML","text":"<p>Learn how to implement effective logging, monitoring, and error handling in your KSML stream processing pipelines using the built-in <code>log</code> object and peek operations.</p>"},{"location":"tutorials/beginner/logging-monitoring/#prerequisites","title":"Prerequisites","text":"<p>Before we begin:</p> <ul> <li>Please make sure there is a running Docker Compose KSML environment as described in the Quick Start.</li> <li>We recommend to have completed the KSML Basics Tutorial</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code># Logging and Monitoring Tutorial\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic ksml_logging_input &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic ksml_logging_output &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic ksml_monitoring_output &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic ksml_error_handled_output &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/beginner/logging-monitoring/#basic-logging-with-different-levels","title":"Basic Logging with Different Levels","text":"<p>KSML allows you to log messages at different levels using the <code>log</code> object in Python functions.</p> <p>This producer generates log messages with various importance levels and components:</p> Producer definition for logging messages (click to expand) <pre><code># Producer for logging and monitoring tutorial\n# Generates simple messages with different importance levels for logging demonstration\n\nfunctions:\n  generate_log_message:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      log_counter = 0\n    code: |\n      global log_counter\n\n      levels = [\"INFO\", \"WARN\", \"ERROR\", \"DEBUG\"]\n      messages = [\n        \"System startup completed\",\n        \"Database connection established\", \n        \"User authentication failed\",\n        \"Cache refresh initiated\",\n        \"Memory usage high\",\n        \"Backup process completed\"\n      ]\n\n      key = \"log-\" + str(log_counter)\n      log_counter += 1\n\n      value = {\n        \"level\": random.choice(levels),\n        \"message\": random.choice(messages),\n        \"timestamp\": time.time(),\n        \"importance\": random.randint(1, 10),\n        \"component\": random.choice([\"auth\", \"db\", \"cache\", \"backup\"])\n      }\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  log_message_producer:\n    generator: generate_log_message\n    interval: 2s\n    to:\n      topic: ksml_logging_input\n      keyType: string\n      valueType: json\n</code></pre>"},{"location":"tutorials/beginner/logging-monitoring/#ksml-features-demonstrated","title":"KSML Features Demonstrated:","text":"<ul> <li><code>log</code> object: Built-in logger available in all Python functions</li> <li>Log levels: <code>log.error()</code>, <code>log.warn()</code>, <code>log.info()</code>, <code>log.debug()</code></li> <li><code>peek</code> operation: Non-intrusive message inspection without modification</li> <li><code>forEach</code> function type: Process messages without transforming them</li> </ul> <p>This processor demonstrates logging at different levels using the <code>log</code> object:</p> Processor definition with multi-level logging (click to expand) <pre><code># Processor for logging and monitoring tutorial  \n# Demonstrates basic logging levels using the log object\n\nstreams:\n  log_input:\n    topic: ksml_logging_input\n    keyType: string\n    valueType: json\n  log_output:\n    topic: ksml_logging_output\n    keyType: string\n    valueType: json\n\nfunctions:\n  log_message:\n    type: forEach\n    code: |\n      # Test TRACE logging\n      log.trace(\"TRACE: Processing message with key={}\", key)\n\n      level = value.get(\"level\", \"INFO\")\n      message = value.get(\"message\", \"\")\n      component = value.get(\"component\", \"unknown\")\n\n      # Test DEBUG logging\n      log.debug(\"DEBUG: Message details - level={}, component={}\", level, component)\n\n      if level == \"ERROR\":\n        log.error(\"[{}] {}\", component, message)\n      elif level == \"WARN\":  \n        log.warn(\"[{}] {}\", component, message)\n      elif level == \"DEBUG\":\n        log.debug(\"[{}] {}\", component, message) \n      else:\n        log.info(\"[{}] {}\", component, message)\n\npipelines:\n  logging_pipeline:\n    from: log_input\n    via:\n      - type: peek\n        forEach:\n          code: log_message(key, value)\n    to: log_output\n</code></pre>"},{"location":"tutorials/beginner/logging-monitoring/#monitoring-with-peek-operations","title":"Monitoring with Peek Operations","text":"<p>The <code>peek</code> operation allows non-intrusive monitoring of message flow without modifying the data.</p>"},{"location":"tutorials/beginner/logging-monitoring/#ksml-features-demonstrated_1","title":"KSML Features Demonstrated:","text":"<ul> <li><code>peek</code> operation: Inspect messages in the pipeline without modifying them</li> <li>Global variables: Using <code>globals()</code> to maintain state across function calls</li> <li>Conditional logging: Log only when specific conditions are met</li> <li>Message counting: Track processed messages across invocations</li> </ul> <p>This processor shows message counting and error detection using peek operations:</p> Processor definition for monitoring operations (click to expand) <pre><code># Monitoring processor with message counting and conditional logging\n# Demonstrates peek operations for monitoring without data modification\n\nstreams:\n  monitor_input:\n    topic: ksml_logging_input  \n    keyType: string\n    valueType: json\n  monitor_output:\n    topic: ksml_monitoring_output\n    keyType: string\n    valueType: json\n\nfunctions:\n  monitor_messages:\n    type: forEach\n    code: |\n      # Count messages\n      global message_count\n      if 'message_count' not in globals():\n        message_count = 0\n      message_count += 1\n\n      # Conditional logging - only log ERROR level messages\n      if value.get(\"level\") == \"ERROR\":\n        log.error(\"Error detected: {}\", value.get(\"message\"))\n\n      # Log high importance messages as warnings\n      if value.get(\"importance\", 0) &gt; 8:\n        log.warn(\"High importance message ({}): {}\", \n                value.get(\"importance\"), value.get(\"message\"))\n\n      # Log every 10th message  \n      if message_count % 10 == 0:\n        log.info(\"Processed {} messages\", message_count)\n\npipelines:\n  monitoring_pipeline:\n    from: monitor_input\n    via:\n      - type: peek\n        forEach:\n          code: monitor_messages(key, value)\n    to: monitor_output\n</code></pre>"},{"location":"tutorials/beginner/logging-monitoring/#error-handling-with-logging","title":"Error Handling with Logging","text":"<p>Use try-catch blocks in Python functions to handle errors gracefully and log them appropriately.</p>"},{"location":"tutorials/beginner/logging-monitoring/#ksml-features-demonstrated_2","title":"KSML Features Demonstrated:","text":"<ul> <li><code>transformValue</code> operation: Transform message values with error handling</li> <li><code>valueTransformer</code> function type: Returns transformed values</li> <li>Try-except blocks: Safe processing with error catching</li> <li>Structured logging: Format logs with timestamps and component info</li> <li><code>time.strftime()</code>: Format timestamps for readable logs</li> </ul> <p>This processor demonstrates basic error handling with logging:</p> Processor definition with error handling (click to expand) <pre><code># Error handling processor with structured logging\n# Shows try-catch pattern and structured log messages\n\nstreams:\n  error_input:\n    topic: ksml_logging_input\n    keyType: string  \n    valueType: json\n  error_output:\n    topic: ksml_error_handled_output\n    keyType: string\n    valueType: json\n\nfunctions:\n  safe_process:\n    type: valueTransformer\n    code: |\n      import time\n      try:\n        result = dict(value) if value else {}\n        result[\"processed\"] = True\n\n        # Structured logging with timestamp and component info\n        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        log.info(\"[{}] Component: {} | Message: {}\", \n                timestamp, value.get(\"component\"), value.get(\"message\"))\n\n      except Exception as e:\n        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        log.error(\"[{}] Failed to process: {}\", timestamp, str(e))\n        result = {\"error\": str(e), \"processed\": False}\n    expression: result\n    resultType: json\n\npipelines:\n  error_handling_pipeline:\n    from: error_input\n    via:\n      - type: transformValue\n        mapper: safe_process\n    to: error_output\n</code></pre>"},{"location":"tutorials/beginner/logging-monitoring/#configuring-log-levels","title":"Configuring Log Levels","text":"<p>KSML provides standard logging levels through the <code>log</code> object available in Python functions:</p> <ul> <li>ERROR: Critical errors requiring attention</li> <li>WARN: Potential issues or unusual conditions  </li> <li>INFO: Normal operational events</li> <li>DEBUG: Detailed troubleshooting information</li> <li>TRACE: Very detailed debugging output</li> </ul> <p>By default, KSML shows INFO, WARN, and ERROR logs. You can enable DEBUG and TRACE logging without rebuilding the image.</p>"},{"location":"tutorials/beginner/logging-monitoring/#enabling-all-log-levels-including-debug-and-trace","title":"Enabling All Log Levels (Including DEBUG and TRACE)","text":"<ul> <li>Create a custom logback configuration file <code>logback-trace.xml</code> in your <code>examples</code> directory:</li> </ul> Custom logback configuration for TRACE logging (click to expand) <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration scan=\"false\"&gt;\n    &lt;!-- Define appenders directly without includes --&gt;\n    &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;target&gt;System.out&lt;/target&gt;\n        &lt;encoder&gt;\n            &lt;pattern&gt;%date{\"yyyy-MM-dd'T'HH:mm:ss,SSSXXX\", UTC} %-5level %-36logger{36} %msg%n&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;appender name=\"STDERR\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;target&gt;System.err&lt;/target&gt;\n        &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n            &lt;level&gt;WARN&lt;/level&gt;\n        &lt;/filter&gt;\n        &lt;encoder&gt;\n            &lt;pattern&gt;%date{\"yyyy-MM-dd'T'HH:mm:ss,SSSXXX\", UTC} %-5level %-36logger{36} %msg%n&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;!-- Root logger set to TRACE to allow all logs --&gt;\n    &lt;root level=\"TRACE\"&gt;\n        &lt;appender-ref ref=\"STDOUT\"/&gt;\n        &lt;appender-ref ref=\"STDERR\"/&gt;\n    &lt;/root&gt;\n\n    &lt;!-- Reduce noise from Kafka and other libraries --&gt;\n    &lt;logger name=\"org.apache.kafka\" level=\"WARN\"/&gt;\n    &lt;logger name=\"org.apache.kafka.clients.consumer.ConsumerConfig\" level=\"ERROR\"/&gt;\n    &lt;logger name=\"org.apache.kafka.clients.producer.ProducerConfig\" level=\"ERROR\"/&gt;\n    &lt;logger name=\"org.apache.kafka.clients.admin.AdminClientConfig\" level=\"ERROR\"/&gt;\n    &lt;logger name=\"org.apache.kafka.streams.StreamsConfig\" level=\"ERROR\"/&gt;\n    &lt;logger name=\"io.confluent\" level=\"WARN\"/&gt;\n\n    &lt;!-- Enable TRACE logging for KSML framework --&gt;\n    &lt;logger name=\"io.axual.ksml\" level=\"TRACE\"/&gt;\n\n    &lt;!-- Enable TRACE for all user functions in logging-processor.yaml --&gt;\n    &lt;!-- KSML automatically takes the first letter of your YAML filename as the namespace abbreviation to keep logger names short and manageable.\n    For example: logging-producer.yaml means that the namespace is \"l\" --&gt;\n    &lt;logger name=\"l\" level=\"TRACE\"/&gt;  &lt;!-- Enables TRACE for ALL functions in logging-processor.yaml --&gt;\n&lt;/configuration&gt;\n</code></pre> <ul> <li>Update your <code>docker-compose.yml</code> to use the custom configuration:</li> </ul> <pre><code>ksml:\n  environment:\n    - LOGBACK_CONFIGURATION_FILE=/ksml/logback-trace.xml\n</code></pre> <ul> <li>Restart the containers to see all log levels including DEBUG and TRACE.<ul> <li>To test, add for example <code>log.trace(\"TRACE: Processing message with key={}\", key)</code> into your processing definition.</li> </ul> </li> </ul>"},{"location":"tutorials/beginner/logging-monitoring/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate log levels: <ul> <li>ERROR for failures, WARN for issues, INFO for events, DEBUG for details</li> </ul> </li> <li>Include context: <ul> <li>Add message keys, component names, and relevant metadata</li> </ul> </li> <li>Avoid excessive logging:<ul> <li>Use sampling or conditional logging for high-volume streams</li> </ul> </li> <li>Structure messages:<ul> <li>Use consistent formats with key-value pairs</li> </ul> </li> <li>Monitor performance:<ul> <li>Track throughput, processing time, and error rates</li> </ul> </li> </ol>"},{"location":"tutorials/beginner/logging-monitoring/#conclusion","title":"Conclusion","text":"<p>Effective logging and monitoring enable you to track pipeline behavior, diagnose issues quickly, and maintain reliable KSML applications. Use the <code>log</code> object for different severity levels and <code>peek</code> operations for non-intrusive monitoring.</p>"},{"location":"tutorials/beginner/logging-monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>Error Handling and Recovery for advanced error handling techniques</li> <li>Performance Optimization for optimizing pipeline performance  </li> <li>Intermediate Tutorials for more advanced KSML features</li> </ul>"},{"location":"tutorials/intermediate/","title":"Intermediate Tutorials","text":"<p>Welcome to the KSML intermediate tutorials! These tutorials are designed for users who have completed the beginner tutorials and are ready to explore more advanced KSML features and patterns.</p> <p>These tutorials will help you build more sophisticated data processing applications with KSML, introducing stateful operations, joins, and other advanced concepts.</p>"},{"location":"tutorials/intermediate/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/intermediate/#branching-conditional-message-routing","title":"Branching: Conditional Message Routing","text":"<p>Master KSML's branching capabilities to route messages based on conditions:</p> <ul> <li>Understanding KSML branching and its relationship to Kafka Streams</li> <li>Simple content-based routing patterns</li> <li>Complex multi-condition business logic</li> <li>Advanced branching patterns and best practices</li> <li>Error handling with branches</li> </ul>"},{"location":"tutorials/intermediate/#working-with-aggregations","title":"Working with Aggregations","text":"<p>Learn how to use KSML's stateful operations to aggregate data:</p> <ul> <li>Counting events by key</li> <li>Calculating running averages</li> <li>Using custom aggregation functions</li> <li>Understanding state stores</li> </ul>"},{"location":"tutorials/intermediate/#implementing-joins","title":"Implementing Joins","text":"<p>This tutorial covers how to join data from multiple streams:</p> <ul> <li>Stream-to-stream joins</li> <li>Stream-to-table joins</li> <li>Global table joins</li> <li>Handling join windows and grace periods</li> </ul>"},{"location":"tutorials/intermediate/#using-windowed-operations","title":"Using Windowed Operations","text":"<p>Learn how to process data within time windows:</p> <ul> <li>Tumbling windows</li> <li>Hopping windows</li> <li>Session windows</li> <li>Sliding windows</li> <li>Time-based aggregations</li> </ul>"},{"location":"tutorials/intermediate/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>This tutorial focuses on building robust KSML applications:</p> <ul> <li>Handling malformed data</li> <li>Implementing dead letter queues</li> <li>Recovering from errors</li> <li>Monitoring and alerting</li> </ul>"},{"location":"tutorials/intermediate/#working-with-state-stores","title":"Working with State Stores","text":"<p>Learn how to use and manage state in your KSML applications:</p> <ul> <li>Creating and configuring state stores</li> <li>Reading from and writing to state stores</li> <li>Handling state store failures</li> <li>State store backup and recovery</li> </ul>"},{"location":"tutorials/intermediate/#learning-path","title":"Learning Path","text":"<p>We recommend following these tutorials in order, as they build on concepts introduced in previous tutorials.</p>"},{"location":"tutorials/intermediate/#next-steps","title":"Next Steps","text":"<p>After mastering these intermediate concepts, you're ready for the most advanced KSML topics:</p> <p>\ud83d\udc49 Continue to Advanced Tutorials to learn about custom processors, performance optimization, and integration with external systems.</p>"},{"location":"tutorials/intermediate/aggregations/","title":"Working with Aggregations in KSML","text":"<p>This tutorial explores how to compute statistics, summaries, and time-based analytics from streaming data using KSML's aggregation operations.</p>"},{"location":"tutorials/intermediate/aggregations/#introduction","title":"Introduction","text":"<p>Aggregations are stateful operations that combine multiple records into summary values. They are fundamental to stream processing, enabling real-time analytics from continuous data streams.</p> <p>KSML aggregations(stateful processing capabilities) are built on top of Kafka Streams aggregation operations.</p>"},{"location":"tutorials/intermediate/aggregations/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic financial_transactions &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic transaction_sums &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic payment_amounts &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic payment_totals &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic payment_stream &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic payment_statistics &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_actions &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_action_counts &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic retail_sales &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sales_by_region &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_temperatures &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_window_stats &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_refunds &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_bonuses &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_totals &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#core-aggregation-concepts","title":"Core Aggregation Concepts","text":""},{"location":"tutorials/intermediate/aggregations/#grouping-requirements","title":"Grouping Requirements","text":"<p>All aggregations require data to be grouped by key first:</p> <pre><code># Group by existing key\n- type: groupByKey\n\n# Group by new key using mapper function\n- type: groupBy\n  mapper:\n    expression: value.get(\"category\")\n    resultType: string\n</code></pre> <p>Kafka Streams equivalents:</p> <ul> <li><code>groupByKey</code> \u2192 <code>KStream.groupByKey()</code></li> <li><code>groupBy</code> \u2192 <code>KStream.groupBy()</code></li> </ul>"},{"location":"tutorials/intermediate/aggregations/#state-stores","title":"State Stores","text":"<p>Aggregations maintain state in local stores that are fault-tolerant through changelog topics. A changelog is a compacted Kafka topic that records every state change, allowing the state to be rebuilt if an instance fails or restarts. This provides exactly-once processing guarantees and enables automatic state recovery.</p>"},{"location":"tutorials/intermediate/aggregations/#key-value-stores","title":"Key-Value Stores","text":"<p>Used for regular (non-windowed) aggregations:</p> <pre><code>store:\n  name: my_aggregate_store\n  type: keyValue\n  caching: true           # Enable caching to reduce downstream updates\n  loggingDisabled: false  # Keep changelog for fault tolerance (default: false)\n  persistent: true        # Use RocksDB for persistence (default: true)\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#window-stores","title":"Window Stores","text":"<p>Required for windowed aggregations to store time-based state:</p> <pre><code>store:\n  name: my_window_store\n  type: window\n  windowSize: 1h          # Must match the window duration\n  retention: 24h          # How long to keep expired windows (&gt;= windowSize + grace)\n  retainDuplicates: false # Keep only latest value per window (default: false)\n  caching: true           # Enable caching for better performance\n</code></pre> <p>Important considerations:</p> <ul> <li><code>windowSize</code> must match your <code>windowByTime</code> duration</li> <li><code>retention</code> should be at least <code>windowSize + grace period</code> to handle late-arriving data</li> <li>Longer retention uses more disk space but allows querying historical windows</li> <li>Caching reduces the number of downstream updates and improves performance</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#function-types-in-aggregations","title":"Function Types in Aggregations","text":"<p>Initializer Function Creates the initial state value when a key is seen for the first time. <pre><code>initializer:\n  expression: {\"count\": 0, \"sum\": 0}  # Initial state\n  resultType: json\n</code></pre></p> <p>Aggregator Function Updates the aggregate state by combining the current aggregate with a new incoming value. <pre><code>aggregator:\n  code: |\n    # aggregatedValue: current aggregate\n    # value: new record to add\n    result = {\n      \"count\": aggregatedValue[\"count\"] + 1,\n      \"sum\": aggregatedValue[\"sum\"] + value[\"amount\"]\n    }\n  expression: result\n  resultType: json\n</code></pre></p> <p>Reducer Function (for reduce operations) Combines two values of the same type into a single value, used when no initialization is needed. <pre><code>reducer:\n  code: |\n    # value1, value2: values to combine\n    combined = value1 + value2\n  expression: combined\n  resultType: long\n</code></pre></p>"},{"location":"tutorials/intermediate/aggregations/#types-of-aggregations-in-ksml","title":"Types of Aggregations in KSML","text":"<p>KSML supports several aggregation types, each with specific use cases:</p>"},{"location":"tutorials/intermediate/aggregations/#1-count","title":"1. Count","text":"<p>Counts the number of records per key.</p> <p>Kafka Streams equivalent: <code>KGroupedStream.count()</code></p> <p>When to use:</p> <ul> <li>Tracking event frequencies</li> <li>Monitoring activity levels</li> <li>Simple counting metrics</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#2-reduce","title":"2. Reduce","text":"<p>Combines values using a reducer function without an initial value.</p> <p>Kafka Streams equivalent: <code>KGroupedStream.reduce()</code></p> <p>When to use:</p> <ul> <li>Summing values</li> <li>Finding min/max</li> <li>Combining values of the same type</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#3-aggregate","title":"3. Aggregate","text":"<p>Builds complex aggregations with custom initialization and aggregation logic.</p> <p>Kafka Streams equivalent: <code>KGroupedStream.aggregate()</code></p> <p>When to use:</p> <ul> <li>Computing statistics (avg, stddev)</li> <li>Building complex state</li> <li>Transforming value types during aggregation</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#4-cogroup","title":"4. Cogroup","text":"<p>Aggregates multiple grouped streams together into a single result.</p> <p>Kafka Streams equivalent: <code>CogroupedKStream</code></p> <p>When to use:</p> <ul> <li>Combining data from multiple sources</li> <li>Building unified aggregates from different streams</li> <li>Complex multi-stream analytics</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#count-example","title":"Count Example","text":"<p>Simple counting of events per key:</p> User actions producer (click to expand) <pre><code>functions:\n  generate_user_action:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      user_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\", \"david\", \"eve\", \"frank\", \"grace\", \"henry\"]\n      actions = [\"login\", \"view\", \"click\", \"purchase\", \"logout\", \"search\", \"share\", \"comment\"]\n    code: |\n      global user_counter, users, actions\n\n      # Cycle through users\n      user = users[user_counter % len(users)]\n      user_counter += 1\n\n      # Generate action event\n      value = {\n        \"user_id\": user,\n        \"action\": random.choice(actions),\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use user_id as key\n      key = user\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  user_action_producer:\n    generator: generate_user_action\n    interval: 1s\n    to:\n      topic: user_actions\n      keyType: string\n      valueType: json\n</code></pre> Count user actions processor (click to expand) <pre><code>streams:\n  user_actions:\n    topic: user_actions\n    keyType: string  # user_id\n    valueType: json  # action details\n\n  user_action_counts:\n    topic: user_action_counts\n    keyType: string\n    valueType: string\n\npipelines:\n  count_by_user:\n    from: user_actions\n    via:\n      - type: groupByKey\n      - type: count\n      - type: toStream\n      - type: convertValue\n        into: string\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"User {} has performed {} actions\", key, value)\n    to: user_action_counts\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#how-count-works","title":"How Count Works","text":"<p>The count operation:</p> <ol> <li>Groups messages by key using <code>groupByKey</code></li> <li>Maintains a counter per unique key</li> <li>Increments the counter for each message</li> <li>Outputs the current count as a KTable</li> </ol>"},{"location":"tutorials/intermediate/aggregations/#reduce-example","title":"Reduce Example","text":"<p>The <code>reduce</code> operation combines values without initialization, making it perfect for operations like summing, finding minimums/maximums, or concatenating strings. This section shows two approaches: a simple binary format for efficiency, and a JSON format for human readability.</p>"},{"location":"tutorials/intermediate/aggregations/#simple-reduce-binary-format","title":"Simple Reduce (Binary Format)","text":"<p>This example demonstrates the core reduce concept with minimal complexity, using binary long values for efficiency.</p> <p>What it does:</p> <ol> <li>Generates transactions: Creates random transaction amounts as long values (cents)</li> <li>Groups by account: Groups transactions by account_id (the message key)  </li> <li>Reduces values: Sums all transaction amounts using a simple reducer</li> <li>Outputs totals: Writes aggregated totals as long values</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>groupByKey</code> for partitioning data by key</li> <li><code>reduce</code> operation for stateful aggregation</li> <li>Binary data types for processing efficiency</li> </ul> Simple producer (binary long values) - click to expand <pre><code>functions:\n  generate_transaction:\n    type: generator\n    globalCode: |\n      import random\n      account_counter = 0\n      accounts = [\"ACC001\", \"ACC002\", \"ACC003\", \"ACC004\", \"ACC005\"]\n    code: |\n      global account_counter, accounts\n\n      # Cycle through accounts\n      account = accounts[account_counter % len(accounts)]\n      account_counter += 1\n\n      # Generate transaction amount (in cents)\n      amount = random.randint(100, 50000)\n\n      key = account\n      value = amount\n    expression: (key, value)\n    resultType: (string, long)\n\nproducers:\n  transaction_producer:\n    generator: generate_transaction\n    interval: 1s\n    to:\n      topic: financial_transactions\n      keyType: string\n      valueType: long\n</code></pre> Simple processor (reduce only) - click to expand <pre><code>streams:\n  financial_transactions:\n    topic: financial_transactions\n    keyType: string  # account_id\n    valueType: long  # transaction amount\n\n  transaction_sums:\n    topic: transaction_sums\n    keyType: string\n    valueType: long\n\nfunctions:\n  sum_amounts:\n    type: reducer\n    expression: value1 + value2\n    resultType: long\n\npipelines:\n  sum_transactions:\n    from: financial_transactions\n    via:\n      - type: groupByKey\n      - type: reduce\n        reducer: sum_amounts\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Account {} total: {}\", key, value)\n    to: transaction_sums\n</code></pre> <p>Verifying the results:</p> <p>Since binary data isn't human-readable in Kowl UI, use command-line tools to verify:</p> <pre><code># Check current totals\nkcat -b localhost:9092 -t transaction_sums -C -o end -c 5 -f 'Key: %k, Total: %s\\n' -s value=Q\n\n# Verify by summing all transactions for one account\nkcat -b localhost:9092 -t financial_transactions -C -o beginning -f '%k,%s\\n' -s value=Q -e | \\\n  grep \"ACC001\" | cut -d',' -f2 | awk '{sum += $1} END {print \"Sum:\", sum}'\n</code></pre> <p>Note: Binary formats like <code>long</code> are common in production for performance and storage efficiency.</p>"},{"location":"tutorials/intermediate/aggregations/#human-readable-reduce-json-format","title":"Human-Readable Reduce (JSON Format)","text":"<p>This example shows the same reduce logic but with JSON messages for better visibility in Kafka UI tools.</p> <p>Additional concepts demonstrated:</p> <ul> <li><code>transformValue</code> for data extraction and formatting  </li> <li>Type handling (JSON \u2192 long \u2192 JSON) for processing efficiency</li> <li>Human-readable message formats</li> </ul> JSON producer (human-readable) - click to expand <pre><code>functions:\n  generate_transaction:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      account_counter = 0\n      accounts = [\"ACC001\", \"ACC002\", \"ACC003\", \"ACC004\", \"ACC005\"]\n    code: |\n      global account_counter, accounts\n\n      # Cycle through accounts\n      account = accounts[account_counter % len(accounts)]\n      account_counter += 1\n\n      # Generate transaction amount (in cents to avoid float issues)\n      amount_cents = random.randint(100, 50000)\n\n      # Create human-readable JSON structure\n      value = {\n        \"account_id\": account,\n        \"amount_cents\": amount_cents,\n        \"amount_dollars\": round(amount_cents / 100.0, 2),\n        \"transaction_id\": f\"TXN{account_counter:06d}\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use account_id as key\n      key = account\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  transaction_producer:\n    generator: generate_transaction\n    interval: 1s\n    to:\n      topic: financial_transactions\n      keyType: string\n      valueType: json\n</code></pre> JSON processor (with transformations) - click to expand <pre><code>streams:\n  financial_transactions:\n    topic: financial_transactions\n    keyType: string  # account_id\n    valueType: json  # transaction details\n\n  transaction_sums:\n    topic: transaction_sums\n    keyType: string\n    valueType: json\n\nfunctions:\n  extract_amount:\n    type: valueTransformer\n    code: |\n      # Extract amount_cents from JSON transaction\n      if value is not None:\n        amount = value.get(\"amount_cents\", 0)\n      else:\n        amount = 0\n    expression: amount\n    resultType: long\n\n  sum_amounts:\n    type: reducer\n    code: |\n      # Sum two transaction amounts (in cents)\n      total = value1 + value2\n    expression: total\n    resultType: long\n\n  format_total:\n    type: valueTransformer\n    code: |\n      # Convert long cents to JSON format for human readability\n      if value is not None:\n        result = {\n          \"total_cents\": value,\n          \"total_dollars\": round(value / 100.0, 2)\n        }\n      else:\n        result = {\n          \"total_cents\": 0,\n          \"total_dollars\": 0.0\n        }\n    expression: result\n    resultType: json\n\npipelines:\n  sum_transactions:\n    from: financial_transactions\n    via:\n      - type: peek\n        forEach:\n          code: |\n            amount_dollars = value.get(\"amount_dollars\", 0) if value else 0\n            txn_id = value.get(\"transaction_id\", \"N/A\") if value else \"N/A\"\n            log.info(\"Processing transaction {} for account {}: ${}\", txn_id, key, amount_dollars)\n      - type: transformValue\n        mapper: extract_amount\n      - type: groupByKey\n      - type: reduce\n        reducer: sum_amounts\n      - type: toStream\n      - type: transformValue\n        mapper: format_total\n      - type: peek\n        forEach:\n          code: |\n            total_dollars = value.get(\"total_dollars\", 0) if value else 0\n            log.info(\"Account {} running total: ${}\", key, total_dollars)\n    to: transaction_sums\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#reduce-vs-aggregate","title":"Reduce vs Aggregate","text":"<p>Choose reduce when:</p> <ul> <li>Values are of the same type as the result</li> <li>No initialization is needed</li> <li>Simple combination logic (sum, min, max)</li> </ul> <p>Choose aggregate when:</p> <ul> <li>Result type differs from input type</li> <li>Custom initialization is required</li> <li>Complex state management is needed</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#aggregate-example","title":"Aggregate Example","text":"<p>The <code>aggregate</code> operation provides custom initialization and aggregation logic, making it perfect for building complex statistics or when input and output types differ. This section shows two approaches: a simple binary format for core concepts, and a JSON format for comprehensive statistics.</p>"},{"location":"tutorials/intermediate/aggregations/#simple-aggregate-binary-format","title":"Simple Aggregate (Binary Format)","text":"<p>This example demonstrates the core aggregate concept with minimal complexity, using binary long values.</p> <p>What it does:</p> <ol> <li>Initializes to zero: Starts aggregation with a zero value using simple expression</li> <li>Groups by customer: Groups payment amounts by customer_id (the message key)</li> <li>Sums amounts: Adds each payment amount to the running total</li> <li>Outputs totals: Writes aggregated totals as long values</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>initializer</code> with simple expression (no custom function needed)</li> <li><code>aggregator</code> with simple arithmetic expression  </li> <li>Binary data types for processing efficiency</li> </ul> Simple producer (binary long values) - click to expand <pre><code>functions:\n  generate_amount:\n    type: generator\n    globalCode: |\n      import random\n      customer_counter = 0\n      customers = [\"CUST001\", \"CUST002\", \"CUST003\", \"CUST004\", \"CUST005\"]\n    code: |\n      global customer_counter, customers\n\n      # Cycle through customers\n      customer = customers[customer_counter % len(customers)]\n      customer_counter += 1\n\n      # Generate amount in cents as long\n      amount = random.randint(1000, 50000)  # $10 to $500\n\n      key = customer\n      value = amount\n    expression: (key, value)\n    resultType: (string, long)\n\nproducers:\n  amount_producer:\n    generator: generate_amount\n    interval: 1s\n    to:\n      topic: payment_amounts\n      keyType: string\n      valueType: long\n</code></pre> Simple processor (aggregate only) - click to expand <pre><code>streams:\n  payment_amounts:\n    topic: payment_amounts\n    keyType: string  # customer_id\n    valueType: long  # amount in cents\n\n  payment_totals:\n    topic: payment_totals\n    keyType: string\n    valueType: long\n\nfunctions:\n  init_sum:\n    type: initializer\n    resultType: long\n    code: |\n      return 0\n\n  add_value:\n    type: aggregator\n    resultType: long\n    code: |\n      return aggregatedValue + value\n\npipelines:\n  calculate_totals:\n    from: payment_amounts\n    via:\n      - type: groupByKey\n      - type: aggregate\n        store:\n          name: payment_totals_store\n          type: keyValue\n        initializer: init_sum\n        aggregator: add_value\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Customer {} total: {}\", key, value)\n    to: payment_totals\n</code></pre> <p>Verifying the results:</p> <p>Since binary data isn't human-readable in Kowl UI, use command-line tools to verify:</p> <pre><code># Check current totals (convert cents to dollars)\nkcat -b localhost:9092 -t payment_totals -C -o end -c 5 -f 'Key: %k, Total cents: %s\\n' -s value=Q\n\n# Calculate expected total for one customer\nkcat -b localhost:9092 -t payment_amounts -C -o beginning -f '%k,%s\\n' -s value=Q -e | \\\n  grep \"CUST001\" | cut -d',' -f2 | awk '{sum += $1} END {print \"Expected:\", sum}'\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#complex-aggregate-json-format","title":"Complex Aggregate (JSON Format)","text":"<p>This example shows advanced aggregation with comprehensive statistics using JSON for human readability.</p> <p>Additional concepts demonstrated:</p> <ul> <li>Custom <code>initializer</code> function for complex state initialization</li> <li>Custom <code>aggregator</code> function for multi-field updates</li> <li>JSON state management for rich aggregations</li> <li>Dynamic calculations (average) within aggregation logic</li> </ul> <p>What it does:</p> <ol> <li>Initializes statistics: Creates a JSON structure to track multiple metrics (count, total, min, max, average)</li> <li>Groups by customer: Groups payment events by account_id (the message key)</li> <li>Updates statistics: For each payment, updates all metrics in the aggregated state</li> <li>Calculates average: Dynamically computes the average amount per customer</li> <li>Outputs comprehensive stats: Produces JSON messages with complete payment statistics</li> </ol> JSON payment events producer (click to expand) <pre><code>functions:\n  generate_payment:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      account_counter = 0\n      accounts = [\"ACC001\", \"ACC002\", \"ACC003\", \"ACC004\", \"ACC005\", \"ACC006\"]\n    code: |\n      global account_counter, accounts\n\n      # Cycle through accounts\n      account = accounts[account_counter % len(accounts)]\n      account_counter += 1\n\n      # Generate payment event\n      value = {\n        \"account_id\": account,\n        \"amount\": round(random.uniform(5.0, 500.0), 2),\n        \"currency\": \"USD\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use account_id as key\n      key = account\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  payment_producer:\n    generator: generate_payment\n    interval: 1s\n    to:\n      topic: payment_stream\n      keyType: string\n      valueType: json\n</code></pre> JSON statistics processor (click to expand) <pre><code>streams:\n  payment_stream:\n    topic: payment_stream\n    keyType: string  # customer_id\n    valueType: json  # payment details with amount field\n\n  payment_statistics:\n    topic: payment_statistics\n    keyType: string\n    valueType: json\n\nfunctions:\n  init_stats:\n    type: initializer\n    resultType: json\n    code: |\n      # Initialize statistics\n      return {\n        \"count\": 0,\n        \"total_amount\": 0.0,\n        \"min_amount\": None,\n        \"max_amount\": None\n      }\n\n  update_stats:\n    type: aggregator\n    resultType: json\n    code: |\n      # Update payment statistics\n      if value and aggregatedValue:\n        amount = value.get(\"amount\", 0.0)\n        count = aggregatedValue.get(\"count\", 0) + 1\n        total_amount = aggregatedValue.get(\"total_amount\", 0.0) + amount\n\n        current_min = aggregatedValue.get(\"min_amount\")\n        current_max = aggregatedValue.get(\"max_amount\")\n\n        if current_min is None or amount &lt; current_min:\n          min_amount = amount\n        else:\n          min_amount = current_min\n\n        if current_max is None or amount &gt; current_max:\n          max_amount = amount\n        else:\n          max_amount = current_max\n\n        return {\n          \"count\": count,\n          \"total_amount\": total_amount,\n          \"min_amount\": min_amount,\n          \"max_amount\": max_amount,\n          \"average_amount\": round(total_amount / count, 2)\n        }\n      return aggregatedValue\n\npipelines:\n  calculate_statistics:\n    from: payment_stream\n    via:\n      - type: groupByKey\n      - type: aggregate\n        store:\n          name: payment_stats_store\n          type: keyValue\n          caching: false\n        initializer: init_stats\n        aggregator: update_stats\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            if value:\n              count = value.get(\"count\", 0)\n              total = round(value.get(\"total_amount\", 0), 2)\n              avg = value.get(\"average_amount\", 0)\n              min_amt = value.get(\"min_amount\", 0)\n              max_amt = value.get(\"max_amount\", 0)\n              log.info(\"Customer {} - Count: {}, Total: ${}, Avg: ${}, Min: ${}, Max: ${}\", \n                       key, count, total, avg, min_amt, max_amt)\n    to: payment_statistics\n</code></pre> <p>Both approaches demonstrate the flexibility of the <code>aggregate</code> operation. The simple version focuses on the core concept, while the complex version shows real-world statistical aggregation with human-readable JSON output.</p>"},{"location":"tutorials/intermediate/aggregations/#aggregate-components","title":"Aggregate Components","text":"<ol> <li>Initializer: Creates empty/initial state</li> <li>Aggregator: Updates state with each new value</li> <li>Result: Continuously updated aggregate in state store</li> </ol>"},{"location":"tutorials/intermediate/aggregations/#windowed-aggregations","title":"Windowed Aggregations","text":"<p>Aggregations can be windowed to compute time-based analytics:</p>"},{"location":"tutorials/intermediate/aggregations/#window-types","title":"Window Types","text":""},{"location":"tutorials/intermediate/aggregations/#tumbling-windows","title":"Tumbling Windows","text":"<p>Non-overlapping, fixed-size time windows.</p> <pre><code>- type: windowByTime\n  windowType: tumbling\n  duration: 1h  # 1-hour windows\n  grace: 5m     # Allow 5 minutes for late data\n</code></pre> <p>Use cases: Hourly reports, daily summaries</p>"},{"location":"tutorials/intermediate/aggregations/#hopping-windows","title":"Hopping Windows","text":"<p>Overlapping, fixed-size windows that advance by a hop interval.</p> <pre><code>- type: windowByTime\n  windowType: hopping\n  duration: 1h    # Window size\n  advance: 15m    # Hop interval\n  grace: 5m\n</code></pre> <p>Use cases: Moving averages, overlapping analytics</p>"},{"location":"tutorials/intermediate/aggregations/#session-windows","title":"Session Windows","text":"<p>Dynamic windows based on periods of inactivity.</p> <pre><code>- type: windowBySession\n  inactivityGap: 30m  # Close window after 30 min of inactivity\n  grace: 5m\n</code></pre> <p>Use cases: User sessions, activity bursts</p>"},{"location":"tutorials/intermediate/aggregations/#windowed-aggregation-example","title":"Windowed Aggregation Example","text":"<p>This example demonstrates windowed aggregation by calculating temperature statistics (count, average, min, max) for each sensor within 30-second time windows. The windowed key contains both the original sensor ID and the window time boundaries.</p> <p>What it does:</p> <ol> <li>Groups by sensor: Each sensor's readings are processed separately</li> <li>Windows by time: Creates 30-second tumbling windows for aggregation</li> <li>Calculates statistics: Tracks count, sum, average, min, and max temperature</li> <li>Transforms window key: Converts WindowedString to a regular string key for output</li> <li>Writes to topic: Outputs windowed statistics to <code>sensor_window_stats</code> topic</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>windowByTime</code> with tumbling windows for time-based aggregation</li> <li>Window stores with configurable retention</li> <li>Accessing window metadata (start/end times) from the WindowedString key</li> <li>Transforming WindowedString keys using <code>map</code> with <code>keyValueMapper</code></li> <li>Complex aggregation state using JSON</li> </ul> Temperature sensor producer (click to expand) <pre><code>functions:\n  generate_sensor_reading:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      sensor_counter = 0\n      sensors = [\"temp001\", \"temp002\", \"temp003\"]\n    code: |\n      global sensor_counter, sensors\n\n      # Cycle through sensors\n      sensor = sensors[sensor_counter % len(sensors)]\n      sensor_counter += 1\n\n      # Generate temperature reading\n      temperature = str(round(random.uniform(18.0, 35.0), 1))\n\n      # Use sensor_id as key\n      key = sensor\n    expression: (key, temperature)\n    resultType: (string, string)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_reading\n    interval: 2s\n    to:\n      topic: sensor_temperatures\n      keyType: string\n      valueType: string\n</code></pre> Windowed temperature statistics processor (click to expand) <pre><code>streams:\n  sensor_temperatures:\n    topic: sensor_temperatures\n    keyType: string\n    valueType: string\n\n  sensor_window_stats:\n    topic: sensor_window_stats\n    keyType: string\n    valueType: json\n\nfunctions:\n  init_stats:\n    type: initializer\n    code: |\n      stats = {\n        \"count\": 0,\n        \"sum\": 0.0,\n        \"min\": None,\n        \"max\": None\n      }\n    expression: stats\n    resultType: json\n\n  update_stats:\n    type: aggregator\n    code: |\n      # Convert string to float\n      temp_value = float(value)\n\n      # Update count and sum\n      aggregatedValue[\"count\"] += 1\n      aggregatedValue[\"sum\"] += temp_value\n\n      # Update min/max\n      if aggregatedValue[\"min\"] is None or temp_value &lt; aggregatedValue[\"min\"]:\n        aggregatedValue[\"min\"] = temp_value\n      if aggregatedValue[\"max\"] is None or temp_value &gt; aggregatedValue[\"max\"]:\n        aggregatedValue[\"max\"] = temp_value\n\n      # Calculate average\n      aggregatedValue[\"avg\"] = round(aggregatedValue[\"sum\"] / aggregatedValue[\"count\"], 2)\n\n      result = aggregatedValue\n    expression: result\n    resultType: json\n\n  transform_window_key:\n    type: keyValueMapper\n    code: |\n      # Extract window information from the WindowedString key\n      sensor_id = key[\"key\"]\n      window_start = key[\"startTime\"]\n      window_end = key[\"endTime\"]\n\n      # Create a new string key with window info\n      new_key = f\"{sensor_id}_{window_start}_{window_end}\"\n\n      # Add window metadata to the value\n      new_value = {\n        \"sensor_id\": sensor_id,\n        \"window_start\": window_start,\n        \"window_end\": window_end,\n        \"stats\": value\n      }\n    expression: (new_key, new_value)\n    resultType: (string, json)\n\n  log_window_stats:\n    type: forEach\n    code: |\n      # Log the transformed results\n      sensor_id = value[\"sensor_id\"]\n      window_start = value[\"window_start\"]\n      window_end = value[\"window_end\"]\n      stats = value[\"stats\"]\n\n      log.info(\"Sensor {} | Window [{} - {}] | Stats: {} readings, avg: {}C, min: {}C, max: {}C\", \n               sensor_id, window_start, window_end, \n               stats[\"count\"], stats[\"avg\"], stats[\"min\"], stats[\"max\"])\n\npipelines:\n  windowed_temperature_stats:\n    from: sensor_temperatures\n    via:\n      - type: groupByKey\n      - type: windowByTime\n        windowType: tumbling\n        duration: 30s\n        grace: 5s\n      - type: aggregate\n        store:\n          type: window\n          windowSize: 30s\n          retention: 2m\n          caching: false\n        initializer: init_stats\n        aggregator: update_stats\n      - type: toStream\n      - type: map\n        mapper: transform_window_key\n      - type: peek\n        forEach: log_window_stats\n    to: sensor_window_stats\n</code></pre> <p>Understanding the WindowedString key:</p> <p>When using windowed aggregations, the key becomes a <code>WindowedString</code> object containing:</p> <ul> <li><code>key</code>: The original key (sensor ID)</li> <li><code>start</code>/<code>end</code>: Window boundaries in milliseconds</li> <li><code>startTime</code>/<code>endTime</code>: Human-readable UTC timestamps</li> </ul> <p>This allows you to know exactly which time window each aggregation result belongs to.</p> <p>Output format:</p> <p>The output topic contains messages with:</p> <ul> <li>Key: <code>{sensor_id}_{window_start}_{window_end}</code> (e.g., <code>temp001_2025-08-12T18:58:00Z_2025-08-12T18:58:30Z</code>)</li> <li>Value: JSON object containing:<ul> <li><code>sensor_id</code>: The sensor identifier</li> <li><code>window_start</code>/<code>window_end</code>: Window boundaries in UTC</li> <li><code>stats</code>: Aggregated statistics (count, avg, min, max)</li> </ul> </li> </ul> <p>Example output message: <pre><code>{\n  \"sensor_id\": \"temp001\",\n  \"window_start\": \"2025-08-12T18:58:00Z\",\n  \"window_end\": \"2025-08-12T18:58:30Z\",\n  \"stats\": {\n    \"count\": 3,\n    \"avg\": 24.5,\n    \"min\": 19.6,\n    \"max\": 28.7,\n    \"sum\": 73.5\n  }\n}\n</code></pre></p> <p>Verifying the input data:</p> <p>To check the raw sensor temperature readings (binary double values), use:</p> <pre><code>docker exec broker kafka-console-consumer.sh \\\n  --bootstrap-server broker:9093 \\\n  --topic sensor_temperatures \\\n  --from-beginning \\\n  --max-messages 10 \\\n  --property print.key=true \\\n  --property key.separator=\" | \" \\\n  --key-deserializer org.apache.kafka.common.serialization.StringDeserializer \\\n  --value-deserializer org.apache.kafka.common.serialization.DoubleDeserializer\n</code></pre> <p>Example input messages: <pre><code>temp001 | 24.5\ntemp002 | 31.2\ntemp003 | 18.7\ntemp001 | 26.3\ntemp002 | 29.8\n</code></pre></p>"},{"location":"tutorials/intermediate/aggregations/#advanced-cogroup-operation","title":"Advanced: Cogroup Operation","text":"<p>Cogroup allows combining multiple grouped streams into a single aggregation. This is useful when you need to aggregate data from different sources into one unified result.</p> Orders, Refunds, and Bonuses Producer (click to expand) <pre><code>functions:\n  generate_order:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      order_counter = 0\n      customers = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"]\n    code: |\n      global order_counter, customers\n\n      order_counter += 1\n      customer = random.choice(customers)\n\n      value = {\n        \"order_id\": f\"ORD{order_counter:04d}\",\n        \"customer\": customer,\n        \"amount\": round(random.uniform(10, 500), 2),\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      key = customer\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_order\n    interval: 2s\n    to:\n      topic: customer_orders\n      keyType: string\n      valueType: json\n</code></pre> Cogroup Processor (click to expand) <pre><code>streams:\n  orders:\n    topic: customer_orders\n    keyType: string\n    valueType: json\n\n  customer_totals:\n    topic: customer_totals\n    keyType: string\n    valueType: json\n\nfunctions:\n  init_order_stats:\n    type: initializer\n    code: |\n      result = {\n        \"total_amount\": 0.0,\n        \"order_count\": 0,\n        \"customer\": \"\"\n      }\n    expression: result\n    resultType: json\n\n  aggregate_orders:\n    type: aggregator\n    code: |\n      # Add order amount and increment count\n      if value is not None:\n        aggregatedValue[\"total_amount\"] = aggregatedValue.get(\"total_amount\", 0) + value.get(\"amount\", 0)\n        aggregatedValue[\"order_count\"] = aggregatedValue.get(\"order_count\", 0) + 1\n        aggregatedValue[\"customer\"] = key\n\n      result = aggregatedValue\n    expression: result\n    resultType: json\n\npipelines:\n  customer_order_totals:\n    from: orders\n    via:\n      - type: groupByKey\n      - type: cogroup\n        aggregator: aggregate_orders\n      - type: aggregate\n        initializer: init_order_stats\n        store:\n          name: customer_totals_store\n          type: keyValue\n          caching: true\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"CUSTOMER ORDER TOTALS - Customer: {}, Total: ${}, Count: {}\",\n                     key,\n                     value.get(\"total_amount\", 0),\n                     value.get(\"order_count\", 0))\n    to: customer_totals\n</code></pre>"},{"location":"tutorials/intermediate/aggregations/#how-cogroup-works","title":"How Cogroup Works","text":"<p>The cogroup operation:</p> <ol> <li>Groups each stream independently by key</li> <li>Combines the grouped streams using cogroup operations</li> <li>Each stream contributes to the aggregate with its own aggregator function</li> <li>Final aggregate operation computes the combined result</li> </ol> <p>Note: Cogroup is an advanced feature that requires careful coordination between multiple streams. Ensure all streams are properly grouped and that aggregator functions handle null values appropriately.</p>"},{"location":"tutorials/intermediate/aggregations/#complex-example-regional-sales-analytics","title":"Complex Example: Regional Sales Analytics","text":"<p>This example demonstrates rekeying (changing the grouping key) and windowed aggregation to calculate regional sales statistics:</p> <p>What it does:</p> <ol> <li>Receives sales events keyed by product ID with region, amount, and quantity data</li> <li>Rekeys by region - changes the grouping from product to geographical region</li> <li>Windows by time - creates 1-minute tumbling windows for aggregation</li> <li>Aggregates metrics - tracks total sales, quantities, transaction counts, and per-product breakdowns</li> <li>Outputs regional statistics - writes windowed regional summaries to output topic</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>map</code> with <code>keyValueMapper</code> to change the message key (rekeying)</li> <li><code>groupByKey</code> after rekeying to group by the new key</li> <li><code>windowByTime</code> for time-based regional analytics</li> <li>Complex aggregation state with nested data structures</li> <li>Tracking multiple metrics in a single aggregation</li> </ul> Sales events producer (click to expand) <pre><code>functions:\n  generate_sales_event:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      product_counter = 0\n      products = [\"laptop\", \"phone\", \"tablet\", \"monitor\", \"keyboard\", \"mouse\", \"headphones\", \"webcam\"]\n      regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n    code: |\n      global product_counter, products, regions\n\n      # Cycle through products\n      product = products[product_counter % len(products)]\n      product_counter += 1\n\n      # Generate sale event\n      value = {\n        \"product_id\": product,\n        \"region\": random.choice(regions),\n        \"amount\": round(random.uniform(10, 1000), 2),\n        \"quantity\": random.randint(1, 5),\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use product_id as key\n      key = product\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  sales_producer:\n    generator: generate_sales_event\n    interval: 2s\n    to:\n      topic: retail_sales\n      keyType: string\n      valueType: json\n</code></pre> Regional sales analytics processor (click to expand) <pre><code>streams:\n  sales_events:\n    topic: retail_sales\n    keyType: string  # Product ID\n    valueType: json  # Sale details including region, amount, quantity\n\n  sales_by_region:\n    topic: sales_by_region\n    keyType: json:windowed(string)  # Windowed Region\n    valueType: json  # Aggregated sales statistics\n\nfunctions:\n  extract_region:\n    type: keyTransformer\n    resultType: string\n    code: |\n      # Extract region from the sale event and use it as the new key\n      return value.get(\"region\", \"unknown\")\n\n  initialize_sales_stats:\n    type: initializer\n    resultType: json\n    code: |\n      return {\"total_sales\": 0.0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}}\n\n  aggregate_sales:\n    type: aggregator\n    code: |\n      # Initialize aggregatedValue if None (first aggregation)\n      if aggregatedValue is None:\n        aggregatedValue = {\"total_sales\": 0.0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}}\n\n      # Extract data from the sale\n      product_id = value.get(\"product_id\", \"unknown\")\n      amount = value.get(\"amount\", 0)\n      quantity = value.get(\"quantity\", 0)\n\n      # Update aggregated values\n      aggregatedValue[\"total_sales\"] += amount\n      aggregatedValue[\"total_quantity\"] += quantity\n      aggregatedValue[\"transaction_count\"] += 1\n\n      # Track per-product statistics\n      if product_id not in aggregatedValue[\"products\"]:\n        aggregatedValue[\"products\"][product_id] = {\"sales\": 0.0, \"quantity\": 0}\n\n      aggregatedValue[\"products\"][product_id][\"sales\"] += amount\n      aggregatedValue[\"products\"][product_id][\"quantity\"] += quantity\n\n      # Calculate average sale\n      aggregatedValue[\"avg_sale\"] = round(aggregatedValue[\"total_sales\"] / aggregatedValue[\"transaction_count\"], 2)\n\n      return aggregatedValue\n    resultType: json\n\n  transform_window_key:\n    type: valueTransformer\n    resultType: json\n    code: |\n      # Extract window information from the WindowedString key\n      region = key[\"key\"]\n      window_start = key[\"startTime\"]\n      window_end = key[\"endTime\"]\n\n      # Add window metadata to the value\n      return {\n        \"region\": region,\n        \"window_start\": window_start,\n        \"window_end\": window_end,\n        \"stats\": value\n      }\n\npipelines:\n  regional_sales_analytics:\n    from: sales_events\n    via:\n      # Group by region instead of product ID\n      - type: selectKey\n        mapper: extract_region\n      - type: groupByKey\n      # Use tumbling window of 1 minute for demo\n      - type: windowByTime\n        windowType: tumbling\n        duration: 1m\n        grace: 10s\n      # Aggregate sales data\n      - type: aggregate\n        store:\n          name: sales_by_region_store\n          type: window\n          windowSize: 1m\n          retention: 10m\n        initializer: initialize_sales_stats\n        aggregator: aggregate_sales\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n      - type: transformValue\n        mapper: transform_window_key\n      - type: peek\n        forEach:\n          code: |\n            region = value[\"region\"]\n            stats = value[\"stats\"]\n            log.info(\"Region {} sales (1-min window): ${} from {} transactions\", \n                     region, stats[\"total_sales\"], stats[\"transaction_count\"])\n    # Output to region-specific topic\n    to: sales_by_region\n</code></pre> <p>Pipeline flow:</p> <ol> <li>Input: Sales events with product_id as key</li> <li>Rekey: Map operation changes key to region</li> <li>Group: Group by the new region key</li> <li>Window: Apply 1-minute tumbling windows</li> <li>Aggregate: Calculate regional statistics per window</li> <li>Output: Windowed regional sales summaries</li> </ol>"},{"location":"tutorials/intermediate/aggregations/#performance-considerations","title":"Performance Considerations","text":""},{"location":"tutorials/intermediate/aggregations/#state-store-types","title":"State Store Types","text":"<p>RocksDB (default)</p> <ul> <li>Persistent, can handle large state</li> <li>Slower than in-memory</li> <li>Survives restarts</li> </ul> <p>In-Memory</p> <ul> <li>Fast but limited by heap size</li> <li>Lost on restart (rebuilt from changelog)</li> <li>Good for small, temporary state</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li> <p>Enable Caching <pre><code>store:\n  caching: true  # Reduces downstream updates\n</code></pre></p> </li> <li> <p>Tune Commit Intervals</p> <ul> <li>Longer intervals = better throughput, higher latency</li> <li>Shorter intervals = lower latency, more overhead</li> </ul> <p>To configure commit intervals, change your Kafka broker settings:    <pre><code># In your ksml-runner.yaml or application config\nkafka:\n  commit.interval.ms: 30000  # 30 seconds for better throughput\n  # or\n  commit.interval.ms: 100    # 100ms for lower latency\n</code></pre></p> </li> <li> <p>Pre-filter Data</p> <ul> <li>Filter before grouping to reduce state size</li> <li>Remove unnecessary fields early</li> </ul> </li> <li> <p>Choose Appropriate Window Sizes</p> <ul> <li>Smaller windows = less memory</li> <li>Consider business requirements vs resources</li> </ul> </li> </ol>"},{"location":"tutorials/intermediate/aggregations/#memory-management","title":"Memory Management","text":"<p>Monitor state store sizes:</p> <ul> <li>Each unique key requires memory</li> <li>Windowed aggregations multiply by number of windows</li> <li>Use retention policies to limit window history</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"tutorials/intermediate/aggregations/#forgetting-to-group","title":"Forgetting to Group","text":"<p>Problem: Aggregation operations require grouped streams</p> <p>Solution: Always use <code>groupByKey</code> or <code>groupBy</code> before aggregating</p>"},{"location":"tutorials/intermediate/aggregations/#null-value-handling","title":"Null Value Handling","text":"<p>Problem: Null values can cause aggregation failures</p> <p>Solution: Check for nulls in aggregator functions: <pre><code>code: |\n  if value is None:\n    return aggregatedValue\n  # ... rest of logic\n</code></pre></p>"},{"location":"tutorials/intermediate/aggregations/#type-mismatches","title":"Type Mismatches","text":"<p>Problem: Result type doesn't match expression output</p> <p>Solution: Ensure <code>resultType</code> matches what your expression returns</p>"},{"location":"tutorials/intermediate/aggregations/#window-size-vs-retention","title":"Window Size vs Retention","text":"<p>Problem: Confusion between window size and retention</p> <p>Solution: </p> <ul> <li>Window size = duration of each window</li> <li>Retention = how long to keep old windows</li> <li>Retention should be &gt; window size</li> </ul>"},{"location":"tutorials/intermediate/aggregations/#late-arriving-data","title":"Late Arriving Data","text":"<p>Problem: Data arrives after window closes</p> <p>Solution: Configure appropriate grace periods: <pre><code>grace: 5m  # Allow 5 minutes for late data\n</code></pre></p>"},{"location":"tutorials/intermediate/aggregations/#conclusion","title":"Conclusion","text":"<p>KSML aggregations enable powerful real-time analytics:</p> <ul> <li>Count for frequency analysis</li> <li>Reduce for simple combinations</li> <li>Aggregate for complex statistics</li> <li>Windowed operations for time-based analytics</li> <li>Cogroup for multi-stream aggregations</li> </ul> <p>Choose the appropriate aggregation type based on your use case, and always consider state management and performance implications.</p>"},{"location":"tutorials/intermediate/aggregations/#further-reading","title":"Further Reading","text":"<ul> <li>Reference: Stateful Operations</li> </ul>"},{"location":"tutorials/intermediate/branching/","title":"Branching in KSML: Conditional Message Routing","text":""},{"location":"tutorials/intermediate/branching/#what-is-branching","title":"What is Branching?","text":"<p>Branching in KSML allows you to split a stream into multiple paths based on conditions. Each branch can apply different transformations and route messages to different output topics. This is essential for building sophisticated data pipelines that need to handle different types of data or route messages based on business rules.</p>"},{"location":"tutorials/intermediate/branching/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_input &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic datacenter_sensors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic warehouse_sensors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic office_sensors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic unknown_sensors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic order_input &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic priority_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic regional_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic international_orders &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/branching/#relationship-to-kafka-streams","title":"Relationship to Kafka Streams","text":"<p>KSML branching is built on top of Kafka Streams' <code>BranchedKStream</code> functionality. When you define a <code>branch</code> section in KSML:</p> <ol> <li>KSML generates Kafka Streams code that calls <code>KStream.split()</code> to create a <code>BranchedKStream</code></li> <li>Each branch condition becomes a predicate function passed to <code>BranchedKStream.branch()</code></li> <li>Branch order matters - messages are evaluated against conditions in the order they're defined</li> <li>Default branches handle messages that don't match any specific condition</li> </ol> <p>This abstraction allows you to define complex routing logic in YAML without writing Java code.</p>"},{"location":"tutorials/intermediate/branching/#basic-branching-syntax","title":"Basic Branching Syntax","text":"<p>The basic structure of branching in KSML:</p> <pre><code>pipelines:\n  my_pipeline:\n    from: input_stream\n    via:\n      # Optional transformations before branching\n      - type: mapValues\n        mapper: some_transformer\n    branch:\n      # Branch 1: Messages matching a condition\n      - if: condition_predicate\n        via:\n          # Optional transformations for this branch\n          - type: mapValues\n            mapper: branch1_transformer\n        to: output_topic_1\n\n      # Branch 2: Another condition\n      - if: another_predicate\n        to: output_topic_2\n\n      # Default branch: Everything else\n      - to: default_output_topic\n</code></pre>"},{"location":"tutorials/intermediate/branching/#example-1-simple-content-based-routing","title":"Example 1: Simple Content-Based Routing","text":"<p>Let's start with a basic example that routes sensor data based on location.</p>"},{"location":"tutorials/intermediate/branching/#producer-definition","title":"Producer Definition","text":"Sensor Data Producer - click to expand <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# Producer for branching tutorial - generates sensor data from different locations\n\nfunctions:\n  generate_location_sensors:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      locations = [\"data_center\", \"warehouse\", \"office\", \"factory\", \"unknown_site\"]\n    code: |\n      global counter, locations\n      counter += 1\n\n      # Generate sensor data with different locations\n      sensor_data = {\n        \"sensor_id\": f\"sensor_{counter:03d}\",\n        \"location\": random.choice(locations),\n        \"temperature\": round(random.uniform(15.0, 35.0), 1),\n        \"humidity\": random.randint(30, 80),\n        \"timestamp\": counter * 1000\n      }\n\n      key = f\"sensor_{counter:03d}\"\n    expression: (key, sensor_data)\n    resultType: (string, json)\n\nproducers:\n  location_sensor_producer:\n    generator: generate_location_sensors\n    interval: 2s\n    to:\n      topic: sensor_input\n      keyType: string\n      valueType: json\n</code></pre>"},{"location":"tutorials/intermediate/branching/#processor-definition","title":"Processor Definition","text":"Location-Based Routing Processor - click to expand <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# Simple location-based routing example demonstrating basic branching\n\nstreams:\n  sensor_input:\n    topic: sensor_input\n    keyType: string\n    valueType: json\n\n  datacenter_sensors:\n    topic: datacenter_sensors\n    keyType: string\n    valueType: json\n\n  warehouse_sensors:\n    topic: warehouse_sensors\n    keyType: string\n    valueType: json\n\n  office_sensors:\n    topic: office_sensors\n    keyType: string\n    valueType: json\n\n  unknown_sensors:\n    topic: unknown_sensors\n    keyType: string\n    valueType: json\n\nfunctions:\n  is_datacenter:\n    type: predicate\n    expression: value.get(\"location\") == \"data_center\"\n\n  is_warehouse:\n    type: predicate\n    expression: value.get(\"location\") == \"warehouse\"\n\n  is_office:\n    type: predicate\n    expression: value.get(\"location\") == \"office\"\n\npipelines:\n  location_routing:\n    from: sensor_input\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processing sensor from: {}\", value.get(\"location\"))\n    branch:\n      - if: is_datacenter\n        via:\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"Routing data center sensor: {}\", key)\n        to: datacenter_sensors\n\n      - if: is_warehouse\n        via:\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"Routing warehouse sensor: {}\", key)\n        to: warehouse_sensors\n\n      - if: is_office\n        via:\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"Routing office sensor: {}\", key)\n        to: office_sensors\n\n      # Default branch for unknown locations\n      - via:\n          - type: peek\n            forEach:\n              code: |\n                log.warn(\"Unknown location sensor: {} from {}\", key, value.get(\"location\"))\n        to: unknown_sensors\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Simple condition matching: Routes messages based on sensor location</li> <li>Multiple branches: Separate paths for data center, warehouse, and office sensors</li> <li>Default branch: Handles sensors from unknown locations</li> <li>Logging: Each branch logs messages for monitoring</li> </ul> <p>Expected Behavior:</p> <ul> <li>Messages from \"data_center\" are routed to <code>datacenter_sensors</code> topic</li> <li>Messages from \"warehouse\" are routed to <code>warehouse_sensors</code> topic  </li> <li>Messages from \"office\" are routed to <code>office_sensors</code> topic</li> <li>All other locations are routed to <code>unknown_sensors</code> topic</li> </ul>"},{"location":"tutorials/intermediate/branching/#example-2-multi-condition-data-processing-pipeline","title":"Example 2: Multi-Condition Data Processing Pipeline","text":"<p>This example shows more complex branching with data transformation and business logic.</p>"},{"location":"tutorials/intermediate/branching/#producer-definition_1","title":"Producer Definition","text":"Order Events Producer - click to expand <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# Producer for order processing branching example\n\nfunctions:\n  generate_orders:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n      regions = [\"US\", \"EU\", \"APAC\", \"LATAM\"]\n      customer_types = [\"premium\", \"standard\", \"basic\"]\n      products = [\"laptop\", \"phone\", \"tablet\", \"monitor\", \"keyboard\"]\n    code: |\n      global counter, regions, customer_types, products\n      counter += 1\n\n      # Generate diverse order data for branching examples\n      order = {\n        \"order_id\": f\"order_{counter:04d}\",\n        \"customer_id\": f\"customer_{random.randint(1, 100):03d}\",\n        \"customer_type\": random.choice(customer_types),\n        \"product\": random.choice(products),\n        \"quantity\": random.randint(1, 5),\n        \"unit_price\": round(random.uniform(50.0, 2000.0), 2),\n        \"region\": random.choice(regions),\n        \"timestamp\": counter * 1000\n      }\n\n      # Calculate total\n      order[\"total_amount\"] = round(order[\"quantity\"] * order[\"unit_price\"], 2)\n\n      # Add some order-specific flags\n      if order[\"total_amount\"] &gt; 1000:\n        order[\"high_value\"] = True\n\n      if order[\"region\"] not in [\"US\", \"EU\"]:\n        order[\"international\"] = True\n\n      key = f\"order_{counter:04d}\"\n    expression: (key, order)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_orders\n    interval: 3s\n    to:\n      topic: order_input\n      keyType: string\n      valueType: json\n</code></pre>"},{"location":"tutorials/intermediate/branching/#processor-definition_1","title":"Processor Definition","text":"Order Processing Pipeline - click to expand <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# Complex order processing pipeline demonstrating multi-condition branching\n\nstreams:\n  order_input:\n    topic: order_input\n    keyType: string\n    valueType: json\n\n  priority_orders:\n    topic: priority_orders\n    keyType: string\n    valueType: json\n\n  regional_orders:\n    topic: regional_orders\n    keyType: string\n    valueType: json\n\n  international_orders:\n    topic: international_orders\n    keyType: string\n    valueType: json\n\nfunctions:\n  is_priority:\n    type: predicate\n    code: |\n      # Priority: Premium customers with high-value orders\n      return (value.get(\"customer_type\") == \"premium\" and \n              value.get(\"total_amount\", 0) &gt; 1000)\n\n  is_regional:\n    type: predicate\n    code: |\n      # Regional: US/EU orders that aren't priority\n      return (value.get(\"region\") in [\"US\", \"EU\"] and \n              not (value.get(\"customer_type\") == \"premium\" and value.get(\"total_amount\", 0) &gt; 1000))\n\n  is_international:\n    type: predicate\n    code: |\n      # International: Non-US/EU regions\n      return value.get(\"region\") not in [\"US\", \"EU\"]\n\n  add_priority_processing:\n    type: valueTransformer\n    code: |\n      # Add priority processing metadata\n      value[\"processing_tier\"] = \"priority\"\n      value[\"sla_hours\"] = 4\n      value[\"processed_at\"] = \"2025-01-01T00:00:00Z\"\n    expression: value\n    resultType: json\n\n  add_regional_processing:\n    type: valueTransformer\n    code: |\n      # Add regional processing metadata\n      value[\"processing_tier\"] = \"regional\" \n      value[\"sla_hours\"] = 24\n      value[\"processed_at\"] = \"2025-01-01T00:00:00Z\"\n    expression: value\n    resultType: json\n\n  add_international_processing:\n    type: valueTransformer\n    code: |\n      # Add international processing metadata\n      value[\"processing_tier\"] = \"international\"\n      value[\"sla_hours\"] = 72\n      value[\"customs_required\"] = True\n      value[\"processed_at\"] = \"2025-01-01T00:00:00Z\"\n    expression: value\n    resultType: json\n\npipelines:\n  order_processing:\n    from: order_input\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processing order: {} (${} from {} customer in {})\", \n                   value.get(\"order_id\"), \n                   value.get(\"total_amount\"),\n                   value.get(\"customer_type\"), \n                   value.get(\"region\"))\n    branch:\n      # Branch 1: Priority orders (premium customers, high value)\n      - if: is_priority\n        via:\n          - type: mapValues\n            mapper: add_priority_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"PRIORITY: Order {} - ${} premium order\", \n                       value.get(\"order_id\"), value.get(\"total_amount\"))\n        to: priority_orders\n\n      # Branch 2: Regional orders (US/EU, not priority)\n      - if: is_regional\n        via:\n          - type: mapValues\n            mapper: add_regional_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"REGIONAL: Order {} from {}\", \n                       value.get(\"order_id\"), value.get(\"region\"))\n        to: regional_orders\n\n      # Branch 3: International orders  \n      - if: is_international\n        via:\n          - type: mapValues\n            mapper: add_international_processing\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"INTERNATIONAL: Order {} from {} (customs required)\", \n                       value.get(\"order_id\"), value.get(\"region\"))\n        to: international_orders\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Complex conditions: Multiple criteria (order value, customer type, region)</li> <li>Transformations per branch: Different processing logic for each order type</li> <li>Business rules: Priority routing based on customer tier and order value</li> <li>Data enrichment: Adding processing timestamps and status fields</li> </ul> <p>Expected Behavior:</p> <ul> <li>High-value orders (&gt;$1000) from premium customers are routed to <code>priority_orders</code> topic</li> <li>US/EU orders (not priority) are routed to <code>regional_orders</code> topic  </li> <li>International orders (APAC/LATAM) are routed to <code>international_orders</code> topic</li> </ul>"},{"location":"tutorials/intermediate/branching/#advanced-branching-patterns","title":"Advanced Branching Patterns","text":""},{"location":"tutorials/intermediate/branching/#nested-branching","title":"Nested Branching","text":"<p>You can create nested branching by having branches that contain their own branching logic:</p> <pre><code>pipelines:\n  nested_example:\n    from: input_stream\n    branch:\n      - if: \n          expression: value.get(\"category\") == \"electronics\"\n        via:\n          - type: transformValue\n            mapper: enrich_electronics\n        branch:\n          - if:\n              expression: value.get(\"price\") &gt; 500\n            to: expensive_electronics\n          - to: affordable_electronics\n\n      - if:\n          expression: value.get(\"category\") == \"clothing\" \n        to: clothing_items\n</code></pre>"},{"location":"tutorials/intermediate/branching/#multiple-output-topics","title":"Multiple Output Topics","text":"<p>A single branch can route to multiple topics:</p> <pre><code>branch:\n  - if: urgent_condition\n    to:\n      - urgent_processing\n      - audit_log\n      - notifications\n</code></pre>"},{"location":"tutorials/intermediate/branching/#branch-with-complex-via-operations","title":"Branch with Complex Via Operations","text":"<p>Branches can contain multiple transformation steps:</p> <pre><code>branch:\n  - if: needs_processing\n    via:\n      - type: filter\n        if: additional_validation\n      - type: transformValue\n        mapper: enrich_data\n      - type: mapKey\n        mapper: generate_new_key\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processed: {}\", value)\n    to: processed_output\n</code></pre>"},{"location":"tutorials/intermediate/branching/#error-handling-in-branching","title":"Error Handling in Branching","text":"<p>Branching is commonly used for error handling patterns:</p> <pre><code>functions:\n  is_valid:\n    type: predicate\n    code: |\n      try:\n        # Validation logic\n        required_fields = [\"id\", \"timestamp\", \"data\"]\n        return all(field in value for field in required_fields)\n      except:\n        return False\n\n  is_error:\n    type: predicate\n    expression: \"'error' in value or value.get('status') == 'failed'\"\n\npipelines:\n  error_handling:\n    from: input_stream\n    via:\n      - type: mapValues\n        mapper: safe_processor  # This adds error info if processing fails\n    branch:\n      # Route errors to dead letter queue\n      - if: is_error\n        to: error_topic\n\n      # Route valid data for further processing  \n      - if: is_valid\n        to: processed_data\n\n      # Route everything else to manual review\n      - to: manual_review\n</code></pre>"},{"location":"tutorials/intermediate/branching/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/intermediate/branching/#1-order-matters","title":"1. Order Matters","text":"<p>Branches are evaluated in the order they're defined. Place more specific conditions first:</p> <pre><code>branch:\n  # Specific condition first\n  - if:\n      expression: value.get(\"priority\") == \"urgent\" and value.get(\"amount\") &gt; 10000\n    to: urgent_high_value\n\n  # General condition second  \n  - if:\n      expression: value.get(\"priority\") == \"urgent\"\n    to: urgent_orders\n\n  # Default branch last\n  - to: standard_orders\n</code></pre>"},{"location":"tutorials/intermediate/branching/#2-use-meaningful-branch-names","title":"2. Use Meaningful Branch Names","text":"<p>When working with complex branching, use the name attribute for clarity:</p> <pre><code>branch:\n  - name: high_priority_branch\n    if: high_priority_predicate\n    to: priority_queue\n</code></pre>"},{"location":"tutorials/intermediate/branching/#3-keep-conditions-simple","title":"3. Keep Conditions Simple","text":"<p>Complex logic should be in predicate functions, not inline expressions:</p> <pre><code>functions:\n  is_complex_condition:\n    type: predicate\n    code: |\n      # Complex business logic here\n      return (value.get(\"score\") &gt; 85 and \n              value.get(\"region\") in [\"US\", \"EU\"] and\n              len(value.get(\"tags\", [])) &gt; 2)\n\nbranch:\n  - if: is_complex_condition  # Clean and readable\n    to: qualified_items\n</code></pre>"},{"location":"tutorials/intermediate/branching/#4-handle-all-cases","title":"4. Handle All Cases","text":"<p>Always ensure messages have somewhere to go:</p> <pre><code>branch:\n  - if: condition_a\n    to: topic_a\n  - if: condition_b  \n    to: topic_b\n  # Always include a default case\n  - to: default_topic\n</code></pre>"},{"location":"tutorials/intermediate/branching/#5-add-monitoring","title":"5. Add Monitoring","text":"<p>Use <code>peek</code> operations to monitor branch behavior:</p> <pre><code>branch:\n  - if: important_condition\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Important message routed: {}\", key)\n    to: important_topic\n</code></pre>"},{"location":"tutorials/intermediate/branching/#performance-considerations","title":"Performance Considerations","text":""},{"location":"tutorials/intermediate/branching/#predicate-function-performance","title":"Predicate Function Performance","text":"<ul> <li>Keep predicate functions lightweight</li> <li>Avoid expensive operations in conditions</li> <li>Cache results when possible</li> </ul>"},{"location":"tutorials/intermediate/branching/#branch-count","title":"Branch Count","text":"<ul> <li>Too many branches can impact performance</li> <li>Consider using lookup tables for many conditions</li> <li>Group similar conditions when possible</li> </ul>"},{"location":"tutorials/intermediate/branching/#memory-usage","title":"Memory Usage","text":"<ul> <li>Each branch creates a separate stream processing path</li> <li>Monitor memory usage with many concurrent branches</li> </ul>"},{"location":"tutorials/intermediate/branching/#conclusion","title":"Conclusion","text":"<p>Branching in KSML provides powerful conditional routing capabilities that map directly to Kafka Streams' branching functionality. It enables you to:</p> <ul> <li>Route messages based on content, business rules, or data quality</li> <li>Apply different transformations to different types of data</li> <li>Implement error handling patterns like dead letter queues</li> <li>Build complex pipelines with multiple processing paths</li> </ul>"},{"location":"tutorials/intermediate/branching/#next-steps","title":"Next Steps","text":"<ul> <li>Reference: Branch Operations</li> </ul>"},{"location":"tutorials/intermediate/error-handling/","title":"Error Handling and Recovery in KSML","text":"<p>This tutorial explores comprehensive strategies for handling errors and implementing recovery mechanisms in KSML applications, helping you build robust and resilient stream processing pipelines.</p>"},{"location":"tutorials/intermediate/error-handling/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic incoming_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic valid_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic invalid_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_readings &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic processed_sensors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_errors &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic payment_requests &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic processed_payments &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic failed_payments &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic api_operations &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic successful_operations &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic failed_operations &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic service_requests &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic service_responses &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic circuit_events &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/error-handling/#introduction-to-error-handling","title":"Introduction to Error Handling","text":"<p>Error handling is critical for production stream processing applications. Common error scenarios include:</p> <ul> <li>Data Quality Issues: Invalid, malformed, or missing data</li> <li>External Service Failures: Network timeouts, API errors, service unavailability  </li> <li>Resource Constraints: Memory limitations, disk space issues</li> <li>Business Rule Violations: Data that doesn't meet application requirements</li> <li>Transient Failures: Temporary network issues, rate limiting, service overload</li> </ul> <p>Without proper error handling, these issues can cause application crashes, data loss, incorrect results, or inconsistent state.</p>"},{"location":"tutorials/intermediate/error-handling/#core-error-handling-patterns","title":"Core Error Handling Patterns","text":""},{"location":"tutorials/intermediate/error-handling/#1-validation-and-filtering","title":"1. Validation and Filtering","text":"<p>Proactively validate data and filter out problematic messages before they cause downstream errors.</p> Order Events Producer - click to expand <pre><code># Producer for validation example - generates orders with various quality issues\n\nfunctions:\n  generate_orders:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      # Generate different order scenarios\n      if counter % 4 == 1:\n        # Valid order\n        order = {\n          \"order_id\": f\"order_{counter}\",\n          \"customer_id\": f\"customer_{random.randint(1, 5)}\",\n          \"product_id\": f\"product_{random.randint(1, 3)}\",\n          \"quantity\": random.randint(1, 5),\n          \"price\": round(random.uniform(10.0, 100.0), 2)\n        }\n      elif counter % 4 == 2:\n        # Missing required field\n        order = {\n          \"order_id\": f\"order_{counter}\",\n          \"customer_id\": f\"customer_{random.randint(1, 5)}\"\n          # Missing product_id, quantity, price\n        }\n      elif counter % 4 == 3:\n        # Invalid quantity\n        order = {\n          \"order_id\": f\"order_{counter}\",\n          \"customer_id\": f\"customer_{random.randint(1, 5)}\",\n          \"product_id\": f\"product_{random.randint(1, 3)}\",\n          \"quantity\": -1,\n          \"price\": round(random.uniform(10.0, 100.0), 2)\n        }\n      else:\n        # Malformed order\n        order = {\"malformed\": True}\n\n      key = f\"order_{counter}\"\n    expression: (key, order)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_orders\n    interval: 2s\n    count: 16\n    to:\n      topic: incoming_orders\n      keyType: string\n      valueType: json\n</code></pre> Validation and Filtering Processor - click to expand <pre><code># Validation and filtering example using branch operation\n\nstreams:\n  incoming_orders:\n    topic: incoming_orders\n    keyType: string\n    valueType: json\n\n  valid_orders:\n    topic: valid_orders\n    keyType: string\n    valueType: json\n\n  invalid_orders:\n    topic: invalid_orders\n    keyType: string\n    valueType: json\n\nfunctions:\n  add_validation_status:\n    type: valueTransformer\n    code: |\n      value[\"status\"] = \"valid\"\n      value[\"validated_at\"] = key\n    expression: value\n    resultType: json\n\n  add_error_details:\n    type: valueTransformer\n    code: |\n      value[\"status\"] = \"invalid\"\n      if \"malformed\" in value:\n        value[\"error_reason\"] = \"malformed_data\"\n      elif \"product_id\" not in value:\n        value[\"error_reason\"] = \"missing_required_fields\"\n      elif value.get(\"quantity\", 0) &lt;= 0:\n        value[\"error_reason\"] = \"invalid_quantity\"\n      else:\n        value[\"error_reason\"] = \"validation_failed\"\n    expression: value\n    resultType: json\n\npipelines:\n  validate_orders:\n    from: incoming_orders\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processing order: {}\", key)\n    branch:\n      # Valid orders branch\n      - if:\n          expression: value and \"malformed\" not in value and \"product_id\" in value and value.get(\"quantity\", 0) &gt; 0\n        via:\n          - type: transformValue\n            mapper: add_validation_status\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"VALID ORDER: {}\", key)\n        to: valid_orders\n      # Invalid orders branch  \n      - via:\n          - type: transformValue\n            mapper: add_error_details\n          - type: peek\n            forEach:\n              code: |\n                log.info(\"INVALID ORDER: {} - {}\", key, value.get(\"error_reason\"))\n        to: invalid_orders\n</code></pre> <p>What it does:</p> <ul> <li>Produces order events: Creates orders with varying quality - some valid with all fields, some missing product_id, some with invalid quantity, some marked as \"malformed\" </li> <li>Validates with branching: Uses branch operation with expression to check <code>value and \"malformed\" not in value and \"product_id\" in value and value.get(\"quantity\", 0) &gt; 0</code></li> <li>Routes by validity: Valid orders go to valid_orders topic with \"valid\" status, everything else goes to invalid_orders topic</li> <li>Categorizes errors: Adds specific error_reason (malformed_data, missing_required_fields, invalid_quantity, validation_failed) to invalid orders</li> <li>Logs decisions: Tracks processing with logging for valid/invalid determinations and specific error reasons for debugging</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#2-try-catch-error-handling","title":"2. Try-Catch Error Handling","text":"<p>Use try-catch blocks in Python functions to handle exceptions gracefully and provide fallback behavior.</p> Sensor Data Producer - click to expand <pre><code># Producer for try-catch example - generates sensor data with various error conditions\n\nfunctions:\n  generate_sensor_data:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      # Generate different sensor scenarios\n      if counter % 5 == 1:\n        # Normal reading\n        data = {\n          \"sensor_id\": f\"sensor_{counter % 3 + 1}\",\n          \"temperature\": round(random.uniform(20.0, 80.0), 2),\n          \"humidity\": round(random.uniform(30.0, 90.0), 2)\n        }\n      elif counter % 5 == 2:\n        # Invalid temperature (string instead of number)\n        data = {\n          \"sensor_id\": f\"sensor_{counter % 3 + 1}\",\n          \"temperature\": \"invalid\",\n          \"humidity\": round(random.uniform(30.0, 90.0), 2)\n        }\n      elif counter % 5 == 3:\n        # Missing humidity field\n        data = {\n          \"sensor_id\": f\"sensor_{counter % 3 + 1}\",\n          \"temperature\": round(random.uniform(20.0, 80.0), 2)\n        }\n      elif counter % 5 == 4:\n        # Extreme values that should trigger warnings\n        data = {\n          \"sensor_id\": f\"sensor_{counter % 3 + 1}\",\n          \"temperature\": round(random.uniform(100.0, 150.0), 2),\n          \"humidity\": round(random.uniform(95.0, 100.0), 2)\n        }\n      else:\n        # Null/None data\n        data = None\n\n      key = f\"sensor_{counter % 3 + 1}\"\n    expression: (key, data)\n    resultType: (string, json)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_data\n    interval: 2s\n    count: 20\n    to:\n      topic: sensor_readings\n      keyType: string\n      valueType: json\n</code></pre> Try-Catch Processor - click to expand <pre><code># Try-catch error handling example using exception handling in functions\n\nstreams:\n  sensor_readings:\n    topic: sensor_readings\n    keyType: string\n    valueType: json\n\n  processed_sensors:\n    topic: processed_sensors\n    keyType: string\n    valueType: json\n\n  sensor_errors:\n    topic: sensor_errors\n    keyType: string\n    valueType: json\n\nfunctions:\n  process_sensor_reading:\n    type: valueTransformer\n    code: |\n      try:\n        # Attempt to process the sensor data\n        if value is None:\n          raise ValueError(\"Null sensor data\")\n\n        sensor_id = value.get(\"sensor_id\")\n        if not sensor_id:\n          raise ValueError(\"Missing sensor_id\")\n\n        # Try to get and validate temperature\n        temp = value.get(\"temperature\")\n        if temp is None:\n          raise ValueError(\"Missing temperature\")\n\n        # Convert temperature to float (may raise ValueError)\n        temp_float = float(temp)\n\n        # Get humidity with default\n        humidity = float(value.get(\"humidity\", 0))\n\n        # Create processed result\n        result = {\n          \"sensor_id\": sensor_id,\n          \"temperature\": temp_float,\n          \"humidity\": humidity,\n          \"status\": \"normal\" if temp_float &lt; 100 else \"warning\",\n          \"processed_at\": sensor_id  # Use sensor_id as timestamp marker\n        }\n\n      except (ValueError, TypeError) as e:\n        # Handle processing errors\n        result = {\n          \"sensor_id\": value.get(\"sensor_id\", \"unknown\") if value else \"unknown\",\n          \"error\": str(e),\n          \"status\": \"error\",\n          \"original_data\": value,\n          \"processed_at\": key\n        }\n\n    expression: result\n    resultType: json\n\npipelines:\n  process_sensors:\n    from: sensor_readings\n    via:\n      - type: transformValue\n        mapper: process_sensor_reading\n      - type: peek\n        forEach:\n          code: |\n            status = value.get(\"status\", \"unknown\")\n            sensor_id = value.get(\"sensor_id\", \"unknown\")\n            if status == \"error\":\n              log.error(\"SENSOR ERROR - {}: {}\", sensor_id, value.get(\"error\"))\n            else:\n              log.info(\"SENSOR OK - {}: temp={}C\", sensor_id, value.get(\"temperature\"))\n    branch:\n      # Route successful processing\n      - if:\n          expression: value.get(\"status\") != \"error\"\n        to: processed_sensors\n      # Route errors to error topic\n      - to: sensor_errors\n</code></pre> <p>What it does:</p> <ul> <li>Produces sensor data: Creates sensor readings with varying quality - some valid with all fields, some with missing sensor_id or temperature, some with non-numeric temperature values</li> <li>Handles with try-catch: Uses try-catch in valueTransformer to catch ValueError and TypeError exceptions during processing</li> <li>Processes safely: Attempts to convert temperature to float, validate sensor_id, set default humidity values within exception handling</li> <li>Creates error objects: When exceptions occur, returns error object with sensor_id, error message, status=\"error\", original_data for debugging</li> <li>Routes by success: Uses branch to send successful processing to processed_sensors, errors to sensor_errors topic based on status field</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#3-dead-letter-queue-pattern","title":"3. Dead Letter Queue Pattern","text":"<p>Route messages that cannot be processed to dedicated error topics for later analysis or reprocessing.</p> Payment Requests Producer - click to expand <pre><code># Producer for dead letter queue example - generates payment requests with various conditions\n\nfunctions:\n  generate_payment_requests:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      # Generate different payment scenarios\n      if counter % 6 == 1:\n        # Valid payment\n        payment = {\n          \"payment_id\": f\"pay_{counter}\",\n          \"amount\": round(random.uniform(10.0, 500.0), 2),\n          \"currency\": \"USD\",\n          \"merchant_id\": f\"merchant_{random.randint(1, 3)}\"\n        }\n      elif counter % 6 == 2:\n        # Invalid amount\n        payment = {\n          \"payment_id\": f\"pay_{counter}\",\n          \"amount\": -50.0,\n          \"currency\": \"USD\",\n          \"merchant_id\": f\"merchant_{random.randint(1, 3)}\"\n        }\n      elif counter % 6 == 3:\n        # Missing required fields\n        payment = {\n          \"payment_id\": f\"pay_{counter}\",\n          \"amount\": round(random.uniform(10.0, 500.0), 2)\n        }\n      elif counter % 6 == 4:\n        # Invalid currency\n        payment = {\n          \"payment_id\": f\"pay_{counter}\",\n          \"amount\": round(random.uniform(10.0, 500.0), 2),\n          \"currency\": \"INVALID\",\n          \"merchant_id\": f\"merchant_{random.randint(1, 3)}\"\n        }\n      elif counter % 6 == 5:\n        # Retry-eligible error (temporary network issue simulation)\n        payment = {\n          \"payment_id\": f\"pay_{counter}\",\n          \"amount\": round(random.uniform(10.0, 500.0), 2),\n          \"currency\": \"USD\",\n          \"merchant_id\": \"temp_failure_merchant\"\n        }\n      else:\n        # Completely malformed\n        payment = {\"invalid\": True, \"data\": counter}\n\n      key = f\"payment_{counter}\"\n    expression: (key, payment)\n    resultType: (string, json)\n\nproducers:\n  payment_producer:\n    generator: generate_payment_requests\n    interval: 2s\n    count: 18\n    to:\n      topic: payment_requests\n      keyType: string\n      valueType: json\n</code></pre> Dead Letter Queue Processor - click to expand <pre><code># Dead letter queue pattern with retry logic for transient failures\n\nstreams:\n  payment_requests:\n    topic: payment_requests\n    keyType: string\n    valueType: json\n\n  processed_payments:\n    topic: processed_payments\n    keyType: string\n    valueType: json\n\n  failed_payments:\n    topic: failed_payments\n    keyType: string\n    valueType: json\n\nfunctions:\n  process_payment:\n    type: valueTransformer\n    code: |\n      # Validate payment request\n      if value is None or \"invalid\" in value:\n        result = {\n          \"payment_id\": \"unknown\",\n          \"status\": \"permanent_failure\",\n          \"error\": \"malformed_request\",\n          \"retry_eligible\": False,\n          \"original_request\": value\n        }\n      elif value.get(\"amount\", 0) &lt;= 0:\n        result = {\n          \"payment_id\": value.get(\"payment_id\", \"unknown\"),\n          \"status\": \"permanent_failure\", \n          \"error\": \"invalid_amount\",\n          \"retry_eligible\": False,\n          \"original_request\": value\n        }\n      elif \"currency\" not in value or \"merchant_id\" not in value:\n        result = {\n          \"payment_id\": value.get(\"payment_id\", \"unknown\"),\n          \"status\": \"permanent_failure\",\n          \"error\": \"missing_required_fields\",\n          \"retry_eligible\": False,\n          \"original_request\": value\n        }\n      elif value.get(\"currency\") not in [\"USD\", \"EUR\", \"GBP\"]:\n        result = {\n          \"payment_id\": value.get(\"payment_id\", \"unknown\"),\n          \"status\": \"permanent_failure\",\n          \"error\": \"unsupported_currency\",\n          \"retry_eligible\": False,\n          \"original_request\": value\n        }\n      elif value.get(\"merchant_id\") == \"temp_failure_merchant\":\n        # Simulate temporary network failure\n        result = {\n          \"payment_id\": value.get(\"payment_id\", \"unknown\"),\n          \"status\": \"temporary_failure\",\n          \"error\": \"network_timeout\",\n          \"retry_eligible\": True,\n          \"original_request\": value\n        }\n      else:\n        # Process successful payment\n        result = {\n          \"payment_id\": value.get(\"payment_id\"),\n          \"amount\": value.get(\"amount\"),\n          \"currency\": value.get(\"currency\"),\n          \"merchant_id\": value.get(\"merchant_id\"),\n          \"status\": \"processed\",\n          \"processed_at\": key\n        }\n\n    expression: result\n    resultType: json\n\npipelines:\n  process_payments:\n    from: payment_requests\n    via:\n      - type: transformValue\n        mapper: process_payment\n      - type: peek\n        forEach:\n          code: |\n            status = value.get(\"status\", \"unknown\")\n            payment_id = value.get(\"payment_id\", \"unknown\")\n            if status == \"processed\":\n              log.info(\"PAYMENT SUCCESS - {}: ${}\", payment_id, value.get(\"amount\"))\n            elif status == \"temporary_failure\":\n              log.warn(\"PAYMENT RETRY - {}: {}\", payment_id, value.get(\"error\"))\n            else:\n              log.error(\"PAYMENT FAILED - {}: {}\", payment_id, value.get(\"error\"))\n    branch:\n      # Route successful payments\n      - if:\n          expression: value.get(\"status\") == \"processed\"\n        to: processed_payments\n      # Route failed payments to dead letter queue\n      - to: failed_payments\n</code></pre> <p>What it does:</p> <ul> <li>Produces payment requests: Creates payment events with varying scenarios - some successful, some with insufficient funds, some with invalid cards, some with network issues</li> <li>Simulates processing: Mock payment processing with different failure types - permanent (invalid_card, insufficient_funds) vs transient (network_error, timeout)</li> <li>Classifies errors: Determines retry eligibility based on error type - network/timeout errors are retryable, invalid card/insufficient funds are permanent</li> <li>Enriches with context: Adds processing metadata, error classification, retry recommendations, original request data to failed messages</li> <li>Routes by result: Uses branch to send successful payments to processed_payments, failures to failed_payments topic with full error context</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#4-retry-strategies-with-exponential-backoff","title":"4. Retry Strategies with Exponential Backoff","text":"<p>Implement sophisticated retry logic for transient failures with exponential backoff and jitter.</p> API Operations Producer - click to expand <pre><code># Producer for retry strategies example - generates API operations with failure scenarios\n\nfunctions:\n  generate_api_operations:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      # Generate different API operation scenarios\n      if counter % 5 == 1:\n        # Successful operation\n        operation = {\n          \"operation_id\": f\"op_{counter}\",\n          \"api_endpoint\": \"/users/create\",\n          \"should_fail\": False,\n          \"retry_count\": 0\n        }\n      elif counter % 5 == 2:\n        # Transient network error (retryable)\n        operation = {\n          \"operation_id\": f\"op_{counter}\",\n          \"api_endpoint\": \"/users/update\", \n          \"should_fail\": True,\n          \"failure_type\": \"network_timeout\",\n          \"retry_count\": 0\n        }\n      elif counter % 5 == 3:\n        # Rate limit error (retryable)\n        operation = {\n          \"operation_id\": f\"op_{counter}\",\n          \"api_endpoint\": \"/data/fetch\",\n          \"should_fail\": True,\n          \"failure_type\": \"rate_limit\",\n          \"retry_count\": 0\n        }\n      elif counter % 5 == 4:\n        # Permanent error (not retryable)\n        operation = {\n          \"operation_id\": f\"op_{counter}\",\n          \"api_endpoint\": \"/users/delete\",\n          \"should_fail\": True,\n          \"failure_type\": \"not_found\",\n          \"retry_count\": 0\n        }\n      else:\n        # Authentication error (not retryable)\n        operation = {\n          \"operation_id\": f\"op_{counter}\",\n          \"api_endpoint\": \"/admin/config\",\n          \"should_fail\": True,\n          \"failure_type\": \"auth_failed\",\n          \"retry_count\": 0\n        }\n\n      key = f\"op_{counter}\"\n    expression: (key, operation)\n    resultType: (string, json)\n\nproducers:\n  api_operation_producer:\n    generator: generate_api_operations\n    interval: 3s\n    count: 15\n    to:\n      topic: api_operations\n      keyType: string\n      valueType: json\n</code></pre> Retry Strategies Processor - click to expand <pre><code># Retry strategies with exponential backoff for transient failures\n\nstreams:\n  api_operations:\n    topic: api_operations\n    keyType: string\n    valueType: json\n\n  successful_operations:\n    topic: successful_operations\n    keyType: string\n    valueType: json\n\n  failed_operations:\n    topic: failed_operations\n    keyType: string\n    valueType: json\n\nfunctions:\n  process_with_retry:\n    type: valueTransformer\n    globalCode: |\n      import random\n      import math\n\n      MAX_RETRIES = 3\n      BASE_DELAY_MS = 1000\n\n      def calculate_backoff_delay(attempt):\n        # Exponential backoff: 1s, 2s, 4s\n        delay = BASE_DELAY_MS * (2 ** attempt)\n        # Add 20% jitter\n        jitter = delay * 0.2 * (random.random() * 2 - 1)\n        return int(delay + jitter)\n\n    code: |\n      operation_id = value.get(\"operation_id\", \"unknown\")\n      retry_count = value.get(\"retry_count\", 0)\n      should_fail = value.get(\"should_fail\", False)\n      failure_type = value.get(\"failure_type\", \"\")\n\n      # Determine if this operation should succeed or fail\n      if not should_fail:\n        # Success case\n        result = {\n          \"operation_id\": operation_id,\n          \"api_endpoint\": value.get(\"api_endpoint\"),\n          \"status\": \"success\",\n          \"retry_count\": retry_count,\n          \"final_attempt\": True\n        }\n      else:\n        # Check if error is retryable\n        retryable_errors = [\"network_timeout\", \"rate_limit\", \"server_error\"]\n        is_retryable = failure_type in retryable_errors\n\n        if is_retryable and retry_count &lt; MAX_RETRIES:\n          # Increment retry count and calculate delay\n          new_retry_count = retry_count + 1\n          backoff_delay = calculate_backoff_delay(new_retry_count - 1)\n\n          # Simulate: 30% chance of success after retry\n          if random.random() &lt; 0.3:\n            result = {\n              \"operation_id\": operation_id,\n              \"api_endpoint\": value.get(\"api_endpoint\"),\n              \"status\": \"success_after_retry\",\n              \"retry_count\": new_retry_count,\n              \"backoff_delay\": backoff_delay,\n              \"final_attempt\": True\n            }\n          else:\n            result = {\n              \"operation_id\": operation_id,\n              \"api_endpoint\": value.get(\"api_endpoint\"),\n              \"status\": \"retry_needed\",\n              \"retry_count\": new_retry_count,\n              \"failure_type\": failure_type,\n              \"backoff_delay\": backoff_delay,\n              \"final_attempt\": new_retry_count &gt;= MAX_RETRIES\n            }\n        else:\n          # Not retryable or max retries exceeded\n          result = {\n            \"operation_id\": operation_id,\n            \"api_endpoint\": value.get(\"api_endpoint\"),\n            \"status\": \"permanent_failure\",\n            \"retry_count\": retry_count,\n            \"failure_type\": failure_type,\n            \"final_attempt\": True\n          }\n\n    expression: result\n    resultType: json\n\npipelines:\n  retry_processor:\n    from: api_operations\n    via:\n      - type: transformValue\n        mapper: process_with_retry\n      - type: peek\n        forEach:\n          code: |\n            status = value.get(\"status\", \"unknown\")\n            op_id = value.get(\"operation_id\", \"unknown\")\n            retry_count = value.get(\"retry_count\", 0)\n\n            if status == \"success\":\n              log.info(\"API SUCCESS - {}: completed\", op_id)\n            elif status == \"success_after_retry\":\n              log.info(\"API RETRY SUCCESS - {}: completed after {} retries\", op_id, retry_count)\n            elif status == \"retry_needed\":\n              delay = value.get(\"backoff_delay\", 0)\n              log.warn(\"API RETRY - {}: attempt {} failed, retry in {}ms\", op_id, retry_count, delay)\n            else:\n              log.error(\"API FAILED - {}: {}\", op_id, value.get(\"failure_type\"))\n    branch:\n      # Route successful operations\n      - if:\n          expression: value.get(\"status\") in [\"success\", \"success_after_retry\"]\n        to: successful_operations\n      # Route failed operations\n      - to: failed_operations\n</code></pre> <p>What it does:</p> <ul> <li>Produces API operations: Creates operations (fetch_user, update_profile, delete_account) with deliberate failures for some operations to test retry logic</li> <li>Simulates API calls: Mock external API with different failure scenarios (timeout, rate_limit, server_error) and success cases  </li> <li>Implements retry logic: Calculates exponential backoff delays (1s, 2s, 4s, 8s) with added jitter to prevent retry storms</li> <li>Tracks attempts: Maintains retry count, determines retry eligibility based on error type and max attempts (3), stores original operation data</li> <li>Routes by outcome: Successful operations go to successful_operations, failed/exhausted retries go to failed_operations with retry history</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#5-circuit-breaker-pattern","title":"5. Circuit Breaker Pattern","text":"<p>Prevent cascading failures by temporarily stopping calls to failing services, allowing them to recover.</p> Service Requests Producer - click to expand <pre><code># Producer for circuit breaker example - generates service requests with failure patterns\n\nfunctions:\n  generate_service_requests:\n    type: generator\n    globalCode: |\n      import random\n      counter = 0\n    code: |\n      global counter\n      counter += 1\n\n      # Generate different service request patterns\n      if counter &lt;= 5:\n        # Initial successful requests\n        request = {\n          \"request_id\": f\"req_{counter}\",\n          \"service\": \"user_service\",\n          \"should_succeed\": True\n        }\n      elif counter &lt;= 10:\n        # Start failing to trigger circuit breaker\n        request = {\n          \"request_id\": f\"req_{counter}\",\n          \"service\": \"user_service\",\n          \"should_succeed\": False,\n          \"error_type\": \"timeout\"\n        }\n      elif counter &lt;= 15:\n        # Continue with failures (circuit should be open)\n        request = {\n          \"request_id\": f\"req_{counter}\",\n          \"service\": \"user_service\",\n          \"should_succeed\": False,\n          \"error_type\": \"connection_failed\"\n        }\n      else:\n        # Service recovery - mix of success/failure\n        request = {\n          \"request_id\": f\"req_{counter}\",\n          \"service\": \"user_service\",\n          \"should_succeed\": random.choice([True, True, False])  # 66% success\n        }\n\n      key = f\"req_{counter}\"\n    expression: (key, request)\n    resultType: (string, json)\n\nproducers:\n  service_request_producer:\n    generator: generate_service_requests\n    interval: 2s\n    count: 20\n    to:\n      topic: service_requests\n      keyType: string\n      valueType: json\n</code></pre> Circuit Breaker Processor - click to expand <pre><code># Circuit breaker pattern to prevent cascading failures\n\nstreams:\n  service_requests:\n    topic: service_requests\n    keyType: string\n    valueType: json\n\n  service_responses:\n    topic: service_responses\n    keyType: string\n    valueType: json\n\n  circuit_events:\n    topic: circuit_events\n    keyType: string\n    valueType: json\n\nfunctions:\n  circuit_breaker_handler:\n    type: valueTransformer\n    globalCode: |\n      # Circuit breaker state\n      failure_count = 0\n      success_count = 0\n      circuit_state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n      last_failure_time = 0\n\n      # Configuration\n      FAILURE_THRESHOLD = 3\n      SUCCESS_THRESHOLD = 2\n      TIMEOUT_MS = 10000  # 10 seconds\n\n    code: |\n      global failure_count, success_count, circuit_state, last_failure_time\n\n      import time\n      current_time = int(time.time() * 1000)\n\n      request_id = value.get(\"request_id\", \"unknown\")\n      should_succeed = value.get(\"should_succeed\", True)\n\n      # Check if we should transition from OPEN to HALF_OPEN\n      if circuit_state == \"OPEN\" and (current_time - last_failure_time) &gt; TIMEOUT_MS:\n        circuit_state = \"HALF_OPEN\"\n        success_count = 0\n\n      # Handle request based on circuit state\n      if circuit_state == \"OPEN\":\n        # Circuit is open - reject request immediately\n        result = {\n          \"request_id\": request_id,\n          \"status\": \"circuit_open\",\n          \"circuit_state\": circuit_state,\n          \"failure_count\": failure_count,\n          \"message\": \"Circuit breaker is OPEN - request rejected\"\n        }\n      else:\n        # Circuit is CLOSED or HALF_OPEN - try to process request\n        if should_succeed:\n          # Request succeeds\n          success_count += 1\n\n          if circuit_state == \"HALF_OPEN\" and success_count &gt;= SUCCESS_THRESHOLD:\n            # Reset circuit breaker\n            circuit_state = \"CLOSED\"\n            failure_count = 0\n            success_count = 0\n\n          result = {\n            \"request_id\": request_id,\n            \"status\": \"success\",\n            \"circuit_state\": circuit_state,\n            \"success_count\": success_count,\n            \"failure_count\": failure_count,\n            \"service\": value.get(\"service\")\n          }\n        else:\n          # Request fails\n          failure_count += 1\n          success_count = 0\n          last_failure_time = current_time\n\n          if failure_count &gt;= FAILURE_THRESHOLD:\n            circuit_state = \"OPEN\"\n\n          result = {\n            \"request_id\": request_id,\n            \"status\": \"failure\",\n            \"circuit_state\": circuit_state,\n            \"failure_count\": failure_count,\n            \"error_type\": value.get(\"error_type\", \"unknown\"),\n            \"service\": value.get(\"service\")\n          }\n\n    expression: result\n    resultType: json\n\npipelines:\n  circuit_breaker_processor:\n    from: service_requests\n    via:\n      - type: transformValue\n        mapper: circuit_breaker_handler\n      - type: peek\n        forEach:\n          code: |\n            status = value.get(\"status\", \"unknown\")\n            circuit_state = value.get(\"circuit_state\", \"unknown\")\n            req_id = value.get(\"request_id\", \"unknown\")\n\n            if status == \"circuit_open\":\n              log.warn(\"CIRCUIT OPEN - {}: Request rejected\", req_id)\n            elif status == \"success\":\n              log.info(\"REQUEST OK - {} [{}]: Success\", req_id, circuit_state)\n            else:\n              failures = value.get(\"failure_count\", 0)\n              log.error(\"REQUEST FAILED - {} [{}]: {} failures\", req_id, circuit_state, failures)\n    branch:\n      # Route successful requests\n      - if:\n          expression: value.get(\"status\") == \"success\"\n        to: service_responses\n      # Route circuit breaker events and failures\n      - to: circuit_events\n</code></pre> <p>What it does:</p> <ul> <li>Produces service requests: Creates requests with request_ids, some designed to fail to trigger circuit breaker state changes  </li> <li>Tracks circuit state: Uses global variables (failure_count, circuit_state) to maintain CLOSED/OPEN/HALF_OPEN states across all requests</li> <li>Opens on failures: After 3 consecutive failures, circuit transitions from CLOSED to OPEN state to protect failing service</li> <li>Rejects when open: In OPEN state, immediately returns circuit_open status without attempting processing, showing current failure count</li> <li>Processes requests: In CLOSED state, simulates service calls with success/failure scenarios, updating failure counter and circuit state accordingly</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"tutorials/intermediate/error-handling/#data-type-recommendations","title":"Data Type Recommendations","text":"<ul> <li>Use JSON types: Provides flexibility for error objects and easy inspection in Kowl UI</li> <li>Include context: Add timestamps, retry counts, and error classifications to all error messages</li> <li>Preserve original data: Keep original messages in error objects for debugging</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#function-design-patterns","title":"Function Design Patterns","text":"<ul> <li>Handle null values: Always check for <code>None</code>/<code>null</code> values explicitly</li> <li>Use appropriate exceptions: Choose specific exception types for different error conditions</li> <li>Provide meaningful errors: Include context about what went wrong and potential solutions</li> <li>Log appropriately: Use different log levels (info/warn/error) based on severity</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<ul> <li>Track error rates: Monitor error percentages by type and set appropriate thresholds</li> <li>Circuit breaker metrics: Alert when circuits open and track recovery times  </li> <li>Retry success rates: Measure effectiveness of retry strategies</li> <li>Dead letter queue size: Monitor unprocessable message volume</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#testing-error-scenarios","title":"Testing Error Scenarios","text":"<ul> <li>Simulate failures: Test error handling code with various failure scenarios</li> <li>Load testing: Ensure error handling works under high load conditions</li> <li>Recovery testing: Verify systems can recover from failure states</li> </ul>"},{"location":"tutorials/intermediate/error-handling/#error-pattern-selection-guide","title":"Error Pattern Selection Guide","text":"Pattern Use Case Benefits When to Use Validation &amp; Filtering Data quality issues Early detection, clear routing Input data validation, format checking Try-Catch Function-level errors Graceful degradation Type conversion, calculations, parsing Dead Letter Queue Permanent failures No data loss, failure analysis Malformed data, business rule violations Retry Strategies Transient failures Fault tolerance, automatic recovery Network timeouts, rate limits, temporary errors Circuit Breaker External service failures Prevents cascading failures API calls, database connections, service dependencies"},{"location":"tutorials/intermediate/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Stream Processing Operations Reference</li> <li>Function Types Reference</li> <li>Advanced Stream Processing Patterns</li> </ul>"},{"location":"tutorials/intermediate/joins/","title":"Implementing Joins in KSML","text":"<p>This tutorial explores how to implement join operations in KSML, allowing you to combine data from multiple streams or tables to create enriched datasets.</p>"},{"location":"tutorials/intermediate/joins/#introduction","title":"Introduction","text":"<p>Joins are fundamental operations in stream processing that combine data from multiple sources based on common keys. KSML provides three main types of joins, each serving different use cases in real-time data processing.</p> <p>KSML joins are built on top of Kafka Streams join operations, providing a YAML-based interface to powerful stream processing capabilities without requiring Java code.</p>"},{"location":"tutorials/intermediate/joins/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_clicks &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_purchases &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic correlated_user_actions &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_conversions &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_clicks_by_product &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_purchases_by_product &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic new_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_data &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic enriched_orders &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic orders_with_customer_data &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic product_catalog &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic orders_with_product_details &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_activity_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_location_data &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic activity_with_location &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_login_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_logout_events &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_session_analysis &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_profiles &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic customer_preferences &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic enriched_customer_data &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/joins/#core-join-concepts","title":"Core Join Concepts","text":""},{"location":"tutorials/intermediate/joins/#valuejoiner-functions","title":"ValueJoiner Functions","text":"<p>All join operations require a valueJoiner function to combine data from both sides:</p> <pre><code>valueJoiner:\n  type: valueJoiner\n  code: |\n    # value1: left stream/table value\n    # value2: right stream/table value\n    result = {\"combined\": value1, \"enriched\": value2}\n  expression: result\n  resultType: json\n</code></pre> <p>Requirements:</p> <ul> <li>Must include <code>expression</code> and <code>resultType</code> fields</li> <li>Handle null values gracefully (especially for left/outer joins)</li> <li>Return appropriate data structure for downstream processing</li> </ul>"},{"location":"tutorials/intermediate/joins/#co-partitioning-requirements","title":"Co-partitioning Requirements","text":"<p>Stream-stream and stream-table joins require co-partitioning:</p> <ul> <li>Same number of partitions in both topics</li> <li>Same partitioning strategy (how keys map to partitions)</li> <li>Same key type and serialization format</li> </ul> <p>GlobalTable joins don't require co-partitioning since data is replicated to all instances.</p>"},{"location":"tutorials/intermediate/joins/#types-of-joins-in-ksml","title":"Types of Joins in KSML","text":"<p>KSML supports three main categories of joins, each with specific characteristics and use cases:</p>"},{"location":"tutorials/intermediate/joins/#1-stream-stream-joins","title":"1. Stream-Stream Joins","text":"<p>Join two event streams within a time window to correlate related events.</p> <p>Kafka Streams equivalent: <code>KStream.join()</code>, <code>KStream.leftJoin()</code>, <code>KStream.outerJoin()</code></p> <p>When to use:</p> <ul> <li>Correlating events from different systems</li> <li>Tracking user behavior across multiple actions</li> <li>Detecting patterns that span multiple event types</li> </ul> <p>Key characteristics:</p> <ul> <li>Requires time windows for correlation</li> <li>Both streams must be co-partitioned</li> <li>Results are emitted when matching events occur within the window</li> <li>Supports inner, left, and outer join semantics</li> </ul>"},{"location":"tutorials/intermediate/joins/#2-stream-table-joins","title":"2. Stream-Table Joins","text":"<p>Enrich a stream of events with the latest state from a changelog table.</p> <p>Kafka Streams equivalent: <code>KStream.join()</code>, <code>KStream.leftJoin()</code> with KTable</p> <p>When to use:</p> <ul> <li>Enriching events with reference data</li> <li>Adding current state information to events</li> <li>Looking up the latest value for a key</li> </ul> <p>Key characteristics:</p> <ul> <li>Stream events are enriched with the latest table value</li> <li>Table provides point-in-time lookups</li> <li>Requires co-partitioning between stream and table</li> <li>Supports inner and left join semantics</li> </ul>"},{"location":"tutorials/intermediate/joins/#3-stream-globaltable-joins","title":"3. Stream-GlobalTable Joins","text":"<p>Enrich events using replicated reference data available on all instances.</p> <p>Kafka Streams equivalent: <code>KStream.join()</code>, <code>KStream.leftJoin()</code> with GlobalKTable</p> <p>When to use:</p> <ul> <li>Joining with reference data (product catalogs, configuration)</li> <li>Foreign key joins where keys don't match directly</li> <li>Avoiding co-partitioning requirements</li> </ul> <p>Key characteristics:</p> <ul> <li>GlobalTable is replicated to all application instances</li> <li>Supports foreign key extraction via mapper functions</li> <li>No co-partitioning required</li> <li>Supports inner and left join semantics</li> </ul>"},{"location":"tutorials/intermediate/joins/#stream-stream-join","title":"Stream-Stream Join","text":"<p>Stream-stream joins correlate events from two streams within a specified time window. This is essential for detecting patterns and relationships between different event types.</p>"},{"location":"tutorials/intermediate/joins/#use-case-user-behavior-analysis","title":"Use Case: User Behavior Analysis","text":"<p>Track user shopping behavior by correlating clicks and purchases within a 30-minute window to understand the customer journey.</p> <p>What it does:</p> <ul> <li>Produces two streams: Creates product_clicks (user clicks on products) and product_purchases (user buys products) with user_id keys and timestamps</li> <li>Joins with time window: Uses join operation with 30-minute timeDifference to correlate clicks with purchases that happen within 30 minutes</li> <li>Stores in window stores: Both streams use 60-minute window stores (2\u00d730min) with retainDuplicates=true to buffer events for correlation</li> <li>Combines with valueJoiner: When matching user_id found in both streams within time window, combines click and purchase data into single result</li> <li>Outputs correlations: Returns JSON showing both events with conversion analysis (did click lead to purchase, what products involved)</li> </ul> Producer: Clicks and Purchases (click to expand) <pre><code>functions:\n  generate_click:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      click_counter = 0\n      users = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n      products = [\"PROD001\", \"PROD002\", \"PROD003\", \"PROD004\", \"PROD005\"]\n    code: |\n      global click_counter, users, products\n\n      click_counter += 1\n      user_id = random.choice(users)\n\n      value = {\n        \"click_id\": f\"CLK{click_counter:04d}\",\n        \"user_id\": user_id,\n        \"product_id\": random.choice(products),\n        \"page\": \"product_detail\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      key = user_id\n    expression: (key, value)\n    resultType: (string, json)\n\n  generate_purchase:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      purchase_counter = 0\n      users = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n      products = [\"PROD001\", \"PROD002\", \"PROD003\", \"PROD004\", \"PROD005\"]\n    code: |\n      global purchase_counter, users, products\n\n      # Only generate purchases 30% of the time to simulate conversion\n      if random.random() &gt; 0.3:\n        # Return a proper tuple with None values to skip generation\n        key = None\n        value = None\n      else:\n        purchase_counter += 1\n        user_id = random.choice(users)\n\n        value = {\n          \"purchase_id\": f\"PUR{purchase_counter:04d}\",\n          \"user_id\": user_id,\n          \"product_id\": random.choice(products),\n          \"amount\": round(random.uniform(20.0, 300.0), 2),\n          \"timestamp\": int(time.time() * 1000)\n        }\n\n        key = user_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  click_producer:\n    generator: generate_click\n    interval: 1s\n    to:\n      topic: product_clicks\n      keyType: string\n      valueType: json\n\n  purchase_producer:\n    generator: generate_purchase\n    interval: 3s\n    to:\n      topic: product_purchases\n      keyType: string\n      valueType: json\n</code></pre> Processor: Stream-Stream Join (click to expand) <pre><code>streams:\n  product_clicks:\n    topic: product_clicks\n    keyType: string\n    valueType: json\n\n  product_purchases:\n    topic: product_purchases\n    keyType: string\n    valueType: json\n\n  correlated_user_actions:\n    topic: correlated_user_actions\n    keyType: string\n    valueType: json\n\nfunctions:\n  correlate_click_and_purchase:\n    type: valueJoiner\n    code: |\n      result = {}\n\n      # Add click data\n      if value1 is not None:\n        result[\"click\"] = value1\n\n      # Add purchase data\n      if value2 is not None:\n        result[\"purchase\"] = value2\n\n      # Calculate conversion if both exist\n      if value1 is not None and value2 is not None:\n        result[\"converted\"] = True\n        click_time = value1.get(\"timestamp\", 0)\n        purchase_time = value2.get(\"timestamp\", 0)\n        result[\"conversion_time_ms\"] = purchase_time - click_time\n      else:\n        result[\"converted\"] = False\n\n      new_value = result\n    expression: new_value\n    resultType: json\n\npipelines:\n  match_clicks_with_purchases:\n    from: product_clicks\n    via:\n      - type: join\n        stream: product_purchases\n        valueJoiner: correlate_click_and_purchase\n        timeDifference: 30m  # Look for purchases within 30 minutes of a click\n        grace: 5m  # Grace period for late events\n        thisStore:\n          name: clicks_join_store\n          type: window\n          windowSize: 60m  # Must be 2*timeDifference\n          retention: 65m   # Must be 2*timeDifference + grace = 60m + 5m\n          retainDuplicates: true\n        otherStore:\n          name: purchases_join_store\n          type: window\n          windowSize: 60m  # Must be 2*timeDifference\n          retention: 65m   # Must be 2*timeDifference + grace = 60m + 5m\n          retainDuplicates: true\n      - type: peek\n        forEach:\n          code: log.info(\"CORRELATION - user={}, converted={}, click_product={}, purchase_product={}\", key, value.get(\"converted\"), value.get(\"click\", {}).get(\"product_id\"), value.get(\"purchase\", {}).get(\"product_id\"))\n    to: correlated_user_actions\n</code></pre>"},{"location":"tutorials/intermediate/joins/#key-configuration-points","title":"Key Configuration Points","text":"<p>The aim here is to show how time windows must be used to correlate events from different streams. The configuration demonstrates:</p> <ul> <li>timeDifference: 30m - Maximum time gap between correlated events</li> <li>Window Stores: Both streams need stores with <code>retainDuplicates: true</code></li> <li>Window Size: Must be <code>2 \u00d7 timeDifference</code> (60m) to buffer events from both streams</li> <li>Retention: Must be <code>2 \u00d7 timeDifference + grace</code> (65m) for proper state cleanup</li> <li>Grace Period: 5m allowance for late-arriving events</li> </ul> <p>This configuration ensures events are only correlated within a reasonable time frame while managing memory efficiently.</p>"},{"location":"tutorials/intermediate/joins/#stream-table-join","title":"Stream-Table Join","text":"<p>Stream-table joins enrich streaming events with the latest state from a changelog table. This pattern is common for adding reference data to events.</p>"},{"location":"tutorials/intermediate/joins/#use-case-order-enrichment","title":"Use Case: Order Enrichment","text":"<p>Enrich order events with customer information by joining the orders stream with a customers table.</p> <p>What it does:</p> <ul> <li>Produces orders and customers: Creates order stream (keyed by order_id) and customer table (keyed by customer_id) with matching customer references</li> <li>Rekeys for join: Transforms order key from order_id to customer_id using extract_customer_id function to enable join with customers table</li> <li>Joins stream with table: Uses join operation to combine each order with latest customer data from customers table based on customer_id</li> <li>Combines data: valueJoiner function merges order details with customer information into enriched result containing both datasets</li> <li>Restores original key: Transforms key back from customer_id to order_id using restore_order_key for downstream processing consistency</li> </ul> Producer: Orders (click to expand) <pre><code>functions:\n  generate_order:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      order_counter = 0\n      customers = [\"CUST001\", \"CUST002\", \"CUST003\", \"CUST004\", \"CUST005\"]\n      products = [\"PROD001\", \"PROD002\", \"PROD003\", \"PROD004\", \"PROD005\"]\n    code: |\n      global order_counter, customers, products\n\n      order_counter += 1\n      order_id = f\"ORD{order_counter:04d}\"\n      customer_id = random.choice(customers)\n\n      # Generate order\n      value = {\n        \"order_id\": order_id,\n        \"customer_id\": customer_id,\n        \"product_id\": random.choice(products),\n        \"quantity\": random.randint(1, 5),\n        \"amount\": round(random.uniform(10.0, 200.0), 2),\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use order_id as key (natural key for orders)\n      key = order_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_order\n    interval: 2s\n    to:\n      topic: new_orders\n      keyType: string\n      valueType: json\n</code></pre> Producer: Customers (click to expand) <pre><code>functions:\n  generate_customer:\n    type: generator\n    globalCode: |\n      import random\n      customers_data = [\n        {\"id\": \"CUST001\", \"name\": \"Alice Johnson\", \"email\": \"alice@email.com\", \"region\": \"US-WEST\"},\n        {\"id\": \"CUST002\", \"name\": \"Bob Smith\", \"email\": \"bob@email.com\", \"region\": \"US-EAST\"},\n        {\"id\": \"CUST003\", \"name\": \"Carol Davis\", \"email\": \"carol@email.com\", \"region\": \"EU-WEST\"},\n        {\"id\": \"CUST004\", \"name\": \"David Wilson\", \"email\": \"david@email.com\", \"region\": \"ASIA-PACIFIC\"},\n        {\"id\": \"CUST005\", \"name\": \"Eva Brown\", \"email\": \"eva@email.com\", \"region\": \"EU-CENTRAL\"}\n      ]\n      customer_index = 0\n    code: |\n      global customers_data, customer_index\n\n      # Cycle through customers\n      customer = customers_data[customer_index % len(customers_data)]\n      customer_index += 1\n\n      value = {\n        \"name\": customer[\"name\"],\n        \"email\": customer[\"email\"],\n        \"region\": customer[\"region\"],\n        \"status\": \"active\"\n      }\n\n      key = customer[\"id\"]\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  customer_producer:\n    generator: generate_customer\n    interval: 10s\n    to:\n      topic: customer_data\n      keyType: string\n      valueType: json\n</code></pre> Processor: Stream-Table Join (click to expand) <pre><code>streams:\n  orders:\n    topic: new_orders\n    keyType: string  # Order ID\n    valueType: json  # Order details\n\n  enriched_orders:\n    topic: orders_with_customer_data\n    keyType: string  # Order ID\n    valueType: json  # Combined order and customer data\n\ntables:\n  customers:\n    topic: customer_data\n    keyType: string  # Customer ID\n    valueType: json  # Customer details\n\nfunctions:\n  extract_customer_id:\n    type: keyTransformer\n    code: |\n      # Extract customer_id from order to use as key for join\n      new_key = value.get(\"customer_id\") if value else None\n    expression: new_key\n    resultType: string\n\n  join_order_with_customer:\n    type: valueJoiner\n    code: |\n      # Combine order and customer information\n      result = {}\n\n      # Add order details\n      if value1 is not None:\n        result.update(value1)\n\n      # Add customer details\n      if value2 is not None:\n        result[\"customer\"] = value2\n\n      new_value = result\n    expression: new_value\n    resultType: json\n\n  restore_order_key:\n    type: keyTransformer\n    code: |\n      # Restore order_id as key after join\n      new_key = value.get(\"order_id\") if value else None\n    expression: new_key\n    resultType: string\n\npipelines:\n  enrich_orders:\n    from: orders\n    via:\n      # Rekey to customer_id for join\n      - type: transformKey\n        mapper: extract_customer_id\n      # Join with customers table\n      - type: join\n        table: customers\n        valueJoiner: join_order_with_customer\n      # Rekey back to order_id\n      - type: transformKey\n        mapper: restore_order_key\n      - type: peek\n        forEach:\n          code: log.info(\"ENRICHED ORDER - key={}, order_id={}, customer={}\", key, value.get(\"order_id\"), value.get(\"customer\", {}).get(\"name\"))\n    to: enriched_orders\n</code></pre>"},{"location":"tutorials/intermediate/joins/#rekeying-pattern","title":"Rekeying Pattern","text":"<p>The rekeying pattern is essential when join keys don't match naturally:</p> <ul> <li>Use <code>transformKey</code> to extract the join key from the stream</li> <li>Perform the join operation</li> <li>Optionally restore the original key for downstream consistency</li> </ul>"},{"location":"tutorials/intermediate/joins/#stream-globaltable-join","title":"Stream-GlobalTable Join","text":"<p>GlobalTable joins enable enrichment with reference data that's replicated across all instances, supporting foreign key relationships.</p>"},{"location":"tutorials/intermediate/joins/#use-case-product-catalog-enrichment","title":"Use Case: Product Catalog Enrichment","text":"<p>Enrich orders with product details using a foreign key join with a global product catalog.</p> <p>What it does:</p> <ul> <li>Produces orders and products: Creates order stream with product_id references and product_catalog globalTable with product details  </li> <li>Uses foreign key mapper: Extracts product_id from order value using keyValueMapper to look up in product_catalog globalTable</li> <li>Joins with globalTable: No co-partitioning needed - globalTable replicated to all instances, joins orders with product details by product_id</li> <li>Enriches with product data: Combines order information with product details (name, price, category) from catalog</li> <li>Outputs enriched orders: Returns orders enhanced with full product information for downstream processing</li> </ul> Producer: Orders and Products (click to expand) <pre><code>functions:\n  generate_order:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      order_counter = 0\n      customers = [\"CUST001\", \"CUST002\", \"CUST003\", \"CUST004\", \"CUST005\"]\n      products = [\"PROD001\", \"PROD002\", \"PROD003\", \"PROD004\", \"PROD005\"]\n    code: |\n      global order_counter, customers, products\n\n      order_counter += 1\n      order_id = f\"ORD{order_counter:04d}\"\n      customer_id = random.choice(customers)\n      product_id = random.choice(products)\n\n      # Generate order with product_id for foreign key join\n      value = {\n        \"order_id\": order_id,\n        \"customer_id\": customer_id,\n        \"product_id\": product_id,\n        \"quantity\": random.randint(1, 5),\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      # Use order_id as key (natural key for orders)\n      key = order_id\n    expression: (key, value)\n    resultType: (string, json)\n\n  generate_product:\n    type: generator\n    globalCode: |\n      import random\n      products_data = [\n        {\"id\": \"PROD001\", \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99},\n        {\"id\": \"PROD002\", \"name\": \"Headphones\", \"category\": \"Electronics\", \"price\": 149.99},\n        {\"id\": \"PROD003\", \"name\": \"Coffee Maker\", \"category\": \"Appliances\", \"price\": 79.99},\n        {\"id\": \"PROD004\", \"name\": \"Backpack\", \"category\": \"Accessories\", \"price\": 49.99},\n        {\"id\": \"PROD005\", \"name\": \"Desk Lamp\", \"category\": \"Furniture\", \"price\": 39.99}\n      ]\n      product_index = 0\n    code: |\n      global products_data, product_index\n\n      # Cycle through products to populate the global table\n      product = products_data[product_index % len(products_data)]\n      product_index += 1\n\n      value = {\n        \"name\": product[\"name\"],\n        \"category\": product[\"category\"],\n        \"price\": product[\"price\"],\n        \"in_stock\": True\n      }\n\n      # Product ID as key for the global table\n      key = product[\"id\"]\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  order_producer:\n    generator: generate_order\n    interval: 2s\n    to:\n      topic: new_orders\n      keyType: string\n      valueType: json\n\n  product_producer:\n    generator: generate_product\n    interval: 10s\n    to:\n      topic: product_catalog\n      keyType: string\n      valueType: json\n</code></pre> Processor: Foreign Key Join (click to expand) <pre><code>streams:\n  orders:\n    topic: new_orders\n    keyType: string  # Order ID\n    valueType: json  # Order details including product_id\n\n  orders_with_product_details:\n    topic: orders_with_product_details\n    keyType: string  # Order ID\n    valueType: json  # Order enriched with product information\n\nglobalTables:\n  products:\n    topic: product_catalog\n    keyType: string  # Product ID\n    valueType: json  # Product details\n\nfunctions:\n  extract_product_id:\n    type: keyValueMapper\n    code: |\n      # Map from order (key, value) to product_id for join\n      product_id = value.get(\"product_id\") if value else None\n    expression: product_id\n    resultType: string\n\n  join_order_with_product:\n    type: valueJoiner\n    code: |\n      # Combine order and product information\n      result = {}\n\n      # Add order details\n      if value1 is not None:\n        result.update(value1)\n\n      # Add product details\n      if value2 is not None:\n        result[\"product_details\"] = value2\n        # Calculate total price\n        quantity = value1.get(\"quantity\", 0) if value1 else 0\n        price = value2.get(\"price\", 0) if value2 else 0\n        result[\"total_price\"] = quantity * price\n\n      new_value = result\n    expression: new_value\n    resultType: json\n\npipelines:\n  enrich_orders_with_products:\n    from: orders\n    via:\n      - type: join\n        globalTable: products\n        mapper: extract_product_id\n        valueJoiner: join_order_with_product\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"ENRICHED ORDER - key={}, order_id={}, product={}, total_price={}\", \n                     key, \n                     value.get(\"order_id\"), \n                     value.get(\"product_details\", {}).get(\"name\"),\n                     value.get(\"total_price\"))\n    to: orders_with_product_details\n</code></pre>"},{"location":"tutorials/intermediate/joins/#foreign-key-extraction","title":"Foreign Key Extraction","text":"<p>The <code>mapper</code> function extracts the foreign key from stream records:</p> <ul> <li>Function Type: <code>keyValueMapper</code> (not <code>foreignKeyExtractor</code>)</li> <li>Input: Stream's key and value</li> <li>Output: Key to lookup in the GlobalTable</li> <li>Example: Extract product_id from order to join with product catalog</li> </ul>"},{"location":"tutorials/intermediate/joins/#stream-table-left-join","title":"Stream-Table Left Join","text":"<p>Left joins preserve all records from the stream (left side) while adding data from the table (right side) when available. This is useful for enriching events with optional reference data.</p>"},{"location":"tutorials/intermediate/joins/#use-case-activity-enrichment-with-location","title":"Use Case: Activity Enrichment with Location","text":"<p>Enrich user activity events with location data, preserving all activities even when location information is missing.</p> <p>What it does:</p> <ul> <li>Produces activities and locations: Creates user_activity_events stream and user_location_data table, with some users having location data, others not</li> <li>Uses leftJoin: Joins activity stream with location table - preserves all activities even when no location data exists for user</li> <li>Handles missing data: valueJoiner receives null for location when user not in location table, includes null-check logic</li> <li>Enriches optionally: Adds location data when available, leaves location fields empty/null when not available  </li> <li>Preserves all activities: All activity events flow through to output regardless of whether location data exists, maintaining complete activity stream</li> </ul> Producer: User Activity and Locations (click to expand) <pre><code>functions:\n  generate_user_activity:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      activity_counter = 0\n    code: |\n      global activity_counter\n\n      user_ids = [\"user001\", \"user002\", \"user003\", \"user004\", \"user005\"]\n      activity_types = [\"login\", \"logout\", \"page_view\", \"purchase\", \"search\"]\n\n      user_id = random.choice(user_ids)\n      activity = {\n        \"activity_id\": f\"activity_{activity_counter+1:03d}\",\n        \"user_id\": user_id,\n        \"activity_type\": random.choice(activity_types),\n        \"timestamp\": int(time.time() * 1000),\n        \"session_id\": f\"session_{user_id}_{random.randint(1, 5)}\"\n      }\n\n      activity_counter += 1\n      result = (user_id, activity)\n    expression: result\n    resultType: (string, json)\n\n  generate_user_locations:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      location_counter = 0\n      user_ids = [\"user001\", \"user002\", \"user003\", \"user005\"]  # Note: user004 missing\n      locations = [\n        {\"country\": \"USA\", \"city\": \"New York\", \"timezone\": \"EST\"},\n        {\"country\": \"UK\", \"city\": \"London\", \"timezone\": \"GMT\"},\n        {\"country\": \"Germany\", \"city\": \"Berlin\", \"timezone\": \"CET\"},\n        {\"country\": \"Japan\", \"city\": \"Tokyo\", \"timezone\": \"JST\"}\n      ]\n    code: |\n      global location_counter\n\n      if location_counter &lt; len(user_ids):\n        user_id = user_ids[location_counter]\n        location = random.choice(locations)\n        location_data = {\n          \"user_id\": user_id,\n          \"country\": location[\"country\"],\n          \"city\": location[\"city\"],\n          \"timezone\": location[\"timezone\"],\n          \"updated_at\": int(time.time() * 1000)\n        }\n\n        location_counter += 1\n        result = (user_id, location_data)\n      else:\n        result = None\n    expression: result\n    resultType: (string, json)\n\nproducers:\n  user_activity_producer:\n    generator: generate_user_activity\n    interval: 1s\n    to:\n      topic: user_activity_events\n      keyType: string\n      valueType: json\n\n  user_location_producer:\n    generator: generate_user_locations\n    interval: 2s\n    count: 4  # Only 4 users have location data\n    to:\n      topic: user_location_data\n      keyType: string\n      valueType: json\n</code></pre> Processor: Stream-Table Left Join (click to expand) <pre><code>streams:\n  user_activity:\n    topic: user_activity_events\n    keyType: string  # User ID\n    valueType: json  # Activity details\n\n  enriched_activity:\n    topic: activity_with_location\n    keyType: string  # User ID\n    valueType: json  # Activity enriched with location\n\ntables:\n  user_locations:\n    topic: user_location_data\n    keyType: string  # User ID\n    valueType: json  # Location details\n\nfunctions:\n  join_activity_with_location:\n    type: valueJoiner\n    globalCode: |\n      import time\n    code: |\n      # Left join: always preserve activity data, add location if available\n      result = {}\n\n      # Always include activity data (left side)\n      if value1 is not None:\n        result.update(value1)\n\n      # Add location data if available (right side)\n      if value2 is not None:\n        result[\"location\"] = {\n          \"country\": value2.get(\"country\"),\n          \"city\": value2.get(\"city\"),\n          \"timezone\": value2.get(\"timezone\")\n        }\n      else:\n        # Location not available for this user\n        result[\"location\"] = {\n          \"country\": \"UNKNOWN\",\n          \"city\": \"UNKNOWN\", \n          \"timezone\": \"UTC\"\n        }\n\n      # Add enrichment metadata\n      result[\"enriched\"] = value2 is not None\n      result[\"enriched_at\"] = int(time.time() * 1000)\n\n      new_value = result\n    expression: new_value\n    resultType: json\n\npipelines:\n  enrich_activity_with_location:\n    from: user_activity\n    via:\n      # Left join with location table - preserves all activities\n      - type: leftJoin\n        table: user_locations\n        valueJoiner: join_activity_with_location\n      - type: peek\n        forEach:\n          code: log.info(\"ENRICHED ACTIVITY - user={}, activity={}, location={}/{}, enriched={}\", key, value.get(\"activity_type\"), value.get(\"location\", {}).get(\"city\"), value.get(\"location\", {}).get(\"country\"), value.get(\"enriched\"))\n    to: enriched_activity\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Left join semantics: All activity events are preserved, even when location data is missing</li> <li>Graceful null handling: Unknown locations get default values instead of causing errors</li> <li>Enrichment metadata: Tracks whether enrichment data was available</li> <li>Real-world pattern: Common scenario where reference data may be incomplete</li> </ul> <p>Expected Behavior:</p> <ul> <li>Activities for users with location data are enriched with country/city information</li> <li>Activities for users without location data get \"UNKNOWN\" placeholders</li> <li>All activities are preserved regardless of location availability</li> </ul>"},{"location":"tutorials/intermediate/joins/#stream-stream-outer-join","title":"Stream-Stream Outer Join","text":"<p>Outer joins capture events from either stream, making them ideal for tracking incomplete or partial interactions between two event types.</p>"},{"location":"tutorials/intermediate/joins/#use-case-loginlogout-session-analysis","title":"Use Case: Login/Logout Session Analysis","text":"<p>Track user sessions by correlating login and logout events, capturing incomplete sessions where only one event type occurs.</p> <p>What it does:</p> <ul> <li>Produces login and logout events: Creates separate streams for user_login_events and user_logout_events with overlapping but not always matching users</li> <li>Uses outerJoin: Correlates events within 10-minute window, captures login-only, logout-only, and complete login+logout sessions</li> <li>Handles three scenarios: Complete sessions (both events), LOGIN_ONLY (user logged in, no logout captured), LOGOUT_ONLY (logout without login)</li> <li>Calculates session data: For complete sessions computes session duration, for incomplete sessions identifies the pattern and timing</li> <li>Outputs all patterns: Returns session analysis showing complete sessions with duration, or incomplete sessions with pattern classification</li> </ul> Producer: Login and Logout Events (click to expand) <pre><code>functions:\n  generate_login_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      login_counter = 0\n    code: |\n      global login_counter\n\n      user_ids = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"]\n      user_id = random.choice(user_ids)\n\n      login_event = {\n        \"event_id\": f\"login_{login_counter+1:03d}\",\n        \"user_id\": user_id,\n        \"login_time\": int(time.time() * 1000),\n        \"device\": random.choice([\"mobile\", \"desktop\", \"tablet\"]),\n        \"ip_address\": f\"192.168.1.{random.randint(1, 255)}\"\n      }\n\n      login_counter += 1\n      result = (user_id, login_event)\n    expression: result\n    resultType: (string, json)\n\n  generate_logout_events:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      logout_counter = 0\n    code: |\n      global logout_counter\n\n      user_ids = [\"alice\", \"bob\", \"charlie\", \"frank\", \"grace\"]  # Some different users\n      user_id = random.choice(user_ids)\n\n      logout_event = {\n        \"event_id\": f\"logout_{logout_counter+1:03d}\",\n        \"user_id\": user_id,\n        \"logout_time\": int(time.time() * 1000),\n        \"session_duration\": random.randint(60, 7200)  # 1 min to 2 hours\n      }\n\n      logout_counter += 1\n      result = (user_id, logout_event)\n    expression: result\n    resultType: (string, json)\n\nproducers:\n  login_producer:\n    generator: generate_login_events\n    interval: 2s\n    to:\n      topic: user_login_events\n      keyType: string\n      valueType: json\n\n  logout_producer:\n    generator: generate_logout_events\n    interval: 3s\n    to:\n      topic: user_logout_events\n      keyType: string\n      valueType: json\n</code></pre> Processor: Stream-Stream Outer Join (click to expand) <pre><code>streams:\n  user_logins:\n    topic: user_login_events\n    keyType: string\n    valueType: json\n\n  user_logouts:\n    topic: user_logout_events\n    keyType: string\n    valueType: json\n\n  session_analysis:\n    topic: user_session_analysis\n    keyType: string\n    valueType: json\n\nfunctions:\n  analyze_user_session:\n    type: valueJoiner\n    globalCode: |\n      import time\n    code: |\n      # Outer join: capture login-only, logout-only, and matched events\n      result = {}\n\n      # Determine session type based on what data is available\n      if value1 is not None and value2 is not None:\n        # Both login and logout available - complete session\n        result = {\n          \"session_type\": \"COMPLETE\",\n          \"user_id\": value1.get(\"user_id\"),\n          \"login_event\": value1,\n          \"logout_event\": value2,\n          \"session_duration\": value2.get(\"session_duration\"),\n          \"device\": value1.get(\"device\"),\n          \"login_time\": value1.get(\"login_time\"),\n          \"logout_time\": value2.get(\"logout_time\")\n        }\n      elif value1 is not None:\n        # Login only - user still active or logout not captured\n        result = {\n          \"session_type\": \"LOGIN_ONLY\",\n          \"user_id\": value1.get(\"user_id\"),\n          \"login_event\": value1,\n          \"logout_event\": None,\n          \"device\": value1.get(\"device\"),\n          \"login_time\": value1.get(\"login_time\"),\n          \"status\": \"active_or_missing_logout\"\n        }\n      elif value2 is not None:\n        # Logout only - login not captured or user was already logged in\n        result = {\n          \"session_type\": \"LOGOUT_ONLY\",\n          \"user_id\": value2.get(\"user_id\"),\n          \"login_event\": None,\n          \"logout_event\": value2,\n          \"session_duration\": value2.get(\"session_duration\"),\n          \"logout_time\": value2.get(\"logout_time\"),\n          \"status\": \"missing_login_or_already_active\"\n        }\n\n      # Add analysis metadata\n      result[\"analyzed_at\"] = int(time.time() * 1000)\n\n      new_value = result\n    expression: new_value\n    resultType: json\n\npipelines:\n  analyze_user_sessions:\n    from: user_logins\n    via:\n      # Outer join to capture all login and logout events\n      - type: outerJoin\n        stream: user_logouts\n        valueJoiner: analyze_user_session\n        timeDifference: 10m  # Look for logouts within 10 minutes of login\n        grace: 2m  # Grace period for late events\n        thisStore:\n          name: login_session_store\n          type: window\n          windowSize: 20m  # Must be 2*timeDifference\n          retention: 22m   # Must be 2*timeDifference + grace\n          retainDuplicates: true\n        otherStore:\n          name: logout_session_store\n          type: window\n          windowSize: 20m  # Must be 2*timeDifference\n          retention: 22m   # Must be 2*timeDifference + grace\n          retainDuplicates: true\n      - type: peek\n        forEach:\n          code: log.info(\"SESSION ANALYSIS - user={}, type={}, device={}, duration={}s\", key, value.get(\"session_type\"), value.get(\"device\", \"N/A\"), value.get(\"session_duration\", \"N/A\"))\n    to: session_analysis\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Outer join semantics: Captures login-only, logout-only, and complete sessions</li> <li>Session analysis: Categorizes different session patterns for analytics</li> <li>Time window correlation: Uses 10-minute window to correlate related events</li> <li>Business insights: Identifies users who login but don't logout (active sessions) or logout without captured login</li> </ul> <p>Expected Behavior:</p> <ul> <li>COMPLETE sessions: When both login and logout events occur within the time window</li> <li>LOGIN_ONLY sessions: Users who logged in but no logout was captured (active sessions)</li> <li>LOGOUT_ONLY sessions: Logout events without corresponding login (users already logged in)</li> </ul>"},{"location":"tutorials/intermediate/joins/#join-type-variants","title":"Join Type Variants","text":"<p>Each join type supports different semantics for handling missing matches:</p>"},{"location":"tutorials/intermediate/joins/#inner-joins","title":"Inner Joins","text":"<p><pre><code>type: join  # Default inner join\n</code></pre> Produces output only when both sides have matching keys.</p>"},{"location":"tutorials/intermediate/joins/#left-joins","title":"Left Joins","text":"<p><pre><code>type: leftJoin\n</code></pre> Always produces output for the left side (stream), with null for missing right side values.</p>"},{"location":"tutorials/intermediate/joins/#outer-joins-stream-stream-only","title":"Outer Joins (Stream-Stream only)","text":"<p><pre><code>type: outerJoin\n</code></pre> Produces output whenever either side has data, with null for missing values.</p> <p>Note: Table and GlobalTable joins don't support outer joins since tables represent current state, not events.</p>"},{"location":"tutorials/intermediate/joins/#performance-considerations","title":"Performance Considerations","text":""},{"location":"tutorials/intermediate/joins/#state-management","title":"State Management","text":"<ul> <li>Window sizes: Larger windows consume more memory but capture more correlations</li> <li>Retention periods: Balance between late data handling and resource usage</li> <li>Grace periods: Allow late arrivals while managing state cleanup</li> </ul>"},{"location":"tutorials/intermediate/joins/#topology-optimization","title":"Topology Optimization","text":"<ul> <li>Join order: Join with smaller datasets first when chaining multiple joins</li> <li>GlobalTable usage: Use for frequently accessed reference data to avoid repartitioning</li> <li>Rekeying overhead: Minimize unnecessary rekeying operations</li> </ul>"},{"location":"tutorials/intermediate/joins/#conclusion","title":"Conclusion","text":"<p>KSML's join operations enable powerful data enrichment patterns:</p> <ul> <li>Stream-stream joins correlate events within time windows</li> <li>Stream-table joins enrich events with current state</li> <li>Stream-GlobalTable joins provide foreign key lookups without co-partitioning</li> </ul> <p>Choose the appropriate join type based on your data characteristics and business requirements.</p>"},{"location":"tutorials/intermediate/joins/#further-reading","title":"Further Reading","text":"<ul> <li>Reference: Join Operations</li> </ul>"},{"location":"tutorials/intermediate/state-stores/","title":"State Stores","text":"<p>State stores are persistent or in-memory storage components used by stateful operations in Kafka Streams to maintain intermediate results, aggregations, and other stateful data. KSML provides flexible configuration options for state stores, allowing you to optimize performance, durability, and storage characteristics for your specific use cases.</p>"},{"location":"tutorials/intermediate/state-stores/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_ownership_data &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic owner_sensor_counts &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_type_totals &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic owner_counts &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/state-stores/#understanding-state-stores-in-kafka-streams","title":"Understanding State Stores in Kafka Streams","text":"<p>State stores serve several critical purposes in stream processing:</p> <ul> <li>Aggregations: Store running totals, counts, averages, and other aggregate calculations</li> <li>Windowing: Maintain data within time windows for temporal analysis</li> <li>Joins: Cache data from one stream to join with another</li> <li>Deduplication: Track processed records to eliminate duplicates</li> <li>State Management: Persist application state across restarts</li> </ul> <p>State stores can be backed by RocksDB (persistent) or kept in memory (non-persistent). Both types can optionally use changelog topics for fault tolerance and recovery.</p>"},{"location":"tutorials/intermediate/state-stores/#state-store-types","title":"State Store Types","text":"<p>KSML supports three types of state stores. For detailed information about each type and their parameters, see the State Store Reference.</p>"},{"location":"tutorials/intermediate/state-stores/#configuration-methods","title":"Configuration Methods","text":"<p>KSML provides two ways to configure state stores:</p>"},{"location":"tutorials/intermediate/state-stores/#1-predefined-store-configuration","title":"1. Predefined Store Configuration","text":"<p>Define stores in the global <code>stores</code> section and reference them by name:</p> Sensor Ownership Producer (click to expand) <pre><code>functions:\n  generate_sensor_data:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      data_counter = 0\n      owners = []\n      owners.extend([\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"])\n      sensor_types = []\n      sensor_types.extend([\"temperature\", \"humidity\", \"pressure\", \"light\"])\n    code: |\n      global data_counter, owners, sensor_types\n\n      data_counter += 1\n      owner = random.choice(owners)\n      sensor_type = random.choice(sensor_types)\n      sensor_id = sensor_type + \"_\" + owner + \"_\" + str(random.randint(1, 3)).zfill(2)\n\n      value = {}\n      value[\"sensor_id\"] = sensor_id\n      value[\"owner\"] = owner\n      value[\"sensor_type\"] = sensor_type\n      value[\"value\"] = round(random.uniform(10.0, 100.0), 2)\n      value[\"timestamp\"] = int(time.time() * 1000)\n\n      key = sensor_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_data\n    interval: 2s\n    to:\n      topic: sensor_ownership_data\n      keyType: string\n      valueType: json\n</code></pre> Predefined Store Processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# This example demonstrates predefined state store configuration for counting sensors by owner\n\nstreams:\n  sensor_source:\n    topic: sensor_ownership_data\n    keyType: string\n    valueType: json\n    offsetResetPolicy: latest\n\n  owner_counts:\n    topic: owner_sensor_counts\n    keyType: string\n    valueType: long\n\nstores:\n  owner_count_store:\n    type: keyValue\n    keyType: string\n    valueType: long\n    caching: false\n    persistent: true\n    logging: true\n\npipelines:\n  count_by_owner:\n    from: sensor_source\n    via:\n      - type: groupBy\n        name: group_by_owner\n        mapper:\n          code: |\n            if value is None:\n              return \"unknown\"\n            if not \"owner\" in value:\n              return \"unknown\"\n          expression: value[\"owner\"]\n          resultType: string\n      - type: count\n        store: owner_count_store\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"PREDEFINED STORE - Owner: {}, sensor count: {}\", key, value)\n    to: owner_counts\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Predefined store configuration: The <code>owner_count_store</code> is defined in the global <code>stores</code> section</li> <li>Store reference: The <code>count</code> operation references the predefined store by name</li> <li>Persistent storage: Data survives application restarts (<code>persistent: true</code>)</li> <li>Changelog logging: State changes are replicated to a Kafka changelog topic for fault tolerance and recovery (<code>logging: true</code>)</li> </ul> <p>You won't be able to read the output in <code>owner_sensor_counts</code> topic because it is binary, to read it you can consume the messages like this:</p> <pre><code># Read owner sensor counts (Long values require specific deserializer)\ndocker exec broker /opt/bitnami/kafka/bin/kafka-console-consumer.sh \\\n  --bootstrap-server broker:9093 \\\n  --topic owner_sensor_counts \\\n  --from-beginning \\\n  --key-deserializer org.apache.kafka.common.serialization.StringDeserializer \\\n  --value-deserializer org.apache.kafka.common.serialization.LongDeserializer \\\n  --property print.key=true \\\n  --property key.separator=\": \" \\\n  --timeout-ms 10000\n</code></pre>"},{"location":"tutorials/intermediate/state-stores/#2-inline-store-configuration","title":"2. Inline Store Configuration","text":"<p>Define stores directly within operations for single-use scenarios:</p> Sensor Ownership Producer (click to expand) <pre><code>functions:\n  generate_sensor_data:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      data_counter = 0\n      owners = []\n      owners.extend([\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"])\n      sensor_types = []\n      sensor_types.extend([\"temperature\", \"humidity\", \"pressure\", \"light\"])\n    code: |\n      global data_counter, owners, sensor_types\n\n      data_counter += 1\n      owner = random.choice(owners)\n      sensor_type = random.choice(sensor_types)\n      sensor_id = sensor_type + \"_\" + owner + \"_\" + str(random.randint(1, 3)).zfill(2)\n\n      value = {}\n      value[\"sensor_id\"] = sensor_id\n      value[\"owner\"] = owner\n      value[\"sensor_type\"] = sensor_type\n      value[\"value\"] = round(random.uniform(10.0, 100.0), 2)\n      value[\"timestamp\"] = int(time.time() * 1000)\n\n      key = sensor_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_data\n    interval: 2s\n    to:\n      topic: sensor_ownership_data\n      keyType: string\n      valueType: json\n</code></pre> Inline Store Processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\n# This example demonstrates inline state store configuration for aggregating sensor data by type\n\nstreams:\n  sensor_source:\n    topic: sensor_ownership_data\n    keyType: string\n    valueType: json\n\n  sensor_type_totals:\n    topic: sensor_type_totals\n    keyType: string\n    valueType: json\n\nfunctions:\n  initialize_sum:\n    type: initializer\n    code: |\n      result = {\"total_value\": 0.0, \"count\": 0}\n    expression: result\n    resultType: json\n\n  update_sum:\n    type: aggregator\n    code: |\n      # Initialize if first time\n      if aggregatedValue is None:\n        aggregatedValue = {}\n        aggregatedValue[\"total_value\"] = 0.0\n        aggregatedValue[\"count\"] = 0\n\n      # Update totals\n      aggregatedValue[\"total_value\"] += value.get(\"value\", 0.0)\n      aggregatedValue[\"count\"] += 1\n\n      # Calculate average\n      aggregatedValue[\"average\"] = round(aggregatedValue[\"total_value\"] / aggregatedValue[\"count\"], 2)\n\n      result = aggregatedValue\n    expression: result\n    resultType: json\n\npipelines:\n  aggregate_by_sensor_type:\n    from: sensor_source\n    via:\n      - type: groupBy\n        name: group_by_sensor_type\n        mapper:\n          code: |\n            if value is None:\n              return \"unknown\"  \n            if not \"sensor_type\" in value:\n              return \"unknown\"\n          expression: value[\"sensor_type\"]\n          resultType: string\n      - type: aggregate\n        store:\n          name: sensor_type_aggregates\n          type: keyValue\n          caching: true\n          persistent: false\n          logging: false\n        initializer: initialize_sum\n        aggregator: update_sum\n      - type: toStream\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"INLINE STORE - Type: {}, avg: {}, count: {}\", key, value.get(\"average\"), value.get(\"count\"))\n    to: sensor_type_totals\n</code></pre> <p>This example demonstrates:</p> <ul> <li>Inline store configuration: Store defined directly within the <code>aggregate</code> operation</li> <li>Custom aggregation: Uses initializer and aggregator functions for complex calculations</li> <li>Memory-only storage: Non-persistent storage for temporary calculations (<code>persistent: false</code>)</li> <li>Caching enabled: Improves performance by batching state updates (<code>caching: true</code>)</li> </ul>"},{"location":"tutorials/intermediate/state-stores/#configuration-parameters","title":"Configuration Parameters","text":"<p>For a complete list of configuration parameters for all store types, see the State Store Reference - Configuration Parameters.</p>"},{"location":"tutorials/intermediate/state-stores/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/intermediate/state-stores/#when-to-use-predefined-stores","title":"When to Use Predefined Stores","text":"<ul> <li>Multiple operations need to access the same store</li> <li>Store configuration is complex or frequently reused</li> <li>You want centralized store management for maintainability</li> </ul>"},{"location":"tutorials/intermediate/state-stores/#when-to-use-inline-stores","title":"When to Use Inline Stores","text":"<ul> <li>Store is used by a single operation</li> <li>Simple, one-off configurations</li> <li>Prototyping or small applications</li> </ul>"},{"location":"tutorials/intermediate/state-stores/#memory-management","title":"Memory Management","text":"<ul> <li>Set appropriate retention periods to prevent unbounded growth</li> <li>Use non-persistent stores for temporary calculations</li> <li>Monitor memory usage, especially with caching enabled</li> </ul>"},{"location":"tutorials/intermediate/state-stores/#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>Enable logging for critical business data</li> <li>Use persistent stores for data that must survive restarts</li> <li>Consider the trade-off between durability and performance</li> </ul> <p>State stores are a powerful feature in KSML that enable sophisticated stateful stream processing patterns. By understanding the configuration options and trade-offs, you can build robust and efficient streaming applications.</p>"},{"location":"tutorials/intermediate/state-stores/#next-steps","title":"Next Steps","text":"<p>Ready to explore more advanced state store patterns? Continue with:</p> <ul> <li>Custom State Stores Tutorial - Learn advanced patterns including window stores, session stores, multi-store coordination, and optimization techniques for production applications</li> </ul>"},{"location":"tutorials/intermediate/windowing/","title":"Working with Windowed Operations in KSML","text":"<p>This tutorial explores how to implement time-based windowing operations in KSML for processing streaming data within specific time boundaries. Windowing is fundamental to stream processing analytics.</p> <p>KSML windowing operations are built on Kafka Streams' windowing capabilities, providing exactly-once processing guarantees and fault-tolerant state management.</p>"},{"location":"tutorials/intermediate/windowing/#introduction-to-windowed-operations","title":"Introduction to Windowed Operations","text":"<p>Windowing divides continuous data streams into finite chunks based on time, enabling:</p> <ul> <li>Time-based aggregations: Calculate metrics within time periods (hourly counts, daily averages)</li> <li>Pattern detection: Identify trends and anomalies over time windows</li> <li>Late data handling: Process out-of-order events with configurable grace periods</li> <li>State management: Maintain temporal state for complex analytics</li> <li>Resource control: Limit memory usage by automatically expiring old windows</li> </ul>"},{"location":"tutorials/intermediate/windowing/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial:</p> <ul> <li>Have Docker Compose KSML environment setup running</li> <li>Add the following topics to your <code>kafka-setup</code> service in docker-compose.yml to run the examples:</li> </ul> Topic creation commands - click to expand <pre><code>kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_clicks &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_click_counts_5min &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic click_counts &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_readings &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_moving_averages &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sensor_averages &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_session_summary &amp;&amp; \\\nkafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic user_sessions &amp;&amp; \\\n</code></pre>"},{"location":"tutorials/intermediate/windowing/#window-types-in-ksml","title":"Window Types in KSML","text":"<p>KSML supports three types of time windows, each suited for different use cases:</p>"},{"location":"tutorials/intermediate/windowing/#tumbling-windows","title":"Tumbling Windows","text":"<p>Concept: Non-overlapping, fixed-size windows where each record belongs to exactly one window.</p> <pre><code>- type: windowByTime\n  windowType: tumbling\n  duration: 5m      # Window size\n  grace: 30s        # Accept late data up to 30 seconds\n</code></pre> <p>Characteristics:</p> <ul> <li>No overlap between windows</li> <li>Clear, distinct time boundaries</li> <li>Memory efficient (fewer windows to maintain)</li> <li>Deterministic results</li> </ul> <p>Use cases:</p> <ul> <li>Hourly/daily reports</li> <li>Billing periods</li> <li>Compliance reporting</li> <li>Clear time-boundary analytics</li> </ul> <p>Kafka Streams equivalent: <code>TimeWindows.of(Duration.ofMinutes(5))</code></p>"},{"location":"tutorials/intermediate/windowing/#hopping-windows","title":"Hopping Windows","text":"<p>Concept: Fixed-size windows that overlap by advancing less than the window size, creating a \"sliding\" effect.</p> <pre><code>- type: windowByTime\n  windowType: hopping\n  duration: 10m     # Window size (how much data to include)\n  advanceBy: 2m     # Advance every 2 minutes (overlap control)\n  grace: 1m         # Late data tolerance\n</code></pre> <p>Characteristics:</p> <ul> <li>Windows overlap (each record appears in multiple windows)</li> <li>Smooth transitions between time periods</li> <li>Higher memory usage (more windows active)</li> <li>Good for trend analysis</li> </ul> <p>Use cases:</p> <ul> <li>Moving averages</li> <li>Smooth metric transitions</li> <li>Real-time dashboards</li> <li>Anomaly detection with context</li> </ul> <p>Kafka Streams equivalent: <code>TimeWindows.of(Duration.ofMinutes(10)).advanceBy(Duration.ofMinutes(2))</code></p>"},{"location":"tutorials/intermediate/windowing/#session-windows","title":"Session Windows","text":"<p>Concept: Dynamic windows that group events based on activity periods, automatically closing after inactivity gaps.</p> <pre><code>- type: windowBySession\n  inactivityGap: 30m  # Close window after 30 minutes of no activity\n  grace: 10s          # Accept late arrivals\n</code></pre> <p>Characteristics:</p> <ul> <li>Variable window sizes based on activity</li> <li>Automatically merge overlapping sessions</li> <li>Perfect for user behavior analysis</li> <li>Complex state management</li> </ul> <p>Use cases:</p> <ul> <li>User browsing sessions</li> <li>IoT device activity periods</li> <li>Fraud detection sessions</li> <li>Activity-based analytics</li> </ul> <p>Kafka Streams equivalent: <code>SessionWindows.with(Duration.ofMinutes(30))</code></p>"},{"location":"tutorials/intermediate/windowing/#windowing-examples","title":"Windowing Examples","text":""},{"location":"tutorials/intermediate/windowing/#tumbling-window-click-counting","title":"Tumbling Window: Click Counting","text":"<p>This example demonstrates tumbling windows by counting user clicks in 5-minute non-overlapping windows.</p> <p>What it does:</p> <ol> <li>Generates user clicks: Simulates users clicking on different pages</li> <li>Groups by user: Each user's clicks are processed separately</li> <li>Windows by time: Creates 5-minute tumbling windows</li> <li>Counts events: Uses simple count aggregation per window</li> <li>Handles window keys: Converts WindowedString keys for output compatibility</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>windowByTime</code> with tumbling windows</li> <li>Window state stores with retention policies</li> <li><code>convertKey</code> for windowed key compatibility</li> <li>Grace periods for late-arriving data</li> </ul> User clicks producer (click to expand) <pre><code>functions:\n  generate_click:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      click_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"]\n      pages = [\"/home\", \"/products\", \"/about\", \"/contact\", \"/checkout\"]\n    code: |\n      global click_counter, users, pages\n\n      click_counter += 1\n      user_id = random.choice(users)\n\n      value = {\n        \"click_id\": f\"click_{click_counter:06d}\",\n        \"user_id\": user_id,\n        \"page\": random.choice(pages),\n        \"session_id\": f\"session_{user_id}_{random.randint(1, 3)}\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      key = user_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  click_producer:\n    generator: generate_click\n    interval: 1s\n    to:\n      topic: user_clicks\n      keyType: string\n      valueType: json\n</code></pre> Tumbling window count processor (click to expand) <pre><code>streams:\n  user_clicks:\n    topic: user_clicks\n    keyType: string\n    valueType: json\n\n  click_counts:\n    topic: user_click_counts_5min\n    keyType: json:windowed(string)\n    valueType: long\n\npipelines:\n  count_clicks_5min:\n    from: user_clicks\n    via:\n      - type: groupByKey\n      - type: windowByTime\n        windowType: tumbling\n        duration: 5m\n        grace: 30s\n      - type: count\n        store:\n          name: click_counts_5min\n          type: window\n          windowSize: 5m\n          retention: 30m\n          caching: false\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n      - type: peek\n        forEach:\n          code: log.info(\"TUMBLING COUNT (5min) - user={}, count={}\", key, value)\n    to: click_counts\n</code></pre> <p>Understanding the window key type:</p> <p>The <code>convertKey</code> operation transforms the internal <code>WindowedString</code> to <code>json:windowed(string)</code> format, which contains:</p> <ul> <li><code>key</code>: Original key (user ID)</li> <li><code>start</code>/<code>end</code>: Window boundaries in milliseconds</li> <li><code>startTime</code>/<code>endTime</code>: Human-readable timestamps</li> </ul> <p>Verifying tumbling window data:</p> <p>The Kowl UI cannot display the value because a long stored in binary format is not automatically deserialized. To view the complete data for the target topic user_click_counts_5min, run: <pre><code># View click counts per 5-minute window\ndocker exec broker kafka-console-consumer.sh \\\n  --bootstrap-server broker:9093 \\\n  --topic user_click_counts_5min \\\n  --from-beginning \\\n  --max-messages 5 \\\n  --property print.key=true \\\n  --property key.separator=\" | \" \\\n  --key-deserializer org.apache.kafka.common.serialization.StringDeserializer \\\n  --value-deserializer org.apache.kafka.common.serialization.LongDeserializer\n</code></pre></p> <p>Example output: <pre><code>{\"end\":1755042000000,\"endTime\":\"2025-08-12T23:40:00Z\",\"key\":\"alice\",\"start\":1755041700000,\"startTime\":\"2025-08-12T23:35:00Z\"} | 8\n{\"end\":1755042000000,\"endTime\":\"2025-08-12T23:40:00Z\",\"key\":\"bob\",\"start\":1755041700000,\"startTime\":\"2025-08-12T23:35:00Z\"} | 12\n</code></pre></p>"},{"location":"tutorials/intermediate/windowing/#hopping-window-moving-averages","title":"Hopping Window: Moving Averages","text":"<p>This example calculates moving averages using overlapping 2-minute windows that advance every 30 seconds.</p> <p>What it does:</p> <ol> <li>Generates sensor readings: Simulates temperature, humidity, and pressure sensors</li> <li>Groups by sensor: Each sensor's readings are processed independently</li> <li>Overlapping windows: 2-minute windows advance every 30 seconds (4x overlap)</li> <li>Calculates averages: Maintains sum and count, then computes final average</li> <li>Smooth transitions: Provides continuous updates every 30 seconds</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>windowByTime</code> with hopping windows and <code>advanceBy</code></li> <li>Custom aggregation with initialization and aggregator functions</li> <li><code>mapValues</code> for post-aggregation processing</li> <li>Multiple overlapping windows for the same data</li> </ul> Sensor data producer (click to expand) <pre><code>functions:\n  generate_sensor_reading:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      reading_counter = 0\n      sensors = [\"temp_01\", \"temp_02\", \"humidity_01\", \"pressure_01\"]\n    code: |\n      global reading_counter, sensors\n\n      reading_counter += 1\n      sensor_id = random.choice(sensors)\n\n      # Generate realistic sensor values\n      if \"temp\" in sensor_id:\n        value_reading = round(random.uniform(18.0, 25.0), 2)\n        unit = \"celsius\"\n      elif \"humidity\" in sensor_id:\n        value_reading = round(random.uniform(30.0, 70.0), 1)\n        unit = \"percent\"\n      else:  # pressure\n        value_reading = round(random.uniform(1010.0, 1030.0), 1)\n        unit = \"hPa\"\n\n      value = {\n        \"reading_id\": reading_counter,\n        \"sensor_id\": sensor_id,\n        \"value\": value_reading,\n        \"unit\": unit,\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      key = sensor_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  sensor_producer:\n    generator: generate_sensor_reading\n    interval: 2s\n    to:\n      topic: sensor_readings\n      keyType: string\n      valueType: json\n</code></pre> Hopping window average processor (click to expand) <pre><code>streams:\n  sensor_readings:\n    topic: sensor_readings\n    keyType: string\n    valueType: json\n\n  sensor_averages:\n    topic: sensor_moving_averages\n    keyType: json:windowed(string)\n    valueType: json\n\nfunctions:\n  initialize_average:\n    type: initializer\n    code: |\n      result = {\"sum\": 0.0, \"count\": 0}\n    expression: result\n    resultType: json\n\n  update_average:\n    type: aggregator\n    code: |\n      # Initialize if first time\n      if aggregatedValue is None:\n        aggregatedValue = {\"sum\": 0.0, \"count\": 0}\n\n      # Update sum and count\n      aggregatedValue[\"sum\"] += value.get(\"value\", 0.0)\n      aggregatedValue[\"count\"] += 1\n\n      new_value = aggregatedValue\n    expression: new_value\n    resultType: json\n\n  calculate_final_average:\n    type: valueTransformer\n    code: |\n      if value is None or value.get(\"count\", 0) == 0:\n        return None\n\n      # Calculate average\n      average = value[\"sum\"] / value[\"count\"]\n\n      result = {\n        \"average\": round(average, 2),\n        \"sample_count\": value[\"count\"],\n        \"total_sum\": round(value[\"sum\"], 2)\n      }\n    expression: result\n    resultType: json\n\npipelines:\n  moving_average_2min:\n    from: sensor_readings\n    via:\n      - type: groupByKey\n      - type: windowByTime\n        windowType: hopping\n        duration: 2m        # 2-minute windows\n        advanceBy: 30s      # Move forward every 30 seconds (overlapping)\n        grace: 10s\n      - type: aggregate\n        store:\n          name: sensor_averages_hopping\n          type: window\n          windowSize: 2m\n          retention: 10m\n          caching: false\n        initializer: initialize_average\n        aggregator: update_average\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n      - type: mapValues\n        mapper: calculate_final_average\n      - type: peek\n        forEach:\n          code: log.info(\"HOPPING AVERAGE (2min/30s) - sensor={}, avg={}\", key, value.get(\"average\") if value else None)\n    to: sensor_averages\n</code></pre> <p>Example output: <pre><code># Raw sensor readings:\ntemp_01 | {\"reading_id\":1,\"sensor_id\":\"temp_01\",\"value\":21.57,\"unit\":\"celsius\",\"timestamp\":1755042846167}\n\n# Moving averages:\n{\"end\":1755043080000,\"endTime\":\"2025-08-12T23:58:00Z\",\"key\":\"temp_01\",\"start\":1755042960000,\"startTime\":\"2025-08-12T23:56:00Z\"} | {\"average\":21.49,\"sample_count\":4,\"total_sum\":85.96}\n</code></pre></p> <p>Understanding the hopping window output:</p> <ul> <li>Key structure: The output key contains window boundaries and the original record key</li> <li><code>start</code>: Window start time (23:56:00Z)</li> <li><code>end</code>: Window end time (23:58:00Z) </li> <li><code>key</code>: Original sensor ID (temp_01)</li> <li>Window duration: 2-minute window covering 23:56:00Z to 23:58:00Z</li> <li>Value aggregation: Contains calculated average (21.49\u00b0C) from 4 sensor readings</li> <li>Overlapping nature: Since windows advance every 30 seconds, this sensor will appear in multiple overlapping windows, each with slightly different averages as new data arrives and old data expires</li> </ul> <p>Why hopping windows for averages?</p> <ul> <li>Smooth updates: New average every 30 seconds instead of waiting 2 minutes</li> <li>Trend detection: Easier to spot gradual changes</li> <li>Real-time dashboards: Continuous data flow for visualization</li> <li>Reduced noise: 2-minute window smooths out brief spikes</li> </ul>"},{"location":"tutorials/intermediate/windowing/#session-window-user-activity-analysis","title":"Session Window: User Activity Analysis","text":"<p>This example uses session windows to analyze user browsing patterns by grouping clicks within activity periods.</p> <p>What it does:</p> <ol> <li>Tracks user clicks: Monitors page visits and click patterns</li> <li>Detects sessions: Groups clicks separated by no more than 2 minutes</li> <li>Aggregates activity: Counts clicks, tracks unique pages, measures duration</li> <li>Session boundaries: Automatically closes sessions after inactivity</li> <li>Rich analytics: Provides comprehensive session summaries</li> </ol> <p>Key KSML concepts demonstrated:</p> <ul> <li><code>windowBySession</code> with inactivity gap detection</li> <li>Session state stores for variable-length windows</li> <li>Complex aggregation state with lists and timestamps</li> <li>Automatic session merging and boundary detection</li> </ul> <p>Session window behavior:</p> <ul> <li>Dynamic sizing: Windows grow and shrink based on activity</li> <li>Automatic merging: Late-arriving data can extend or merge sessions</li> <li>Activity-based: Perfect for user behavior analysis</li> <li>Variable retention: Different sessions can have different lifespans</li> </ul> User clicks producer (click to expand) <pre><code>functions:\n  generate_click:\n    type: generator\n    globalCode: |\n      import random\n      import time\n      click_counter = 0\n      users = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\"]\n      pages = [\"/home\", \"/products\", \"/about\", \"/contact\", \"/checkout\"]\n    code: |\n      global click_counter, users, pages\n\n      click_counter += 1\n      user_id = random.choice(users)\n\n      value = {\n        \"click_id\": f\"click_{click_counter:06d}\",\n        \"user_id\": user_id,\n        \"page\": random.choice(pages),\n        \"session_id\": f\"session_{user_id}_{random.randint(1, 3)}\",\n        \"timestamp\": int(time.time() * 1000)\n      }\n\n      key = user_id\n    expression: (key, value)\n    resultType: (string, json)\n\nproducers:\n  click_producer:\n    generator: generate_click\n    interval: 1s\n    to:\n      topic: user_clicks\n      keyType: string\n      valueType: json\n</code></pre> User session analysis processor (click to expand) <pre><code>streams:\n  user_clicks:\n    topic: user_clicks\n    keyType: string\n    valueType: json\n\n  user_sessions:\n    topic: user_session_summary\n    keyType: json:windowed(string)  \n    valueType: long\n\npipelines:\n  user_session_analysis:\n    from: user_clicks\n    via:\n      - type: groupByKey\n      - type: windowBySession\n        inactivityGap: 2m  # Close session after 2 minutes of inactivity\n        grace: 30s\n      - type: count\n        store:\n          name: user_sessions\n          type: session\n          retention: 1h\n          caching: false\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n      - type: peek\n        forEach:\n          code: log.info(\"USER SESSION - user={}, clicks={}\", key, value)\n    to: user_sessions\n</code></pre> <p>Session window characteristics observed:</p> <ul> <li>Activity-driven boundaries: Sessions start with first click and extend with each new click</li> <li>Inactivity-based closure: Sessions close after 2 minutes of no activity</li> <li>Variable duration: Sessions can be seconds to hours depending on user behavior</li> <li>Real-time updates: Each click updates the session end time and click count</li> <li>User-specific: Each user maintains independent session windows</li> </ul> <p>Example output: <pre><code>USER SESSION - user=alice, clicks=15  (session: 23:44:05Z to 23:46:12Z)\nUSER SESSION - user=bob, clicks=8    (session: 23:44:01Z to 23:45:33Z) \nUSER SESSION - user=alice, clicks=23 (session: 23:47:01Z to 23:49:15Z)\n</code></pre></p> <p>This shows how Alice had two separate sessions with an inactivity gap between them, while Bob had one continuous session.</p> <p>Example output: <pre><code># Input clicks (user_clicks topic):\nalice | {\"click_id\":\"click_000001\",\"user_id\":\"alice\",\"page\":\"/home\",\"session_id\":\"session_alice_1\",\"timestamp\":1755043944188}\nalice | {\"click_id\":\"click_000002\",\"user_id\":\"alice\",\"page\":\"/products\",\"session_id\":\"session_alice_1\",\"timestamp\":1755043945189}\n\n# Session summary (user_session_summary topic):  \n{\"end\":1755043970189,\"endTime\":\"2025-08-13T00:12:50.189Z\",\"key\":\"alice\",\"start\":1755043944188,\"startTime\":\"2025-08-13T00:12:24.188Z\"} | 15\n{\"end\":1755043938275,\"endTime\":\"2025-08-13T00:12:18.275Z\",\"key\":\"diana\",\"start\":1755043938275,\"startTime\":\"2025-08-13T00:12:18.275Z\"} | 1\n</code></pre></p> <p>The session summary shows:</p> <ul> <li>Key: Windowed key with session boundaries and original user key</li> <li>Value: Total click count for that session (15 clicks for Alice, 1 click for Diana)</li> </ul> <p>Understanding the output:</p> <ul> <li>Alice had a session lasting ~26 seconds (00:12:24Z to 00:12:50Z) with 15 clicks</li> <li>Diana had a brief session (single timestamp) with 1 click</li> <li>Sessions automatically close after 2 minutes of inactivity</li> </ul>"},{"location":"tutorials/intermediate/windowing/#advanced-windowing-concepts","title":"Advanced Windowing Concepts","text":""},{"location":"tutorials/intermediate/windowing/#grace-periods-and-late-data","title":"Grace Periods and Late Data","text":"<p>Problem: In distributed systems, data doesn't always arrive in chronological order. Network delays, system failures, and clock skew can cause \"late\" data.</p> <p>Solution: Grace periods allow windows to accept late-arriving data for a specified time after the window officially closes.</p> <pre><code>- type: windowByTime\n  windowType: tumbling\n  duration: 5m\n  grace: 1m        # Accept data up to 1 minute late\n</code></pre> <p>Configuration guidelines:</p> <ul> <li>grace: How long to wait for late data (trade-off: accuracy vs. latency)</li> <li>retention: How long to keep window state (must be \u2265 window size + grace period)</li> <li>caching: Enable for better performance with frequent updates</li> </ul> <p>Example: A 5-minute window ending at 10:05 AM will accept data timestamped before 10:05 AM until 10:06 AM (1-minute grace), then close permanently.</p>"},{"location":"tutorials/intermediate/windowing/#window-state-management","title":"Window State Management","text":"<p>Windowed operations require persistent state to:</p> <ul> <li>Track aggregations across window boundaries</li> <li>Handle late-arriving data</li> <li>Provide exactly-once processing guarantees</li> <li>Enable fault tolerance and recovery</li> </ul> <p>State Store Configuration:</p> <pre><code>store:\n  name: my_window_store\n  type: window             # Required for windowed operations\n  windowSize: 5m           # Must match window duration\n  retention: 30m           # Keep expired windows (\u2265 windowSize + grace)\n  caching: true            # Reduce downstream updates\n  retainDuplicates: false  # Keep only latest value per window\n</code></pre>"},{"location":"tutorials/intermediate/windowing/#performance-considerations","title":"Performance Considerations","text":""},{"location":"tutorials/intermediate/windowing/#memory-usage","title":"Memory Usage","text":"<p>Window count calculation:</p> <ul> <li>Tumbling: <code>retention / window_size</code> windows per key</li> <li>Hopping: <code>retention / advance_by</code> windows per key  </li> <li>Session: Variable, depends on activity patterns</li> </ul> <p>Example: 1-hour retention with 5-minute tumbling windows = 12 windows per key</p>"},{"location":"tutorials/intermediate/windowing/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Right-size windows:</li> <li>Too small (10 seconds): Creates excessive overhead with frequent window updates and high CPU usage</li> <li>Too large (24 hours): Consumes excessive memory and delays results until window closes</li> <li> <p>Balanced approach: Choose window size that matches your business requirements (e.g., 5 minutes for real-time dashboards, 1 hour for reporting)</p> </li> <li> <p>Tune grace periods:</p> </li> <li>Minimal grace (5 seconds): Provides fast processing but may lose legitimate late-arriving data</li> <li>Conservative grace (5 minutes): Handles most network delays and clock skew but slows down result publication</li> <li> <p>Best practice: Set grace period based on your network characteristics and data source reliability</p> </li> <li> <p>Enable caching:</p> </li> <li>Purpose: Reduces the number of downstream updates by batching window changes</li> <li>Benefit: Lower CPU usage and fewer Kafka messages when windows are frequently updated</li> <li> <p>Trade-off: Slightly higher memory usage for improved throughput</p> </li> <li> <p>Optimize retention:</p> </li> <li>Minimum requirement: Window size plus grace period (e.g., 5-minute window + 30-second grace = 5.5 minutes minimum)</li> <li>Memory impact: Longer retention keeps more data in memory for join operations and late data handling</li> <li>Performance balance: Set retention just long enough to handle your latest acceptable late data</li> </ol>"},{"location":"tutorials/intermediate/windowing/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"tutorials/intermediate/windowing/#missing-data-in-windows","title":"Missing Data in Windows","text":"<p>Symptoms: Expected data doesn't appear in window results</p> <p>Causes &amp; Solutions:</p> <ol> <li>Clock skew: Ensure producer/consumer clocks are synchronized</li> <li>Grace period too short: Increase grace period for late data</li> <li>Wrong timestamps: Verify timestamp field extraction</li> <li>Retention too short: Data expired before processing</li> </ol>"},{"location":"tutorials/intermediate/windowing/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: OutOfMemory errors, slow processing</p> <p>Solutions:</p> <ol> <li>Reduce retention periods</li> <li>Increase window sizes (fewer windows)</li> <li>Enable caching to reduce state store pressure</li> <li>Filter data earlier in the pipeline</li> </ol>"},{"location":"tutorials/intermediate/windowing/#inconsistent-results","title":"Inconsistent Results","text":"<p>Symptoms: Different runs produce different window contents</p> <p>Causes:</p> <ol> <li>Late data: Some runs receive different late arrivals</li> <li>Grace period timing: Data arrives just at grace boundary</li> <li>System clock differences: Inconsistent time sources</li> </ol> <p>Solutions:</p> <ol> <li>Use consistent time sources</li> <li>Implement proper grace periods</li> <li>Consider event-time vs processing-time semantics</li> </ol>"},{"location":"tutorials/intermediate/windowing/#window-type-selection-guide","title":"Window Type Selection Guide","text":"Window Type Best For Key Benefits Trade-offs Tumbling Periodic reports, billing cycles, compliance Clear boundaries, memory efficient, deterministic Less granular, potential data delays Hopping Moving averages, real-time dashboards, trend analysis Smooth updates, continuous metrics Higher memory usage, more computation Session User behavior, IoT device activity, fraud detection Activity-driven, variable length, natural boundaries Complex state management, harder to predict"},{"location":"tutorials/intermediate/windowing/#getting-started","title":"Getting Started","text":"<ol> <li>Start simple: Begin with tumbling windows for periodic analytics</li> <li>Add smoothness: Use hopping windows when you need continuous updates</li> <li>Handle activity: Implement session windows for behavior-driven analysis</li> <li>Optimize gradually: Tune grace periods, retention, and caching based on requirements</li> </ol>"},{"location":"tutorials/intermediate/windowing/#further-reading","title":"Further Reading","text":"<ul> <li>Reference: Windowing Operations</li> </ul>"},{"location":"use-cases/","title":"Use Case Guides","text":"<p>Welcome to the KSML Use Case Guides! These guides demonstrate how to apply KSML to solve real-world business problems and implement common stream processing patterns.</p> <p>Unlike the tutorials that focus on specific KSML features, these guides take a problem-first approach, showing you how to combine various KSML capabilities to address practical use cases.</p>"},{"location":"use-cases/#available-use-case-guides","title":"Available Use Case Guides","text":""},{"location":"use-cases/#data-transformation","title":"Data Transformation","text":"<p>This guide covers common data transformation patterns:</p> <ul> <li>Format conversion (JSON to Avro, CSV to JSON, etc.)</li> <li>Data normalization and cleansing</li> <li>Schema evolution handling</li> <li>Complex data transformations</li> </ul>"},{"location":"use-cases/#event-driven-applications","title":"Event-Driven Applications","text":"<p>Learn how to build event-driven applications with KSML:</p> <ul> <li>Implementing the event sourcing pattern</li> <li>Building event-driven microservices</li> <li>Command-query responsibility segregation (CQRS)</li> <li>Event notification systems</li> </ul>"},{"location":"use-cases/#iot-data-processing","title":"IoT Data Processing","text":"<p>Learn how to process IoT data streams with KSML:</p> <ul> <li>Handling high-volume sensor data</li> <li>Device state tracking</li> <li>Geospatial data processing</li> <li>Edge-to-cloud data pipelines</li> </ul>"},{"location":"use-cases/#real-time-analytics","title":"Real-time Analytics","text":"<p>Learn how to implement real-time analytics solutions with KSML:</p> <ul> <li>Building real-time dashboards</li> <li>Calculating key performance indicators</li> <li>Implementing sliding window analytics</li> <li>Detecting anomalies in streaming data</li> </ul>"},{"location":"use-cases/#fraud-detection","title":"Fraud Detection","text":"<p>This guide demonstrates how to implement fraud detection systems:</p> <ul> <li>Pattern recognition in transaction streams</li> <li>Real-time risk scoring</li> <li>Multi-factor anomaly detection</li> <li>Alert generation and notification</li> </ul>"},{"location":"use-cases/#how-to-use-these-guides","title":"How to Use These Guides","text":"<p>Each guide includes:</p> <ol> <li>Problem Statement: A clear description of the business problem or use case</li> <li>Solution Architecture: An overview of the KSML solution design</li> <li>Implementation: Step-by-step instructions with KSML code examples</li> <li>Testing and Validation: How to test and validate the solution</li> <li>Production Considerations: Tips for deploying to production environments</li> </ol> <p>You can follow these guides end-to-end to implement the complete solution, or adapt specific patterns to your own use cases.</p>"},{"location":"use-cases/#additional-resources","title":"Additional Resources","text":"<ul> <li>KSML Definition Reference for a full explanation of the KSML definition syntax</li> <li>Reference Documentation for complete reference for all KSML operations</li> </ul>"},{"location":"use-cases/data-transformation/","title":"Data Transformation with KSML","text":"<p>This tutorial demonstrates how to build a data transformation pipeline using KSML. You'll learn how to convert data between different formats, enrich data with additional information, and handle complex transformations.</p>"},{"location":"use-cases/data-transformation/#introduction","title":"Introduction","text":"<p>Data transformation is a fundamental use case for stream processing. It allows you to:</p> <ul> <li>Convert data between different formats (JSON, XML, Avro, etc.)</li> <li>Normalize and clean data from various sources</li> <li>Enrich data with additional context or reference information</li> <li>Restructure data to meet downstream system requirements</li> <li>Filter out unnecessary information</li> </ul> <p>In this tutorial, we'll build a data transformation pipeline that processes customer data from a legacy system, transforms it into a standardized format, enriches it with additional information, and makes it available for downstream applications.</p>"},{"location":"use-cases/data-transformation/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, you should:</p> <ul> <li>Understand basic KSML concepts (streams, functions, pipelines)</li> <li>Have completed the KSML Basics Tutorial</li> <li>Be familiar with Filtering and Transforming</li> <li>Have a basic understanding of Joins for data enrichment</li> </ul>"},{"location":"use-cases/data-transformation/#the-use-case","title":"The Use Case","text":"<p>Imagine you're working with a company that has acquired another business. You need to integrate customer data from the acquired company's legacy system into your modern data platform. The legacy data:</p> <ul> <li>Is in a different format (XML) than your system (JSON)</li> <li>Uses different field names and conventions</li> <li>Contains some fields you don't need</li> <li>Is missing some information that you need to add from reference data</li> </ul>"},{"location":"use-cases/data-transformation/#defining-the-data-models","title":"Defining the Data Models","text":""},{"location":"use-cases/data-transformation/#source-data-xml","title":"Source Data (XML)","text":"<p>The legacy system provides customer data in XML format:</p> <pre><code>&lt;customer&gt;\n    &lt;cust_id&gt;12345&lt;/cust_id&gt;\n    &lt;fname&gt;John&lt;/fname&gt;\n    &lt;lname&gt;Doe&lt;/lname&gt;\n    &lt;dob&gt;1980-01-15&lt;/dob&gt;\n    &lt;addr&gt;\n        &lt;street&gt;123 Main St&lt;/street&gt;\n        &lt;city&gt;Anytown&lt;/city&gt;\n        &lt;state&gt;CA&lt;/state&gt;\n        &lt;zip&gt;90210&lt;/zip&gt;\n    &lt;/addr&gt;\n    &lt;phone&gt;555-123-4567&lt;/phone&gt;\n    &lt;legacy_segment&gt;A&lt;/legacy_segment&gt;\n    &lt;account_created&gt;2015-03-20&lt;/account_created&gt;\n&lt;/customer&gt;\n</code></pre>"},{"location":"use-cases/data-transformation/#reference-data-json","title":"Reference Data (JSON)","text":"<p>You have a reference table topic with segment code (key) mappings to segment details (value):</p> Key Value A <code>{\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"}</code> B <code>{\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"}</code> C <code>{\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\"}</code>"},{"location":"use-cases/data-transformation/#target-data-json","title":"Target Data (JSON)","text":"<p>You want to transform the data into this format:</p> <pre><code>{\n  \"customer_id\": \"12345\",\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  },\n  \"contact_info\": {\n    \"email\": \"john.doe@example.com\",\n    \"phone\": \"555-123-4567\",\n    \"address\": {\n      \"street\": \"123 Main St\",\n      \"city\": \"Anytown\",\n      \"state\": \"CA\",\n      \"postal_code\": \"90210\",\n      \"country\": \"USA\"\n    }\n  },\n  \"birth_date\": \"1980-01-15\",\n  \"customer_since\": \"2015-03-20\",\n  \"segment\": \"Premium\",\n  \"marketing_preferences\": {\n    \"group\": \"High Value\",\n    \"discount_tier\": \"Tier 1\"\n  },\n  \"metadata\": {\n    \"source\": \"legacy_system\",\n    \"last_updated\": \"2023-07-01T12:00:00Z\"\n  }\n}\n</code></pre>"},{"location":"use-cases/data-transformation/#creating-the-ksml-definition","title":"Creating the KSML Definition","text":"<p>Now, let's create our KSML definition file:</p> Data transformation processor (click to expand) <pre><code># Data Transformation Use Case\n# This definition shows how to take legacy customer data in XML, join it with a customer segments table on Kafka,\n# and write out the customer data in another structure as JSON.\n\nstreams:\n  legacy_customers:\n    topic: legacy_customer_data\n    keyType: string  # customer_id\n    valueType: xml  # XML customer data\n\n  transformed_customers:\n    topic: standardized_customer_data\n    keyType: string  # customer_id\n    valueType: json  # transformed customer data\n\ntables:\n  segment_reference:\n    topic: customer_segments\n    keyType: string  # segment code\n    valueType: json  # segment details\n\nfunctions:\n  transform_customer:\n    type: valueTransformer\n    code: |\n      import datetime\n\n      # Extract values from XML\n      customer_id = value.get(\"cust_id\")\n      first_name = value.get(\"fname\")\n      last_name = value.get(\"lname\")\n      birth_date = value.get(\"dob\")\n      phone = value.get(\"phone\")\n      legacy_segment = value.get(\"legacy_segment\")\n      customer_since = value.get(\"account_created\")\n\n      # Extract address\n      address = value.get(\"addr\")\n      street = address.get(\"street\")\n      city = address.get(\"city\")\n      state = address.get(\"state\")\n      zip_code = address.get(\"zip\")\n\n      # Generate email (not in source data)\n      email = f\"{first_name.lower()}.{last_name.lower()}@example.com\"\n\n      # Get segment info from reference data\n      segment_info = value.get(\"segment_info\")\n      segment_name = segment_info.get(\"segment_name\") if segment_info else \"Unknown\"\n      discount_tier = segment_info.get(\"discount_tier\") if segment_info else \"Unknown\"\n      marketing_group = segment_info.get(\"marketing_group\") if segment_info else \"Unknown\"\n\n      # Current timestamp for metadata\n      current_time = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n      # Create transformed customer object\n      return {\n        \"customer_id\": customer_id,\n        \"name\": {\n          \"first\": first_name,\n          \"last\": last_name\n        },\n        \"contact_info\": {\n          \"email\": email,\n          \"phone\": phone,\n          \"address\": {\n            \"street\": street,\n            \"city\": city,\n            \"state\": state,\n            \"postal_code\": zip_code,\n            \"country\": \"USA\"  # Default value not in source\n          }\n        },\n        \"birth_date\": birth_date,\n        \"customer_since\": customer_since,\n        \"segment\": segment_name,\n        \"marketing_preferences\": {\n          \"group\": marketing_group,\n          \"discount_tier\": discount_tier\n        },\n        \"metadata\": {\n          \"source\": \"legacy_system\",\n          \"last_updated\": current_time\n        }\n      }\n    resultType: json\n\npipelines:\n  customer_transformation_pipeline:\n    from: legacy_customers\n    via:\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Processing customer: {}\", key)\n      - type: transformKey\n        mapper:\n          expression: value.get(\"legacy_segment\")\n          resultType: string\n      - type: leftJoin\n        table: segment_reference\n        valueJoiner:\n          expression: |\n            {\n              **value1,\n              \"segment_info\": value2\n            }\n          resultType: json\n      - type: transformValue\n        mapper: transform_customer\n      - type: transformKey\n        mapper:\n          expression: value.get(\"customer_id\")\n          resultType: string\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Transformed customer: {} = {}\", key, value)\n    to: transformed_customers\n</code></pre>"},{"location":"use-cases/data-transformation/#setting-up-two-producers-for-test-data","title":"Setting up two producers for test data","text":"<p>To test out the topology above, we create two test data producers.</p> <p>The first producer is a single shot producer that generates data for the <code>customer_segments</code> topic:</p> Segment data producer (click to expand) <pre><code># Segment Data Producer\n# This producer is part of the Data Transformation Use Case in KSML documentation.\n# It produces customer segment data to a topic in JSON format.\n\nfunctions:\n  customer_segment_generator:\n    globalCode: |\n      import random\n      count = 0\n    code: |\n      global count\n      refs = {\n        0: {\"key\": \"A\", \"value\": {\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"}},\n        1: {\"key\": \"B\", \"value\": {\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"}},\n        2: {\"key\": \"C\", \"value\": {\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\" }}\n      }\n      key = refs.get(count)[\"key\"]\n      value = refs.get(count)[\"value\"]\n      count = (count + 1) % 3\n      return (key, value)\n    resultType: (string, struct)\n\nproducers:\n  customer_segment_producer:\n    generator: customer_segment_generator\n    interval: 10s\n    count: 3\n    to:\n      topic: customer_segments\n      keyType: string  # segment code\n      valueType: json  # segment details\n</code></pre> <p>The second producer produces a message every second to the <code>legacy_customer_data</code> topic, using a randomly chosen segment:</p> Customer data producer (click to expand) <pre><code># Customer Data Producer\n# This producer is part of the Data Transformation Use Case in KSML documentation.\n# It produces legacy customer data to a topic in XML format.\n\nfunctions:\n  legacy_customer_data_generator:\n    globalCode: |\n      import random\n    code: |\n      value = {\n        \"cust_id\": random.randrange(100000),\n        \"fname\": \"John\",\n        \"lname\": \"Doe\",\n        \"dob\": \"1980-01-15\",\n        \"addr\": {\n          \"street\": \"123 Main St\",\n          \"city\": \"Anytown\",\n          \"state\": \"CA\",\n          \"zip\": \"90210\"\n        },\n        \"phone\": \"555-123-4567\",\n        \"legacy_segment\": random.choice([\"A\",\"B\",\"C\"]),\n        \"account_created\": \"2015-03-20\"\n      }\n      key = value.get(\"cust_id\")\n      return (key, value)\n    resultType: (string, struct)\n\nproducers:\n  legacy_customer_data_producer:\n    generator: legacy_customer_data_generator\n    interval: 1s\n    to:\n      topic: legacy_customer_data\n      keyType: string  # customer_id\n      valueType: xml  # XML customer data\n</code></pre>"},{"location":"use-cases/data-transformation/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Save the processor definition to    <code>data_transformation.yaml</code>.</li> <li>Save the producers to    <code>segment-data-producer.yaml</code>    and    <code>customer-data-producer.yaml</code>.</li> <li>Set up your <code>ksml-runner.yaml</code> configuration, pointing to your Kafka installation.</li> </ol> KSML runner configuration (click to expand) <pre><code>ksml:\n  enableProducers: true              # Set to true to allow producer definitions to be parsed in the KSML definitions and be executed.\n  enablePipelines: true              # Set to true to allow pipeline definitions to be parsed in the KSML definitions and be executed.\n  definitions:\n    segment_data_producer: segment-data-producer.yaml\n    customer_data_producer: customer-data-producer.yaml\n    data_transformation: data-transformation.yaml\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: your.app.id\n</code></pre> <ol> <li>Start the <code>customer_segment_producer</code> to produce the sample segment information to Kafka.</li> <li>Start the <code>legacy_customer_data_producer</code> to produce some sample data to the input topic.</li> <li>Start the <code>data_transformation</code> topology to initiate the continuous data transformation logic.</li> <li>Monitor the output topic to see the transformed data.</li> </ol>"},{"location":"use-cases/data-transformation/#advanced-transformation-techniques","title":"Advanced Transformation Techniques","text":""},{"location":"use-cases/data-transformation/#handling-missing-or-invalid-data","title":"Handling Missing or Invalid Data","text":"<p>In real-world scenarios, source data often has missing or invalid fields. You can enhance the transformation function to handle these cases:</p> <pre><code># Check if a field exists before accessing it\nbirth_date = value.get(\"dob\") if value.get(\"dob\") is not None else \"Unknown\"\n\n# Provide default value in case dictionary key does not exist\nstate = address.get(\"state\", \"N/A\")\n\n# Validate data\nif phone and not phone.startswith(\"555-\"):\n    log.warn(\"Invalid phone format for customer {}: {}\", customer_id, phone)\n    phone = \"Unknown\"\n</code></pre>"},{"location":"use-cases/data-transformation/#schema-evolution-handling","title":"Schema Evolution Handling","text":"<p>To handle changes in the source or target schema over time:</p> <pre><code># Version-aware transformation\nschema_version = value.get(\"version\", \"1.0\")\nif schema_version == \"1.0\":\n    # Original transformation logic\n    value = transform_v1(value)\nelif schema_version == \"2.0\":\n    # Updated transformation logic for new schema\n    value = transform_v2(value)\nelse:\n    log.error(\"Unknown schema version: {}\", schema_version)\n</code></pre>"},{"location":"use-cases/data-transformation/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to:</p> <ul> <li>Transform data between different formats (XML to JSON)</li> <li>Restructure data to match a target schema</li> <li>Enrich data with information from reference sources</li> <li>Handle missing or derived fields</li> <li>Process and validate data during transformation</li> </ul> <p>Data transformation is a powerful use case for KSML, allowing you to integrate data from various sources and prepare it for downstream applications without complex coding.</p>"},{"location":"use-cases/data-transformation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Real-Time Analytics to analyze your transformed data</li> <li>Explore Event-Driven Applications to trigger actions based on data changes</li> <li>Check out External Integration for connecting to external systems</li> </ul>"},{"location":"use-cases/event-driven-applications/","title":"Event-Driven Applications with KSML","text":"<p>This tutorial demonstrates how to build event-driven applications using KSML. You'll learn how to detect specific events in your data streams and trigger appropriate actions in response.</p>"},{"location":"use-cases/event-driven-applications/#introduction","title":"Introduction","text":"<p>Event-driven architecture is a powerful paradigm for building responsive, real-time applications. In this approach:</p> <ul> <li>Systems react to events as they occur</li> <li>Components communicate through events rather than direct calls</li> <li>Business logic is triggered by changes in state</li> <li>Applications can scale and evolve independently</li> </ul> <p>KSML is particularly well-suited for event-driven applications because it allows you to:</p> <ul> <li>Process streams of events in real-time</li> <li>Detect complex patterns and conditions</li> <li>Transform events into actionable insights</li> <li>Trigger downstream processes automatically</li> </ul>"},{"location":"use-cases/event-driven-applications/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, you should:</p> <ul> <li>Understand basic KSML concepts (streams, functions, pipelines)</li> <li>Have completed the KSML Basics Tutorial</li> <li>Be familiar with Filtering and Transforming</li> <li>Have a basic understanding of Complex Event Processing</li> </ul>"},{"location":"use-cases/event-driven-applications/#the-use-case","title":"The Use Case","text":"<p>In this tutorial, we'll build an event-driven inventory management system for an e-commerce platform. The system will:</p> <ol> <li>Monitor product inventory levels in real-time</li> <li>Detect when items are running low</li> <li>Generate reorder events for the procurement system</li> <li>Alert warehouse staff about critical inventory situations</li> <li>Update inventory dashboards in real-time</li> </ol>"},{"location":"use-cases/event-driven-applications/#defining-the-data-models","title":"Defining the Data Models","text":""},{"location":"use-cases/event-driven-applications/#inventory-update-events","title":"Inventory Update Events","text":"<pre><code>{\n  \"product_id\": \"prod-123\",\n  \"product_name\": \"Wireless Headphones\",\n  \"category\": \"electronics\",\n  \"current_stock\": 15,\n  \"warehouse_id\": \"wh-east-1\",\n  \"timestamp\": 1625097600000,\n  \"unit_price\": 79.99\n}\n</code></pre>"},{"location":"use-cases/event-driven-applications/#order-events","title":"Order Events","text":"<pre><code>{\n  \"order_id\": \"order-456\",\n  \"customer_id\": \"cust-789\",\n  \"items\": [\n    {\n      \"product_id\": \"prod-123\",\n      \"quantity\": 2,\n      \"unit_price\": 79.99\n    }\n  ],\n  \"order_total\": 159.98,\n  \"timestamp\": 1625097600000\n}\n</code></pre>"},{"location":"use-cases/event-driven-applications/#reorder-events-output","title":"Reorder Events (Output)","text":"<pre><code>{\n  \"event_id\": \"reorder-789\",\n  \"product_id\": \"prod-123\",\n  \"product_name\": \"Wireless Headphones\",\n  \"current_stock\": 5,\n  \"reorder_quantity\": 50,\n  \"priority\": \"normal\",\n  \"warehouse_id\": \"wh-east-1\",\n  \"timestamp\": 1625097600000\n}\n</code></pre>"},{"location":"use-cases/event-driven-applications/#alert-events-output","title":"Alert Events (Output)","text":"<pre><code>{\n  \"alert_id\": \"alert-123\",\n  \"alert_type\": \"critical_inventory\",\n  \"product_id\": \"prod-123\",\n  \"product_name\": \"Wireless Headphones\",\n  \"current_stock\": 2,\n  \"threshold\": 5,\n  \"warehouse_id\": \"wh-east-1\",\n  \"timestamp\": 1625097600000,\n  \"message\": \"Critical inventory level: Wireless Headphones (2 units remaining)\"\n}\n</code></pre>"},{"location":"use-cases/event-driven-applications/#creating-the-ksml-definition","title":"Creating the KSML Definition","text":"<p>Now, let's create our KSML definition file:</p> Inventory event processor (click to expand) <pre><code>streams:\n  order_events:\n    topic: order_events\n    keyType: string  # order_id\n    valueType: json  # order data\n\n  inventory_updates:\n    topic: inventory_updates\n    keyType: string  # product_id\n    valueType: json  # alert data\n\n  reorder_events:\n    topic: reorder_events\n    keyType: string  # event_id\n    valueType: json  # reorder data\n\n  inventory_alerts:\n    topic: inventory_alerts\n    keyType: string  # alert_id\n    valueType: json  # alert data\n\ntables:\n  product_catalog:\n    topic: product_catalog\n    keyType: string  # product_id\n    valueType: json  # product details including thresholds\n\nstores:\n  inventory_store:\n    type: keyValue\n    keyType: string\n    valueType: json\n\npipelines:\n  # Pipeline for updating inventory based on orders\n  order_pipeline:\n    from: order_events\n    via:\n      - type: transformKeyValueToKeyValueList\n        mapper:\n          code: |\n            # For each item in the order, emit a key-value pair with product_id as key and ordered item as value\n            result = []\n            for item in value.get(\"items\", []):\n              product_id = item.get(\"product_id\")\n              if product_id:\n                result.append((product_id, {**item, \"order_id\":key}))\n            return result\n          resultType: list(tuple(string, struct))\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Ordered item: {}\", value)\n      - type: transformValue\n        mapper:\n          code: |\n            inventory = inventory_store.get(key)\n            if not inventory:\n              inventory = {\n                \"product_id\": key,\n                \"current_stock\": 0\n              }\n            return {            \n              \"order_item\": value,\n              \"inventory\": inventory\n            }\n          resultType: struct\n          stores:\n            - inventory_store\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Looked up inventory: {}\", value)\n      - type: mapValues\n        mapper:\n          code: |\n            # Get the product_id from the key\n            product_id = key\n\n            # Get the ordered item from the value\n            order_item = value.get(\"order_item\")\n            if order_item is None:\n              return None\n\n            # Get the inventory from the value, or set to zero if no inventory found\n            inventory = value.get(\"inventory\")\n\n            # Get the ordered amount\n            ordered_quantity = order_item.get(\"quantity\")\n            if ordered_quantity == 0:\n              return None\n\n            # Update inventory with new stock level\n            current_stock = inventory.get(\"current_stock\", 0)\n            new_stock = max(0, current_stock - ordered_quantity)\n\n            # Update the inventory in the store\n            value = {\n              **inventory,\n              \"current_stock\": new_stock,\n              \"last_order_id\": order_item.get(\"order_id\"),\n              \"last_updated\": int(time.time() * 1000)\n            }\n\n            inventory_store.put(key, value)\n            return value\n          resultType: struct\n          stores:\n            - inventory_store\n      # Only propagate valid inventory updates to the topic\n      - type: filter\n        if:\n          expression: value is not None\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Updating inventory: {}\", value)\n    to: inventory_updates\n\n  # Pipeline for detecting low inventory and generating reorder events\n  reorder_pipeline:\n    from: inventory_updates\n    via:\n      - type: join\n        table: product_catalog\n        valueJoiner:\n          expression: |\n            {\n              \"product\": value2,\n              \"inventory\": value1\n            }\n          resultType: struct\n      - type: filter\n        if:\n          code: |\n            product = value.get(\"product\")\n            inventory = value.get(\"inventory\")\n            return inventory.get(\"current_stock\", 0) &lt;= product.get(\"reorder_threshold\", 0)\n      - type: transformKeyValue\n        mapper:\n          globalCode: |\n            import uuid\n            import time\n          code: |\n            # Get reorder threshold and quantity from product reference data\n            product = value.get(\"product\")\n            reorder_threshold = product.get(\"reorder_threshold\", 10)\n            reorder_quantity = product.get(\"reorder_quantity\", 50)\n\n            # Determine priority based on current stock\n            inventory = value.get(\"inventory\")\n            current_stock = inventory.get(\"current_stock\", 0)\n            priority = \"urgent\" if current_stock &lt;= 5 else \"normal\"\n\n            # Generate event ID\n            event_id = f\"reorder-{uuid.uuid4().hex[:8]}\"\n\n            # Create reorder event\n            value = {\n              \"event_id\": event_id,\n              \"product_id\": product.get(\"id\"),\n              \"product_name\": product.get(\"name\"),\n              \"current_stock\": current_stock,\n              \"reorder_quantity\": reorder_quantity,\n              \"priority\": priority,\n              \"timestamp\": int(time.time() * 1000)\n            }\n          expression: (value.get(\"event_id\"), value)\n          resultType: (string, struct)\n    to: reorder_events\n\n  # Pipeline for detecting critical inventory levels and generating alerts\n  alert_pipeline:\n    from: inventory_updates\n    via:\n      - type: join\n        table: product_catalog\n        valueJoiner:\n          expression: |\n            {\n              \"product\": value2,\n              \"inventory\": value1\n            }\n          resultType: struct\n      - type: filter\n        if:\n          code: |\n            product = value.get(\"product\")\n            inventory = value.get(\"inventory\")\n            return inventory.get(\"current_stock\", 0) &lt;= product.get(\"critical_threshold\", 0)\n      - type: mapValues\n        mapper:\n          code: |\n            import uuid\n            import time\n\n            # Get critical threshold from product reference data\n            product = value.get(\"product\")\n            critical_threshold = product.get(\"critical_threshold\", 5)\n            product_name = product.get(\"name\", \"Unknown\")\n\n            # Get current stock\n            inventory = value.get(\"inventory\")\n            current_stock = inventory.get(\"current_stock\", 0)\n\n            # Generate alert ID\n            alert_id = f\"alert-{uuid.uuid4().hex[:8]}\"\n\n            # Create alert message\n            message = f\"Critical inventory level: {product_name} ({current_stock} units remaining)\"\n\n            # Create alert event\n            return {\n              \"alert_id\": alert_id,\n              \"alert_type\": \"critical_inventory\",\n              \"product_id\": product.get(\"id\"),\n              \"product_name\": product_name,\n              \"current_stock\": current_stock,\n              \"threshold\": critical_threshold,\n              \"timestamp\": int(time.time() * 1000),\n              \"message\": message\n            }\n          resultType: struct\n    to: inventory_alerts\n\n  restock_pipeline:\n    from: reorder_events\n    via:\n      - type: transformKey\n        mapper:\n          expression: value.get(\"product_id\")\n          resultType: string\n      - type: transformValue\n        mapper:\n          code: |\n            inventory = inventory_store.get(key)\n            if not inventory:\n              inventory = {\n                \"product_id\": key,\n                \"current_stock\": 0\n              }\n            return {\n              \"reorder\": value,\n              \"inventory\": inventory\n            }\n          resultType: struct\n          stores:\n            - inventory_store\n      - type: transformValue\n        mapper:\n          code: |\n            reorder = value.get(\"reorder\")\n            inventory = value.get(\"inventory\")\n            value = {\n              **inventory,\n              \"current_stock\": inventory.get(\"current_stock\", 0) + reorder.get(\"reorder_quantity\", 0),\n              \"last_reorder_id\": reorder.get(\"event_id\"),\n              \"last_updated\": int(time.time() * 1000)\n            }\n\n            inventory_store.put(key, value)\n\n            return value\n          resultType: struct\n          stores:\n            - inventory_store\n      - type: peek\n        forEach:\n          code: |\n            log.info(\"Restocked product: {}\", value)\n    to: inventory_updates\n\n  inventory_monitor:\n    from: inventory_updates\n    forEach:\n      code: |\n        log.info(\"Inventory updated: {}\", value)\n</code></pre>"},{"location":"use-cases/event-driven-applications/#setting-up-the-producers-for-test-data","title":"Setting up the producers for test data","text":"<p>To test out the topology above, we create a test data producer definition.</p> <p>The definition consists of two producers. The first producer is a single shot producer that generates three records for the <code>product_catalog</code> topic. The second producer produces a message every second to the <code>order_events</code> topic, using a randomly generated product order:</p> Product and order event producer (click to expand) <pre><code># Product and Order Event Producer\n# This producer is part of the Event-Driven Applications Use Case in KSML documentation.\n# It produces a fixed series of records to the product_catalog topic.\n# Next to that, every second a random product order is generated and sent to the order_events topic.\n\nfunctions:\n  global_function:\n    globalCode: |\n      # Global product catalog\n      products = [\n        {\"id\": \"prod-123\", \"name\": \"Wireless Headphones\", \"category\": \"electronics\", \"price\": 79.99, \"reorder_threshold\": 10, \"reorder_quantity\": 50, \"critical_threshold\": 5},\n        {\"id\": \"prod-456\", \"name\": \"Laptop Charger\", \"category\": \"electronics\", \"price\": 55.99, \"reorder_threshold\": 5, \"reorder_quantity\": 25, \"critical_threshold\": 3},\n        {\"id\": \"prod-789\", \"name\": \"Phone Cover\", \"category\": \"accessories\", \"price\": 12.99, \"reorder_threshold\": 20, \"reorder_quantity\": 100, \"critical_threshold\": 10}\n      ]\n\n  product_catalog_generator:\n    globalCode: |\n      import time\n      count = 0\n    code: |\n      global count\n      count = (count + 1) % len(products)\n      value = products[count]\n      key = value.get(\"id\")\n      return (key, value)\n    resultType: (string, struct)\n\n  order_event_generator:\n    globalCode: |\n      import random\n    code: |\n      items = []\n      total_price = 0\n      for item in range(random.randrange(1,4)):\n        product = random.choice(products)\n        quantity = random.randrange(1,10)\n        price = product.get(\"price\") * quantity\n        total_price += price\n        items += [{\n          \"product_id\": product.get(\"id\"),\n          \"quantity\": quantity,\n          \"unit_price\": product.get(\"price\")\n        }]\n\n      # Return order event\n      value = {\n        \"order_id\": \"order-\"+str(random.randrange(999999)),\n        \"customer_id\": \"cust-\"+str(random.randrange(999999)),\n        \"items\": items,\n        \"order_total\": total_price,\n        \"timestamp\": int(time.time() * 1000)\n      }\n    expression: (value.get(\"order_id\"), value)\n    resultType: (string, struct)\n\nproducers:\n  product_catalog_producer:\n    generator: product_catalog_generator\n    interval: 1\n    count: 3\n    to:\n      topic: product_catalog\n      keyType: string  # Product id\n      valueType: json  # Product information including reorder info\n\n  order_event_producer:\n    generator: order_event_generator\n    interval: 1s\n    to:\n      topic: order_events\n      keyType: string  # stock key (warehouse id + product id)\n      valueType: json  # JSON inventory data\n</code></pre>"},{"location":"use-cases/event-driven-applications/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Save the processor definition to    <code>inventory-event-processors.yaml</code>.</li> <li>Save the producers to    <code>product-and-order-event-producer.yaml</code></li> <li>Set up your <code>ksml-runner.yaml</code> configuration, pointing to your Kafka installation.</li> </ol> KSML runner configuration (click to expand) <pre><code>ksml:\n  enableProducers: true              # Set to true to allow producer definitions to be parsed in the KSML definitions and be executed.\n  enablePipelines: true              # Set to true to allow pipeline definitions to be parsed in the KSML definitions and be executed.\n  definitions:\n    segment_data_producer: segment-data-producer.yaml\n    customer_data_producer: customer-data-producer.yaml\n    data_transformation: data-transformation.yaml\nkafka:\n  bootstrap.servers: broker:9093\n  application.id: your.app.id\n</code></pre> <ol> <li>Start the <code>customer_segment_producer</code> to produce the sample segment information to Kafka.</li> <li>Start the <code>legacy_customer_data_producer</code> to produce some sample data to the input topic.</li> <li>Start the <code>data_transformation</code> topology to initiate the continuous data transformation logic.</li> <li>Monitor the output topic to see the transformed data.</li> </ol>"},{"location":"use-cases/event-driven-applications/#extending-the-event-driven-system","title":"Extending the Event-Driven System","text":""},{"location":"use-cases/event-driven-applications/#integration-with-external-systems","title":"Integration with External Systems","text":"<p>To make this event-driven system truly useful, you can integrate it with external systems:</p> <ol> <li>Procurement System: Connect the reorder events to your procurement system to automatically create purchase orders</li> <li>Notification Service: Send the alerts to a notification service that can email or text warehouse staff</li> <li>Analytics Platform: Stream all events to an analytics platform for business intelligence</li> <li>Dashboard: Connect to a real-time dashboard for inventory visualization</li> </ol>"},{"location":"use-cases/event-driven-applications/#adding-more-event-types","title":"Adding More Event Types","text":"<p>You can extend the system with additional event types:</p> <ul> <li>Price Change Events: Automatically adjust prices based on inventory levels or competitor data</li> <li>Promotion Events: Trigger promotions for overstocked items</li> <li>Fraud Detection Events: Flag suspicious order patterns</li> <li>Shipping Delay Events: Notify customers about potential delays due to inventory issues</li> </ul>"},{"location":"use-cases/event-driven-applications/#best-practices-for-event-driven-applications","title":"Best Practices for Event-Driven Applications","text":"<p>When building event-driven applications with KSML, consider these best practices:</p> <ol> <li>Event Schema Design: Design your events to be self-contained and include all necessary context</li> <li>Idempotent Processing: Ensure your event handlers can process the same event multiple times without side effects</li> <li>Event Versioning: Include version information in your events to handle schema evolution</li> <li>Monitoring and Observability: Add logging and metrics to track event flow and processing</li> <li>Error Handling: Implement proper error handling and dead-letter queues for failed events</li> </ol>"},{"location":"use-cases/event-driven-applications/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to:</p> <ul> <li>Build an event-driven application using KSML</li> <li>Detect specific conditions in your data streams</li> <li>Generate events in response to those conditions</li> <li>Process events to update state and trigger further actions</li> <li>Design an end-to-end event-driven architecture</li> </ul> <p>Event-driven applications are a powerful use case for KSML, allowing you to build responsive, real-time systems that react automatically to changing conditions.</p>"},{"location":"use-cases/event-driven-applications/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Real-Time Analytics to analyze your event data</li> <li>Explore Data Transformation for more complex event processing</li> <li>Check out External Integration for connecting your events to external   systems</li> </ul>"},{"location":"use-cases/fraud-detection/","title":"Fraud Detection with KSML","text":"<p>This guide demonstrates how to build a real-time fraud detection system using KSML. You'll learn how to analyze transaction streams, identify suspicious patterns, and generate alerts for potential fraud.</p>"},{"location":"use-cases/fraud-detection/#introduction","title":"Introduction","text":"<p>Fraud detection is a critical application of stream processing, allowing organizations to identify and respond to fraudulent activities in real-time. Key benefits include:</p> <ul> <li>Minimizing financial losses by detecting fraud as it happens</li> <li>Reducing false positives through multi-factor analysis</li> <li>Adapting to evolving fraud patterns with flexible detection rules</li> <li>Providing immediate alerts to security teams and customers</li> </ul> <p>KSML provides powerful capabilities for building sophisticated fraud detection systems that can process high volumes of transactions and apply complex detection algorithms in real-time.</p>"},{"location":"use-cases/fraud-detection/#prerequisites","title":"Prerequisites","text":"<p>Before starting this guide, you should:</p> <ul> <li>Understand basic KSML concepts (streams, functions, pipelines)</li> <li>Have completed the KSML Basics Tutorial</li> <li>Be familiar with Aggregations</li> <li>Have a basic understanding of Windowed Operations</li> <li>Be familiar with State Stores</li> </ul>"},{"location":"use-cases/fraud-detection/#the-use-case","title":"The Use Case","text":"<p>Imagine you're building a fraud detection system for a financial institution that processes millions of credit card transactions daily. You want to:</p> <ol> <li>Identify suspicious transactions based on multiple risk factors</li> <li>Track unusual patterns in customer behavior</li> <li>Generate real-time alerts for high-risk activities</li> <li>Maintain a low rate of false positives</li> </ol>"},{"location":"use-cases/fraud-detection/#define-the-topics-for-the-use-case","title":"Define the topics for the use case","text":"<p>In earlier tutorials, you created a Docker Compose file with all the necessary containers. For this use case guide, some other topics are needed. To have these created, open the <code>docker-compose.yml</code> in the examples directory, and find the definitions for the <code>kafka-setup</code> container which creates the topics.  Change the definition so that the startup command for the setup container (the <code>command</code> section) looks like the following:</p> <code>command</code> section for the kafka-setup container (click to expand) <pre><code>command: \"bash -c 'echo Creating topics... &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic credit_card_transactions &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic high_value_transactions &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic unusual_location_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic transaction_velocity_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic transaction_pattern_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic fraud_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic fraud_notifications'\"\n</code></pre>"},{"location":"use-cases/fraud-detection/#defining-the-data-model","title":"Defining the Data Model","text":"<p>Our transaction data will have the following structure:</p> <pre><code>{\n  \"transaction_id\": \"tx-123456\",\n  \"timestamp\": 1625097600000,\n  \"card_id\": \"card-789\",\n  \"customer_id\": \"cust-456\",\n  \"merchant\": {\n    \"id\": \"merch-123\",\n    \"name\": \"Online Electronics Store\",\n    \"category\": \"electronics\",\n    \"location\": {\n      \"country\": \"US\",\n      \"state\": \"CA\",\n      \"city\": \"San Francisco\"\n    }\n  },\n  \"amount\": 299.99,\n  \"currency\": \"USD\",\n  \"transaction_type\": \"online\",\n  \"ip_address\": \"203.0.113.45\"\n}\n</code></pre>"},{"location":"use-cases/fraud-detection/#creating-the-ksml-definition","title":"Creating the KSML Definition","text":"<p>Now, let's create our KSML definition file. This defines operations that check for transactions coming from unusual locations, coming at an unsual speed, or are for a high amount:</p> Basic fraud detection pipeline (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\nstreams:\n  transactions:\n    topic: credit_card_transactions\n    keyType: string  # transaction_id\n    valueType: json  # transaction data\n\n  high_value_transactions:\n    topic: high_value_transactions\n    keyType: string  # transaction_id\n    valueType: json  # transaction data\n\n  unusual_location_alerts:\n    topic: unusual_location_alerts\n    keyType: string  # card_id\n    valueType: json  # alert data\n\n  velocity_alerts:\n    topic: transaction_velocity_alerts\n    keyType: string  # card_id\n    valueType: json  # alert data\n\n  fraud_alerts:\n    topic: fraud_alerts\n    keyType: string  # transaction_id\n    valueType: json  # consolidated alert data\n\nstores:\n  customer_transaction_history:\n    type: keyValue\n    persistent: true\n    keyType: string\n    valueType: json\n\n  card_location_history:\n    type: keyValue\n    persistent: true\n    keyType: string\n    valueType: json\n\nfunctions:\n  check_high_value:\n    type: predicate\n    code: |\n      # Define thresholds for different merchant categories\n      thresholds = {\n        \"electronics\": 1000,\n        \"jewelry\": 2000,\n        \"travel\": 3000,\n        \"default\": 500\n      }\n\n      category = value.get(\"merchant\", {}).get(\"category\", \"default\")\n      threshold = thresholds.get(category, thresholds[\"default\"])\n\n      return value.get(\"amount\", 0) &gt; threshold\n\n  create_high_value_alert:\n    type: valueTransformer\n    code: |\n      result = {\n        \"transaction_id\": value.get(\"transaction_id\"),\n        \"timestamp\": value.get(\"timestamp\"),\n        \"customer_id\": value.get(\"customer_id\"),\n        \"card_id\": value.get(\"card_id\"),\n        \"merchant\": value.get(\"merchant\"),\n        \"amount\": value.get(\"amount\"),\n        \"alert_type\": \"high_value_transaction\",\n        \"risk_score\": min(100, value.get(\"amount\") / 10)  # Simple scoring based on amount\n      }\n    expression: result\n    resultType: struct\n\n  check_unusual_location:\n    type: valueTransformer\n    code: |\n      card_id = value.get(\"card_id\")\n      current_country = value.get(\"merchant\", {}).get(\"location\", {}).get(\"country\")\n      current_state = value.get(\"merchant\", {}).get(\"location\", {}).get(\"state\")\n\n      # Get location history for this card\n      location_history = card_location_history.get(card_id)\n      unusual_location = False\n\n      if location_history is None:\n        # First transaction for this card, initialize history\n        location_history = {\n          \"last_countries\": [current_country],\n          \"last_states\": [current_state],\n          \"last_transaction_time\": value.get(\"timestamp\")\n        }\n        card_location_history.put(card_id, location_history)\n      else:\n        # Check for unusual location\n        time_since_last = value.get(\"timestamp\") - location_history.get(\"last_transaction_time\", 0)\n\n        # If transaction is in a different country than any in history\n        if current_country not in location_history.get(\"last_countries\", []):\n          unusual_location = True\n\n        # If transaction is in a different state and happened within 2 hours of last transaction\n        elif (current_state not in location_history.get(\"last_states\", []) and time_since_last &lt; 7200000):  # 2 hours in milliseconds\n          unusual_location = True\n\n        # Update location history (keep last 3)\n        last_countries = location_history.get(\"last_countries\", [])\n        if current_country not in last_countries:\n          last_countries.append(current_country)\n        if len(last_countries) &gt; 3:\n          last_countries = last_countries[-3:]\n\n        last_states = location_history.get(\"last_states\", [])\n        if current_state not in last_states:\n          last_states.append(current_state)\n        if len(last_states) &gt; 3:\n          last_states = last_states[-3:]\n\n        location_history = {\n          \"last_countries\": last_countries,\n          \"last_states\": last_states,\n          \"last_transaction_time\": value.get(\"timestamp\")\n        }\n        card_location_history.put(card_id, location_history)\n\n      if unusual_location:\n        result = {\n          \"transaction_id\": value.get(\"transaction_id\"),\n          \"timestamp\": value.get(\"timestamp\"),\n          \"customer_id\": value.get(\"customer_id\"),\n          \"card_id\": card_id,\n          \"current_location\": {\n            \"country\": current_country,\n            \"state\": current_state\n          },\n          \"previous_locations\": {\n            \"countries\": location_history.get(\"last_countries\", [])[:-1],\n            \"states\": location_history.get(\"last_states\", [])[:-1]\n          },\n          \"time_since_last_transaction\": time_since_last,\n          \"alert_type\": \"unusual_location\",\n          \"risk_score\": 70 if current_country not in location_history.get(\"last_countries\", [])[:-1] else 40\n        }\n      else:\n        result = None\n\n    resultType: json\n    expression: result\n    stores:\n      - card_location_history\n\n  check_transaction_velocity:\n    type: valueTransformer\n    code: |\n      card_id = value.get(\"card_id\")\n      current_time = value.get(\"timestamp\")\n      result = None\n\n      # Get transaction history for this card\n      history = customer_transaction_history.get(card_id)\n\n      if history is None:\n        # First transaction for this card, initialize history\n        history = {\n          \"transaction_times\": [current_time],\n          \"transaction_count_1h\": 1,\n          \"transaction_count_24h\": 1,\n          \"total_amount_24h\": value.get(\"amount\", 0)\n        }\n        customer_transaction_history.put(card_id, history)\n        result = None  # No alert for first transaction\n      else:\n\n        # Update transaction history\n        transaction_times = history.get(\"transaction_times\", [])\n        transaction_times.append(current_time)\n\n        # Keep only transactions from the last 24 hours\n        one_day_ago = current_time - 86400000  # 24 hours in milliseconds\n        transaction_times = [t for t in transaction_times if t &gt; one_day_ago]\n\n        # Count transactions in the last hour\n        one_hour_ago = current_time - 3600000  # 1 hour in milliseconds\n        transaction_count_1h = sum(1 for t in transaction_times if t &gt; one_hour_ago)\n\n        # Calculate total amount in the last 24 hours\n        total_amount_24h = history.get(\"total_amount_24h\", 0) + value.get(\"amount\", 0)\n        if len(transaction_times) &lt; len(history.get(\"transaction_times\", [])):\n          # Some transactions dropped out of the 24h window, recalculate total\n          # In a real system, you would store individual transaction amounts\n          # This is simplified for the example\n          total_amount_24h = value.get(\"amount\", 0) * len(transaction_times)\n\n        # Update history\n        history = {\n          \"transaction_times\": transaction_times,\n          \"transaction_count_1h\": transaction_count_1h,\n          \"transaction_count_24h\": len(transaction_times),\n          \"total_amount_24h\": total_amount_24h\n        }\n        customer_transaction_history.put(card_id, history)\n\n        # More than 5 transactions in an hour\n        if transaction_count_1h &gt; 5:\n          result = {\n            \"transaction_id\": value.get(\"transaction_id\"),\n            \"timestamp\": value.get(\"timestamp\"),\n            \"customer_id\": value.get(\"customer_id\"),\n            \"card_id\": card_id,\n            \"transactions_last_hour\": transaction_count_1h,\n            \"transactions_last_day\": len(transaction_times),\n            \"total_amount_24h\": total_amount_24h,\n            \"alert_type\": \"high_transaction_velocity\",\n            \"risk_score\": min(100, transaction_count_1h * 10)\n          }\n\n    resultType: json\n    expression: result\n    stores:\n      - customer_transaction_history\n\n  calculate_fraud_score:\n    type: valueTransformer\n    code: |\n      # Base risk score from the alert\n      risk_score = value.get(\"risk_score\", 0)\n\n      # Additional factors\n      alert_type = value.get(\"alert_type\", \"\")\n\n      # Adjust score based on transaction type\n      if value.get(\"transaction_type\") == \"online\":\n        risk_score += 10\n\n      # Adjust score based on merchant category\n      high_risk_categories = [\"electronics\", \"jewelry\", \"digital_goods\"]\n      if value.get(\"merchant\", {}).get(\"category\") in high_risk_categories:\n        risk_score += 15\n\n      # Cap the score at 100\n      risk_score = min(100, risk_score)\n\n      # Add the calculated score to the alert\n      value[\"final_risk_score\"] = risk_score\n      value[\"is_likely_fraud\"] = risk_score &gt; 70\n\n    expression: value\n    resultType: json\n\npipelines:\n  # Pipeline for high-value transaction detection\n  high_value_detection:\n    from: transactions\n    via:\n      - type: filter\n        if: check_high_value\n      - type: transformValue\n        mapper: create_high_value_alert\n    to: high_value_transactions\n\n  # Pipeline for unusual location detection\n  unusual_location_detection:\n    from: transactions\n    via:\n      - type: transformValue\n        mapper: check_unusual_location\n      - type: filter\n        if:\n          expression: value is not None\n    to: unusual_location_alerts\n\n  # Pipeline for transaction velocity monitoring\n  velocity_monitoring:\n    from: transactions\n    via:\n      - type: transformValue\n        mapper: check_transaction_velocity\n      - type: filter\n        if:\n          expression: value is not None\n    to: velocity_alerts\n\n  # Pipeline for consolidating alerts and calculating final fraud score\n  fraud_scoring_high_value:\n    from: high_value_transactions\n    via:\n      - type: transformValue\n        mapper: calculate_fraud_score\n    to: fraud_alerts\n\n  fraud_scoring_location:\n    from: unusual_location_alerts\n    via:\n      - type: transformValue\n        mapper: calculate_fraud_score\n    to: fraud_alerts\n\n  fraud_scoring_velocity:\n    from: velocity_alerts\n    via:\n      - type: transformValue\n        mapper: calculate_fraud_score\n    to: fraud_alerts\n</code></pre>"},{"location":"use-cases/fraud-detection/#advanced-fraud-detection-techniques","title":"Advanced Fraud Detection Techniques","text":""},{"location":"use-cases/fraud-detection/#pattern-recognition","title":"Pattern Recognition","text":"<p>To detect complex fraud patterns, you can implement more sophisticated algorithms:</p> Pattern recognition processor (click to expand) <pre><code>functions:\n  detect_fraud_pattern:\n    type: valueTransformer\n    code: |\n      # Pattern: Small test transaction followed by large transaction\n      card_id = value.get(\"card_id\")\n      current_amount = value.get(\"amount\", 0)\n\n      # Get transaction history\n      history = customer_transaction_history.get(card_id)\n\n      if history is None or \"recent_transactions\" not in history:\n        # Initialize history\n        history = {\"recent_transactions\": [{\"amount\": current_amount, \"time\": value.get(\"timestamp\")}]}\n        customer_transaction_history.put(card_id, history)\n        return None\n\n      # Add current transaction to history\n      recent_transactions = history.get(\"recent_transactions\", [])\n      recent_transactions.append({\"amount\": current_amount, \"time\": value.get(\"timestamp\")})\n\n      # Keep only recent transactions (last 24 hours)\n      one_day_ago = value.get(\"timestamp\") - 86400000\n      recent_transactions = [t for t in recent_transactions if t.get(\"time\", 0) &gt; one_day_ago]\n\n      # Sort by time\n      recent_transactions.sort(key=lambda x: x.get(\"time\", 0))\n\n      # Look for pattern: small transaction (&lt; $5) followed by large transaction within 30 minutes\n      pattern_found = False\n      for i in range(len(recent_transactions) - 1):\n        if (recent_transactions[i].get(\"amount\", 0) &lt; 5 and \n            recent_transactions[i+1].get(\"amount\", 0) &gt; 100 and\n            recent_transactions[i+1].get(\"time\", 0) - recent_transactions[i].get(\"time\", 0) &lt; 1800000):  # 30 minutes\n          pattern_found = True\n          break\n\n      # Update history\n      history[\"recent_transactions\"] = recent_transactions\n      customer_transaction_history.put(card_id, history)\n\n      if pattern_found:\n        return {\n          \"transaction_id\": value.get(\"transaction_id\"),\n          \"timestamp\": value.get(\"timestamp\"),\n          \"customer_id\": value.get(\"customer_id\"),\n          \"card_id\": card_id,\n          \"alert_type\": \"fraud_pattern_detected\",\n          \"pattern_type\": \"test_then_charge\",\n          \"risk_score\": 85\n        }\n\n      return None\n    resultType: struct\n    stores:\n      - customer_transaction_history\n\n  calculate_fraud_score:\n    type: valueTransformer\n    code: |\n      # Base risk score from the alert\n      risk_score = value.get(\"risk_score\", 0)\n\n      # Additional factors\n      alert_type = value.get(\"alert_type\", \"\")\n\n      # Adjust score based on transaction type\n      if value.get(\"transaction_type\") == \"online\":\n        risk_score += 10\n\n      # Adjust score based on merchant category\n      high_risk_categories = [\"electronics\", \"jewelry\", \"digital_goods\"]\n      if value.get(\"merchant\", {}).get(\"category\") in high_risk_categories:\n        risk_score += 15\n\n      # Cap the score at 100\n      risk_score = min(100, risk_score)\n\n      # Add the calculated score to the alert\n      value[\"final_risk_score\"] = risk_score\n      value[\"is_likely_fraud\"] = risk_score &gt; 70\n</code></pre>"},{"location":"use-cases/fraud-detection/#machine-learning-integration","title":"Machine Learning Integration","text":"<p>For more advanced fraud detection, you can integrate machine learning models:</p> Machine learning fraud detector (click to expand) <pre><code>functions:\n  ml_fraud_prediction:\n    type: valueTransformer\n    parameters:\n      - name: value\n        type: object\n    code: |\n      # In a real implementation, you would call an external ML service\n      # This is a simplified example\n\n      # Extract features\n      features = {\n        \"amount\": value.get(\"amount\", 0),\n        \"is_international\": value.get(\"merchant\", {}).get(\"location\", {}).get(\"country\") != \"US\",\n        \"is_online\": value.get(\"transaction_type\") == \"online\",\n        \"high_risk_merchant\": value.get(\"merchant\", {}).get(\"category\") in [\"electronics\", \"jewelry\", \"digital_goods\"],\n        \"transaction_hour\": (value.get(\"timestamp\", 0) / 3600000) % 24,  # Hour of day\n        \"transaction_count_1h\": value.get(\"transactions_last_hour\", 1),\n        \"transaction_count_24h\": value.get(\"transactions_last_day\", 1)\n      }\n\n      # Simple rule-based model (in reality, this would be a trained ML model)\n      score = 0\n      if features[\"amount\"] &gt; 1000: score += 20\n      if features[\"is_international\"]: score += 15\n      if features[\"is_online\"]: score += 10\n      if features[\"high_risk_merchant\"]: score += 15\n      if features[\"transaction_hour\"] &gt; 22 or features[\"transaction_hour\"] &lt; 6: score += 10\n      if features[\"transaction_count_1h\"] &gt; 3: score += 15\n      if features[\"transaction_count_24h\"] &gt; 10: score += 15\n\n      # Normalize score to 0-100\n      score = min(100, score)\n\n      # Add prediction to the transaction\n      value[\"ml_fraud_score\"] = score\n      value[\"ml_fraud_probability\"] = score / 100.0\n      value[\"ml_is_fraud\"] = score &gt; 60\n\n      return value\n</code></pre>"},{"location":"use-cases/fraud-detection/#real-time-alerting","title":"Real-time Alerting","text":"<p>To make your fraud detection system actionable, you need to generate alerts:</p> Real-time alerting processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/release/1.0.x/docs/ksml-language-spec.json\n\nstreams:\n  fraud_alerts:\n    topic: fraud_alerts\n    keyType: string  # transaction_id\n    valueType: json  # consolidated alert data\n\n  fraud_notifications:\n    topic: fraud_notifications\n    keyType: string  # transaction_id\n    valueType: json  # notifications to be sent out\n\nfunctions:\n  generate_notification:\n    type: valueTransformer\n    code: |\n      risk_score = value.get(\"final_risk_score\", 0)\n\n      # Determine alert level\n      alert_level = \"low\"\n      if risk_score &gt; 70:\n        alert_level = \"high\"\n      elif risk_score &gt; 40:\n        alert_level = \"medium\"\n\n      # Create alert message\n      alert = {\n        \"transaction_id\": value.get(\"transaction_id\"),\n        \"timestamp\": value.get(\"timestamp\"),\n        \"customer_id\": value.get(\"customer_id\"),\n        \"card_id\": value.get(\"card_id\"),\n        \"merchant\": value.get(\"merchant\"),\n        \"amount\": value.get(\"amount\"),\n        \"alert_type\": value.get(\"alert_type\"),\n        \"risk_score\": risk_score,\n        \"alert_level\": alert_level,\n        \"alert_message\": f\"Potential fraud detected: {value.get('alert_type')} with risk score {risk_score}\",\n        \"recommended_action\": \"block\" if alert_level == \"high\" else \"review\"\n      }\n\n      return alert\n\npipelines:\n  alert_generation:\n    from: fraud_alerts\n    via:\n      - type: filter\n        if:\n          expression: value.get(\"final_risk_score\", 0) &gt; 30  # Only notify on medium to high risk\n      - type: transformValue\n        mapper: generate_notification\n    to: fraud_notifications\n</code></pre>"},{"location":"use-cases/fraud-detection/#testing-and-validation","title":"Testing and Validation","text":"<p>To test your fraud detection system:</p> <ol> <li>Generate sample transaction data with known fraud patterns</li> <li>Deploy your KSML application using the proper configuration</li> <li>Monitor the alert topics to verify detection accuracy</li> <li>Adjust thresholds and rules to balance detection rate and false positives</li> </ol>"},{"location":"use-cases/fraud-detection/#production-considerations","title":"Production Considerations","text":"<p>When deploying fraud detection systems to production:</p> <ol> <li>Performance: Ensure your system can handle peak transaction volumes</li> <li>Latency: Minimize processing time to detect fraud quickly</li> <li>False Positives: Continuously tune your system to reduce false alarms</li> <li>Adaptability: Implement mechanisms to update rules and models as fraud patterns evolve</li> <li>Compliance: Ensure your system meets regulatory requirements for financial monitoring</li> <li>Security: Protect sensitive transaction data with appropriate encryption and access controls</li> </ol>"},{"location":"use-cases/fraud-detection/#conclusion","title":"Conclusion","text":"<p>KSML provides a powerful platform for building sophisticated fraud detection systems that can process high volumes of transactions in real-time. By combining multiple detection techniques, including pattern recognition, anomaly detection, and machine learning, you can create a robust system that effectively identifies fraudulent activities while minimizing false positives.</p> <p>For more advanced fraud detection scenarios, explore:</p> <ul> <li>Complex Event Processing for detecting multi-stage fraud patterns</li> <li>External Service Integration for incorporating third-party risk scores</li> <li>KSML Definition Reference for a full explanation of the KSML definition syntax</li> </ul>"},{"location":"use-cases/iot-data-processing/","title":"IoT Data Processing with KSML","text":"<p>This guide demonstrates how to build an IoT data processing application using KSML. You'll learn how to ingest, process, and analyze high-volume sensor data from IoT devices.</p>"},{"location":"use-cases/iot-data-processing/#introduction","title":"Introduction","text":"<p>Internet of Things (IoT) applications generate massive amounts of data from distributed sensors and devices. Processing this data in real-time presents several challenges:</p> <ul> <li>Handling high-volume, high-velocity data streams</li> <li>Managing device state and context</li> <li>Processing geospatial data</li> <li>Implementing edge-to-cloud data pipelines</li> <li>Detecting anomalies and patterns in sensor readings</li> </ul> <p>KSML provides powerful capabilities for building scalable IoT data processing pipelines that address these challenges.</p>"},{"location":"use-cases/iot-data-processing/#prerequisites","title":"Prerequisites","text":"<p>Before starting this guide, you should:</p> <ul> <li>Understand basic KSML concepts (streams, functions, pipelines)</li> <li>Have completed the KSML Basics Tutorial</li> <li>Be familiar with Aggregations</li> <li>Have a basic understanding of State Stores</li> </ul>"},{"location":"use-cases/iot-data-processing/#the-use-case","title":"The Use Case","text":"<p>Imagine you're managing a smart building system with thousands of IoT sensors monitoring:</p> <ul> <li>Temperature and humidity in different rooms</li> <li>Energy consumption of various appliances and systems</li> <li>Occupancy and movement patterns</li> <li>Air quality metrics</li> </ul> <p>You want to process this data in real-time to:</p> <ol> <li>Monitor building conditions and detect anomalies</li> <li>Optimize energy usage based on occupancy patterns</li> <li>Track device health and identify maintenance needs</li> <li>Generate aggregated analytics for building performance</li> </ol>"},{"location":"use-cases/iot-data-processing/#define-the-topics-for-the-use-case","title":"Define the topics for the use case","text":"<p>In earlier tutorials, you created a Docker Compose file with all the necessary containers. For this use case guide, some other topics are needed. To have these created, open the <code>docker-compose.yml</code> in the examples directory, and find the definitions for the <code>kafka-setup</code> container which creates the topics.  Change the definition so that the startup command for the setup container (the <code>command</code> section) looks like the following:</p> <code>command</code> section for the kafka-setup container (click to expand) <pre><code>command: \"bash -c 'echo Creating topics... &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic iot_sensor_readings &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic temperature_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic device_status &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic energy_consumption &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic proximity_alerts &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic building_analytics &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic edge_processed_readings &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic room_temperature_stats'\"\n</code></pre>"},{"location":"use-cases/iot-data-processing/#defining-the-data-model","title":"Defining the Data Model","text":"<p>Our IoT sensor data will have the following structure:</p> <pre><code>{\n  \"device_id\": \"sensor-123\",\n  \"timestamp\": 1625097600000,\n  \"device_type\": \"temperature_sensor\",\n  \"location\": {\n    \"building\": \"headquarters\",\n    \"floor\": 3,\n    \"room\": \"conference-a\",\n    \"coordinates\": {\n      \"lat\": 37.7749,\n      \"lng\": -122.4194\n    }\n  },\n  \"readings\": {\n    \"temperature\": 22.5,\n    \"humidity\": 45.2,\n    \"energy\": 70\n  },\n  \"battery_level\": 87,\n  \"status\": \"active\"\n}\n</code></pre>"},{"location":"use-cases/iot-data-processing/#creating-the-ksml-definition","title":"Creating the KSML Definition","text":"<p>Now, let's create our KSML definition file:</p> IoT data processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  sensor_readings:\n    topic: iot_sensor_readings\n    keyType: string  # device_id\n    valueType: json  # sensor data\n\n  temperature_alerts:\n    topic: temperature_alerts\n    keyType: string  # device_id\n    valueType: json  # alert data\n\n  device_status:\n    topic: device_status\n    keyType: string  # device_id\n    valueType: json  # status information\n\n  energy_consumption:\n    topic: energy_consumption\n    keyType: string  # location\n    valueType: json  # aggregated energy data\n\n  building_analytics:\n    topic: building_analytics\n    keyType: string  # building/floor\n    valueType: json  # aggregated analytics\n\nfunctions:\n  extract_temperature:\n    type: valueTransformer\n    resultType: float\n    expression: value.get(\"readings\", {}).get(\"temperature\")\n\n  check_temperature_threshold:\n    type: predicate\n    code: |\n      temp = value.get(\"readings\", {}).get(\"temperature\")\n      return temp is not None and (temp &gt; 30.0 or temp &lt; 10.0)\n\n  create_temperature_alert:\n    type: valueTransformer\n    resultType: json\n    code: |\n      temp = value.get(\"readings\", {}).get(\"temperature\")\n      return {\n        \"device_id\": value.get(\"device_id\"),\n        \"timestamp\": value.get(\"timestamp\"),\n        \"location\": value.get(\"location\"),\n        \"alert_type\": \"temperature_threshold\",\n        \"reading\": temp,\n        \"status\": \"critical\" if (temp &gt; 35.0 or temp &lt; 5.0) else \"warning\"\n      }\n\n  check_battery_level:\n    type: predicate\n    expression: value.get(\"battery_level\", 100) &lt; 20\n\n  create_battery_alert:\n    type: valueTransformer\n    resultType: json\n    code: |\n      return {\n        \"device_id\": value.get(\"device_id\"),\n        \"timestamp\": value.get(\"timestamp\"),\n        \"location\": value.get(\"location\"),\n        \"alert_type\": \"low_battery\",\n        \"battery_level\": value.get(\"battery_level\"),\n        \"status\": \"warning\"\n      }\n\n  filter_energy_meter:\n    type: predicate\n    resultType: boolean\n    code: |\n      return value.get(\"device_type\") == \"energy_meter\"\n\n  group_by_building_floor:\n    type: keyTransformer\n    resultType: string\n    code: |\n      return value.get(\"location\", {}).get(\"building\") + \"-\" + str(value.get(\"location\", {}).get(\"floor\"))\n\n  init_energy_counter:\n    type: initializer\n    resultType: json\n    code: |\n      return {\"total_consumption\": 0, \"count\": 0}\n\n  aggregate_energy:\n    type: aggregator\n    resultType: json\n    code: |\n      return {\n        \"total_consumption\": aggregatedValue.get(\"total_consumption\") + value.get(\"readings\", {}).get(\"energy\", 0),\n        \"count\": aggregatedValue.get(\"count\") + 1,\n        \"average\": (aggregatedValue.get(\"total_consumption\") + value.get(\"readings\", {}).get(\"energy\", 0)) / (aggregatedValue.get(\"count\") + 1)\n      }\n\npipelines:\n  # Pipeline for temperature monitoring\n  temperature_monitoring:\n    from: sensor_readings\n    via:\n      - type: filter\n        if: check_temperature_threshold\n      - type: transformValue\n        mapper: create_temperature_alert\n    to: temperature_alerts\n\n  # Pipeline for device status monitoring\n  device_status_monitoring:\n    from: sensor_readings\n    via:\n      - type: filter\n        if: check_battery_level\n      - type: transformValue\n        mapper: create_battery_alert\n    to: device_status\n\n  # Pipeline for energy consumption analytics\n  energy_analytics:\n    from: sensor_readings\n    via:\n      - type: filter\n        if: filter_energy_meter\n      - type: groupBy\n        mapper: group_by_building_floor\n      - type: aggregate\n        store:\n          name: energy_consumption_store\n          type: keyValue\n        initializer: init_energy_counter\n        aggregator: aggregate_energy\n      - type: toStream\n    to: energy_consumption\n</code></pre>"},{"location":"use-cases/iot-data-processing/#processing-geospatial-data","title":"Processing Geospatial Data","text":"<p>IoT applications often involve geospatial data processing. Here's how to handle location-based analytics with KSML:</p> Geospatial data processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  sensor_readings:\n    topic: iot_sensor_readings\n    keyType: string\n    valueType: json\n\n  proximity_alerts:\n    topic: proximity_alerts\n    keyType: string\n    valueType: json\n\nfunctions:\n  haversine:\n    type: generic\n    parameters:\n      - name: lat1\n        type: double\n      - name: lon1\n        type: double\n      - name: lat2\n        type: double\n      - name: lon2\n        type: double\n    globalCode: |\n      # Import only once\n      import math\n    code: |\n      R = 6371  # Earth radius in kilometers\n      dlat = math.radians(lat2 - lat1)\n      dlon = math.radians(lon2 - lon1)\n      a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2\n      c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n      distance = R * c\n    expression: distance\n    resultType: double\n\n  calculate_distance:\n    type: valueTransformer\n    code: |\n      # Reference point (e.g., building entrance)\n      ref_lat = 52.0903304\n      ref_lng = 5.1063283\n\n      # Device coordinates\n      device_lat = value.get(\"location\", {}).get(\"coordinates\", {}).get(\"lat\")\n      device_lng = value.get(\"location\", {}).get(\"coordinates\", {}).get(\"lng\")\n\n      if device_lat is not None and device_lng is not None:\n        distance = haversine(ref_lat, ref_lng, device_lat, device_lng)\n        value[\"distance_from_reference\"] = distance\n\n      return value\n\npipelines:\n  proximity_analysis:\n    from: sensor_readings\n    via:\n      - type: transformValue\n        mapper: calculate_distance\n      - type: filter\n        if:\n          expression: value.get(\"distance_from_reference\", float('inf')) &lt; 0.1  # Within 100 meters\n    to: proximity_alerts\n</code></pre>"},{"location":"use-cases/iot-data-processing/#implementing-device-state-tracking","title":"Implementing Device State Tracking","text":"<p>For many IoT applications, tracking device state over time is crucial. Here's how to implement this using KSML's state stores:</p> Device state tracking processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  sensor_readings:\n    topic: iot_sensor_readings\n    keyType: string\n    valueType: json\n\n  device_status:\n    topic: device_status\n    keyType: string\n    valueType: json\n\nstores:\n  device_state_store:\n    type: keyValue\n    persistent: true\n\nfunctions:\n  update_device_state:\n    type: valueTransformer\n    code: |\n      # Get previous state\n      previous_state = device_state_store.get(key)\n\n      # Create new state\n      new_state = {\n        \"device_id\": value.get(\"device_id\"),\n        \"last_reading_time\": value.get(\"timestamp\"),\n        \"current_readings\": value.get(\"readings\"),\n        \"battery_level\": value.get(\"battery_level\"),\n        \"status\": value.get(\"status\")\n      }\n\n      # Add derived fields\n      if previous_state is not None:\n        # Calculate time since last reading\n        time_diff = (value.get(\"timestamp\") - previous_state.get(\"last_reading_time\")) / 1000  # in seconds\n        new_state[\"seconds_since_last_reading\"] = time_diff\n\n        # Calculate rate of change for temperature\n        prev_temp = previous_state.get(\"current_readings\", {}).get(\"temperature\")\n        curr_temp = value.get(\"readings\", {}).get(\"temperature\")\n\n        if prev_temp is not None and curr_temp is not None and time_diff &gt; 0:\n          new_state[\"temperature_change_rate\"] = (curr_temp - prev_temp) / time_diff  # degrees per second\n\n      # Update store\n      device_state_store.put(key, new_state)\n\n      return new_state\n    resultType: struct\n    stores:\n      - device_state_store\n\npipelines:\n  device_state_tracking:\n    from: sensor_readings\n    via:\n      - type: transformValue\n        mapper: update_device_state\n    to: device_status\n</code></pre>"},{"location":"use-cases/iot-data-processing/#edge-to-cloud-processing","title":"Edge-to-Cloud Processing","text":"<p>IoT architectures often involve processing at the edge before sending data to the cloud. KSML can be deployed at both edge and cloud levels:</p>"},{"location":"use-cases/iot-data-processing/#edge-processing","title":"Edge Processing","text":"<p>At the edge, you might want to filter, aggregate, and compress data before sending it to the cloud:</p> Edge processing pipeline (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  raw_sensor_readings:\n    topic: iot_sensor_readings\n    keyType: string\n    valueType: json\n\n  edge_processed_readings:\n    topic: edge_processed_readings\n    keyType: string\n    valueType: json\n\npipelines:\n  edge_preprocessing:\n    from: raw_sensor_readings\n    via:\n      - type: filter\n        if:\n          code: |\n            # Forward temperatures outside comfortable range (more data for cloud analytics)\n            temp = value.get(\"readings\", {}).get(\"temperature\")\n            return temp is not None and (temp &lt; 18.0 or temp &gt; 26.0)\n      - type: transformValue\n        mapper:\n          code: |\n            # Simplify payload for transmission\n            return {\n              \"id\": value.get(\"device_id\"),\n              \"ts\": value.get(\"timestamp\"),\n              \"loc\": value.get(\"location\", {}).get(\"room\"),\n              \"temp\": value.get(\"readings\", {}).get(\"temperature\"),\n              \"hum\": value.get(\"readings\", {}).get(\"humidity\")\n            }\n    to: edge_processed_readings\n</code></pre>"},{"location":"use-cases/iot-data-processing/#cloud-processing","title":"Cloud Processing","text":"<p>In the cloud, you can perform more complex analytics and aggregations:</p> Cloud processing pipeline (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  edge_processed_readings:\n    topic: edge_processed_readings\n    keyType: string\n    valueType: json\n\n  room_temperature_stats:\n    topic: room_temperature_stats\n    keyType: windowed(string)\n    valueType: json\n\nfunctions:\n  expand_abbreviated_fields:\n    type: valueTransformer\n    resultType: json\n    code: |\n      # Expand abbreviated fields\n      return {\n        \"device_id\": value.get(\"id\"),\n        \"timestamp\": value.get(\"ts\"),\n        \"location\": {\"room\": value.get(\"loc\")},\n        \"readings\": {\n          \"temperature\": value.get(\"temp\"),\n          \"humidity\": value.get(\"hum\")\n        }\n      }\n\n  group_by_room:\n    type: keyTransformer\n    resultType: string\n    code: |\n      return value.get(\"location\", {}).get(\"room\")\n\n  init_room_stats:\n    type: initializer\n    resultType: json\n    code: |\n      return {\n        \"room\": None,\n        \"min_temp\": None,\n        \"max_temp\": None,\n        \"avg_temp\": 0.0,\n        \"count\": 0\n      }\n\n  aggregate_room_temp:\n    type: aggregator\n    resultType: json\n    code: |\n      temp = value.get(\"readings\", {}).get(\"temperature\")\n      room = value.get(\"location\", {}).get(\"room\")\n      count = aggregatedValue.get(\"count\")\n\n      # Handle first reading\n      if count == 0:\n        return {\n          \"room\": room,\n          \"min_temp\": temp,\n          \"max_temp\": temp,\n          \"avg_temp\": temp,\n          \"count\": 1\n        }\n      else:\n        return {\n          \"room\": room,\n          \"min_temp\": min(aggregatedValue.get(\"min_temp\"), temp),\n          \"max_temp\": max(aggregatedValue.get(\"max_temp\"), temp),\n          \"avg_temp\": (aggregatedValue.get(\"avg_temp\") * count + temp) / (count + 1),\n          \"count\": count + 1\n        }\n\npipelines:\n  cloud_analytics:\n    from: edge_processed_readings\n    via:\n      - type: transformValue\n        mapper: expand_abbreviated_fields\n      - type: groupBy\n        mapper: group_by_room\n      - type: windowByTime\n        windowType: tumbling\n        duration: 20s\n      - type: aggregate\n        store:\n          name: room_temp_window_store\n          type: window\n          windowSize: 20s\n          retention: 2m\n        initializer: init_room_stats\n        aggregator: aggregate_room_temp\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n    to: room_temperature_stats\n</code></pre>"},{"location":"use-cases/iot-data-processing/#testing-and-validation","title":"Testing and Validation","text":"<p>To test your IoT data processing pipeline:</p> <ol> <li>Generate sample IoT data using a simulator or replay historical data</li> <li>Deploy your KSML application using the proper configuration</li> <li>Monitor the output topics to verify correct processing</li> <li>Use visualization tools to display the processed data</li> </ol> <p>The following producer pipeline can serve as a starting point to generate sample data. This pipeline wll produce sample measurements from three separate rooms with two sensors each, containing randomized data. Occasionally some outlier values are generated so that the alerts will be visible.</p> IoT sample data generator (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  sensor_readings:\n    topic: iot_sensor_readings\n    keyType: string  # device_id\n    valueType: json  # sensor data\n\nfunctions:\n  generate_sensor_reading:\n    type: generic\n    resultType: boolean\n    expression: False\n    globalCode: |\n      import random\n      import time\n\n      # Static data\n      rooms = [\"conference-a\", \"conference-b\", \"conference-c\"]\n      device_types = [\"temperature_sensor\", \"energy_meter\"]\n\n      # Counter for generating unique device IDs\n      global counter\n      counter = 0\n\n      def generate_reading():\n        global counter\n        counter += 1\n\n        # Select room and device type\n        room = rooms[counter % len(rooms)]\n        device_type = device_types[counter % len(device_types)]\n        device_id = f\"{device_type}-{room}\"\n\n        # Generate random readings, generate extreme temperature every 50 readings\n        if counter % 50 == 0:\n          temperature = random.choice([4.0, 36.0])\n        else:\n          temperature = round(random.uniform(19.0, 24.0), 1)\n\n        humidity = round(random.uniform(40.0, 70.0), 1)\n        energy = random.randint(50, 150)\n\n        # Create sensor reading\n        reading = {\n          \"device_id\": device_id,\n          \"timestamp\": int(time.time() * 1000),\n          \"device_type\": device_type,\n          \"location\": {\n            \"building\": \"headquarters\",\n            \"floor\": 3,\n            \"room\": room,\n            \"coordinates\": {\n              \"lat\": 37.7749,\n              \"lng\": -122.4194\n            }\n          },\n          \"readings\": {\n            \"temperature\": temperature,\n            \"humidity\": humidity,\n            \"energy\": energy\n          },\n          \"battery_level\": random.randint(17, 100),\n          \"status\": \"active\"\n        }\n\n        return reading\n\nproducers:\n  iot_sensor_producer:\n    to: sensor_readings\n    interval: 1000  # Generate every second\n    generator:\n      code: |\n        reading = generate_reading()\n      expression: '(reading.get(\"device_id\"), reading)'\n      resultType: (string, json)\n</code></pre>"},{"location":"use-cases/iot-data-processing/#production-considerations","title":"Production Considerations","text":"<p>When deploying IoT data processing pipelines to production:</p> <ol> <li>Scalability: Ensure your Kafka cluster can handle the volume of IoT data</li> <li>Fault Tolerance: Configure proper replication and error handling</li> <li>Data Retention: Set appropriate retention policies for raw and processed data</li> <li>Security: Implement device authentication and data encryption</li> <li>Monitoring: Set up alerts for anomalies in both the data and the processing pipeline</li> <li>Edge Deployment: Consider deploying KSML at the edge for preprocessing</li> </ol>"},{"location":"use-cases/iot-data-processing/#conclusion","title":"Conclusion","text":"<p>KSML provides a powerful and flexible way to process IoT data streams. By combining real-time processing with state management and windowed operations, you can build sophisticated IoT applications that derive valuable insights from your device data.</p> <p>For more advanced IoT scenarios, explore:</p> <ul> <li>Complex Event Processing for pattern detection</li> <li>Custom Functions for domain-specific processing</li> <li>KSML Definition Reference for a full explanation of the KSML definition syntax</li> </ul>"},{"location":"use-cases/real-time-analytics/","title":"Real-Time Analytics with KSML","text":"<p>This tutorial demonstrates how to build a real-time analytics application using KSML. You'll learn how to process streaming data, calculate metrics in real-time, and visualize the results.</p>"},{"location":"use-cases/real-time-analytics/#introduction","title":"Introduction","text":"<p>Real-time analytics is one of the most common use cases for stream processing. By analyzing data as it arrives, you can:</p> <ul> <li>Detect trends and patterns as they emerge</li> <li>Respond quickly to changing conditions</li> <li>Make data-driven decisions with minimal latency</li> <li>Provide up-to-date dashboards and visualizations</li> </ul> <p>In this tutorial, we'll build a real-time analytics pipeline that processes a stream of e-commerce transactions, calculates various metrics, and makes the results available for visualization.</p>"},{"location":"use-cases/real-time-analytics/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, you should:</p> <ul> <li>Understand basic KSML concepts (streams, functions, pipelines)</li> <li>Have completed the KSML Basics Tutorial</li> <li>Be familiar with Aggregations</li> <li>Have a basic understanding of Windowed Operations</li> </ul>"},{"location":"use-cases/real-time-analytics/#the-use-case","title":"The Use Case","text":"<p>Imagine you're running an e-commerce platform and want to analyze transaction data in real-time. You want to track:</p> <ul> <li>Total sales by product category</li> <li>Average order value</li> <li>Transaction volume by region</li> <li>Conversion rates from different marketing channels</li> </ul>"},{"location":"use-cases/real-time-analytics/#define-the-topics-for-the-use-case","title":"Define the topics for the use case","text":"<p>In earlier tutorials, you created a Docker Compose file with all the necessary containers. For this use case guide, some other topics are needed. To have these created, open the <code>docker-compose.yml</code> in the examples directory, and find the definitions for the <code>kafka-setup</code> container which creates the topics.  Change the definition so that the startup command for the setup container (the <code>command</code> section) looks like the following:</p> <code>command</code> section for the kafka-setup container (click to expand) <pre><code>command: \"bash -c 'echo Creating topics... &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic ecommerce_transactions &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic sales_by_category &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic avg_order_value &amp;&amp; \\\n                       kafka-topics.sh --create --if-not-exists --bootstrap-server broker:9093 --partitions 1 --replication-factor 1 --topic transactions_by_region'\"\n</code></pre>"},{"location":"use-cases/real-time-analytics/#defining-the-data-model","title":"Defining the Data Model","text":"<p>Our transaction data will have the following structure:</p> <pre><code>{\n  \"transaction_id\": \"12345\",\n  \"timestamp\": 1625097600000,\n  \"customer_id\": \"cust-789\",\n  \"product_id\": \"prod-456\",\n  \"product_category\": \"electronics\",\n  \"quantity\": 1,\n  \"price\": 499.99,\n  \"region\": \"north_america\",\n  \"marketing_channel\": \"social_media\"\n}\n</code></pre>"},{"location":"use-cases/real-time-analytics/#creating-the-ksml-definition","title":"Creating the KSML Definition","text":"<p>Now, let's create our KSML definition file:</p> Real-time analytics processor (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  transactions:\n    topic: ecommerce_transactions\n    keyType: string  # transaction_id\n    valueType: json  # transaction data\n\n  sales_by_category:\n    topic: sales_by_category\n    keyType: string  # product_category\n    valueType: json  # aggregated sales data\n\n  avg_order_value:\n    topic: avg_order_value\n    keyType: windowed(string)  # time window\n    valueType: json  # average order value\n\n  transactions_by_region:\n    topic: transactions_by_region\n    keyType: string  # region\n    valueType: string  # transaction count\n\nfunctions:\n  calculate_total:\n    type: aggregator\n    resultType: json\n    code: |\n      if aggregatedValue is None:\n        return {\"total_sales\": value.get(\"price\") * value.get(\"quantity\"), \"count\": 1}\n      else:\n        return {\n          \"total_sales\": aggregatedValue.get(\"total_sales\") + (value.get(\"price\") * value.get(\"quantity\")),\n          \"count\": aggregatedValue.get(\"count\") + 1\n        }\n\n  extract_category:\n    type: keyTransformer\n    resultType: string\n    code: |\n      if value is None: return None\n      return value.get(\"product_category\")\n\n  init_category_counter:\n    type: initializer\n    resultType: json\n    code: |\n      return {\"total_sales\": 0, \"count\": 0}\n\n  group_all:\n    type: keyTransformer\n    resultType: string\n    code: |\n      return \"all\"\n\n  init_sales_counter:\n    type: initializer\n    resultType: json\n    code: |\n      return {\"total_sales\": 0.0, \"count\": 0}\n\n  aggregate_window_sales:\n    type: aggregator\n    resultType: json\n    code: |\n      return {\n        \"total_sales\": aggregatedValue.get(\"total_sales\") + (value.get(\"price\") * value.get(\"quantity\")),\n        \"count\": aggregatedValue.get(\"count\") + 1\n      }\n\n  calc_avg_order:\n    type: valueTransformer\n    resultType: json\n    code: |\n      if value and value.get(\"count\") and value.get(\"count\") &gt; 0:\n        return {\"avg_order_value\": value.get(\"total_sales\") / value.get(\"count\")}\n      else:\n        return {\"avg_order_value\": 0}\n\n  extract_region:\n    type: keyTransformer\n    resultType: string\n    code: |\n      if value is None: return None\n      return value.get(\"region\")\n\npipelines:\n  # Pipeline for sales by category\n  sales_by_category_pipeline:\n    from: transactions\n    via:\n      - type: selectKey\n        mapper: extract_category\n      - type: groupByKey\n      - type: aggregate\n        store:\n          name: category_store\n          type: keyValue\n        initializer: init_category_counter\n        aggregator: calculate_total\n      - type: toStream\n    to: sales_by_category\n\n  # Pipeline for average order value (windowed)\n  avg_order_value_pipeline:\n    from: transactions\n    via:\n      - type: groupBy\n        mapper: group_all\n      - type: windowByTime\n        windowType: tumbling\n        duration: 10s\n      - type: aggregate\n        store:\n          name: avg_order_store_10s\n          type: window\n          windowSize: 10s\n          retention: 30s\n        initializer: init_sales_counter\n        aggregator: aggregate_window_sales\n      - type: suppress\n        until: windowCloses\n      - type: toStream\n      - type: convertKey\n        into: json:windowed(string)\n      - type: mapValues\n        mapper: calc_avg_order\n    to: avg_order_value\n\n  # Pipeline for transactions by region\n  transactions_by_region_pipeline:\n    from: transactions\n    via:\n      - type: selectKey\n        mapper: extract_region\n      - type: groupByKey\n      - type: count\n      - type: toStream\n      - type: convertValue\n        into: string\n    to: transactions_by_region\n</code></pre>"},{"location":"use-cases/real-time-analytics/#running-the-application","title":"Running the Application","text":"<p>You can use the following producer pipeline example as a starting point to write some simulated sale data; it will write four hard coded sales records into the input stream and exit.</p> Transaction data producer (click to expand) <pre><code># $schema: https://raw.githubusercontent.com/Axual/ksml/refs/heads/main/docs/ksml-language-spec.json\n\nstreams:\n  transactions:\n    topic: ecommerce_transactions\n    keyType: string  # transaction_id\n    valueType: json  # transaction data\n\n# set up global data and a function that will loop over it once\nfunctions:\n  generate_transactions:\n    type: generic\n    resultType: boolean\n    expression: False\n    globalCode: |\n      global count\n      count = 0\n      global done\n      done = False\n      global orders\n      orders = [\n        {\n          \"transaction_id\": \"12345\",\n          \"timestamp\": 1625097600000,\n          \"customer_id\": \"cust-789\",\n          \"product_id\": \"prod-456\",\n          \"product_category\": \"electronics\",\n          \"quantity\": 1,\n          \"price\": 499.99,\n          \"region\": \"north_america\",\n          \"marketing_channel\": \"social_media\"\n        },\n        {\n          \"transaction_id\": \"12346\",\n          \"timestamp\": 1625097606000,\n          \"customer_id\": \"cust-788\",\n          \"product_id\": \"prod-457\",\n          \"product_category\": \"gardening\",\n          \"quantity\": 1,\n          \"price\": 229.99,\n          \"region\": \"europe\",\n          \"marketing_channel\": \"social_media\"\n        },\n        {\n          \"transaction_id\": \"12349\",\n          \"timestamp\": 1625097606000,\n          \"customer_id\": \"cust-788\",\n          \"product_id\": \"prod-457\",\n          \"product_category\": \"electronics\",\n          \"quantity\": 1,\n          \"price\": 229.99,\n          \"region\": \"europe\",\n          \"marketing_channel\": \"social_media\"\n        },\n        {\n          \"transaction_id\": \"12347\",\n          \"timestamp\": 1625097612000,\n          \"customer_id\": \"cust-789\",\n          \"product_id\": \"prod-458\",\n          \"product_category\": \"gardening\",\n          \"quantity\": 1,\n          \"price\": 12.99,\n          \"region\": \"north_america\",\n          \"marketing_channel\": \"social_media\"\n        }\n      ]\n      nr_orders = len(orders)\n\n      def nextOrder():\n         global count\n         global done\n         result = orders[count]\n         count = count + 1\n         if count &gt;= len(orders):\n           count = 0\n           done = True    # loop once over the data\n         return result\n\nproducers:\n  produce_order:\n    to: transactions\n    interval: 7000  # Emit every 7 seconds to ensure windows close\n    until:\n      expression:\n        done\n    generator:\n      code: |\n        order = nextOrder()\n      expression: '(str(count), order)'\n      resultType: (string, json)\n</code></pre> <p>To run the application:</p> <ol> <li>Save the KSML definition to <code>definitions/analytics.yaml</code></li> <li>Add the <code>example-data-producer.yaml</code> above if desired, or create test data in some other way</li> <li>Create a configuration file at <code>config/application.properties</code> with your Kafka connection details</li> <li>Start the Docker Compose environment: <code>docker-compose up -d</code></li> <li>Monitor the output topics to see the real-time analytics results</li> </ol>"},{"location":"use-cases/real-time-analytics/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Setting up visualizations is outside the scope of this tutorial. You can connect the output topics to visualization tools like:</p> <ul> <li>Grafana</li> <li>Kibana</li> <li>Custom dashboards using web frameworks</li> </ul> <p>For example, to create a simple dashboard with Grafana:</p> <ol> <li>Set up Grafana to connect to your Kafka topics</li> <li>Create dashboards for each metric:</li> <li>Bar chart for sales by category</li> <li>Line chart for average order value over time</li> <li>Map visualization for transactions by region</li> <li>Gauge charts for conversion rates</li> </ol>"},{"location":"use-cases/real-time-analytics/#extending-the-application","title":"Extending the Application","text":"<p>You can extend this application in several ways:</p> <ul> <li>Add anomaly detection to identify unusual patterns</li> <li>Implement trend analysis to detect changing consumer behavior</li> <li>Create alerts for specific conditions (e.g., sales dropping below a threshold)</li> <li>Enrich the data with additional information from reference data sources</li> </ul>"},{"location":"use-cases/real-time-analytics/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to:</p> <ul> <li>Process streaming transaction data in real-time</li> <li>Calculate various business metrics using KSML</li> <li>Structure a real-time analytics application</li> <li>Prepare the results for visualization</li> </ul> <p>Real-time analytics is a powerful use case for KSML, allowing you to gain immediate insights from your streaming data without complex coding.</p>"},{"location":"use-cases/real-time-analytics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Transformation for more complex processing</li> <li>Explore Event-Driven Applications to trigger actions based on analytics</li> <li>Check out Performance Optimization for handling high-volume data</li> </ul>"}]}